<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MongoDB（五）-- 副本集（replica Set）]]></title>
    <url>%2F2019%2F06%2F28%2FMongoDB%EF%BC%88%E4%BA%94%EF%BC%89-%E5%89%AF%E6%9C%AC%E9%9B%86%EF%BC%88replica-Set%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、副本集介绍搭建副本集是为了实现mongodb高可用。 Mongodb(M)表示主节点，Mongodb(S)表示备节点，Mongodb(A)表示仲裁节点。主备节点存储数据，仲裁节点不存储数据。客户端同时连接主节点与备节点，不连接仲裁节点。 仲裁节点是一种特殊的节点，它本身并不存储数据，主要的作用是决定哪一个备节点在主节点挂掉之后提升为主节点，所以客户端不需要连接此节点。 在MongoDB副本集中，主节点负责处理客户端的读写请求，备份节点则负责映射主节点的数据。 备份节点的工作原理过程可以大致描述为，备份节点定期轮询主节点上的数据操作，然后对自己的数据副本进行这些操作，从而保证跟主节点的数据同步。至于主节点上的所有 数据库状态改变 的操作，都会存放在一张特定的系统表中。备份节点则是根据这些数据进行自己的数据更新。 上面提到的 数据库状态改变 的操作，称为oplog（operation log，主节点操作记录）。oplog存储在local数据库的”oplog.rs”表中。副本集中备份节点异步的从主节点同步oplog，然后重新执行它记录的操作，以此达到了数据同步的作用。 Oplog注意点： Oplog的大小是固定的，当集合被填满的时候，新的插入的文档会覆盖老的文档。 Oplog同步数据 初始化：这个过程发生在当副本集中创建一个新的数据库或其中某个节点刚从宕机中恢复，或者向副本集中添加新的成员的时候，默认的，副本集中的节点会从离它最近的节点复制oplog来同步数据，这个最近的节点可以是primary也可以是拥有最新oplog副本的secondary节点。 二、搭建有仲裁节点的副本集1.进入到/usr/java中，新建mongodbRepliSet文件夹，然后在 mongodbRepliSet 文件夹中新建 3个节点，先 mkdir node1。 2.进入到 node1中，新建 data 和 log文件夹，即 mkdir data log，然后进入到 data中，mkdir db。 3.拷贝mongodb的配置文件到 node1中， cp /usr/java/mongoNode/mongodb.conf /usr/java/mongodbRepliSet/node1/mongodb.conf 4.修改配置文件，vim mongodb.conf 1234567dbpath=/usr/java/mongodbRepliSet/node1/data//dblogpath=/usr/java/mongodbRepliSet/node1/log/mongodb.loglogappend=truefork=truebind_ip=192.168.80.128port=27017replSet=JoeSet # 3个节点的这个配置要一样，表示在一个副本集中 5.拷贝 node1 整个文件夹，名为 node2 和 node3 12cp -r node1 node2cp -r node1 node3 6.修改 node2 和 node3 文件夹中的mongodb.conf，修改 dbpath、logpath 和 port（27018、27019）。 7.配置 临时的环境变量，export PATH=/usr/java/mongodb/bin:$PATH 8.查看临时的环境变量是否配置成功：echo $PATH 9.启动3个节点，分别进入 node1 node2和node3中，以配置文件方式启动：mongod –config mongodb.conf 10.客户端连接node1，mongo –host 192.168.80.128 –port 27017 11.上述配置 没有指定哪一个是master、slave、仲裁节点，所以需要执行下 副本集的初始化，执行： rs.initiate({“_id”:”JoeSet”,members:[{“_id”:1,”host”:”192.168.80.128:27017”,priority:3},{“_id”:2,”host”:”192.168.80.128:27018”, priority:9},{“_id”:3,”host”:”192.168.80.128:27019”,arbiterOnly:true}]})。 “_id”: 副本集的名称 “members”: 副本集的服务器列表 “_id”: 服务器的唯一ID “host”: 服务器主机 “priority”: 是优先级，默认为1，优先级0为被动节点，不能成为活跃节点。优先级不位0则按照有大到小选出活跃节点。 “arbiterOnly”: 仲裁节点，只参与投票，不接收数据，也不能成为活跃节点。 执行完 初始化命令后，会变为上图所示。 注意，可能出现的错误： 123456&gt; rs.initiate(&#123;&quot;_id&quot;:&quot;JoeSet&quot;,members:[&#123;&quot;_id&quot;:1,&quot;host&quot;:&quot;192.168.80.128:27017&quot;,priority:3&#125;,&#123;&quot;_id&quot;:2,&quot;host&quot;:&quot;192.168.80.128:27018&quot;, priority:9&#125;,&#123;&quot;_id&quot;:4,&quot;host&quot;:&quot;192.168.80.128:27019&quot;,arbiterOnly:true&#125;]&#125;)&#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;This node, 192.168.80.128:27019, with _id 4 is not electable under the new configuration version 1 for replica set JoeSet&quot;, &quot;code&quot; : 93&#125; 错误原因，是因为客户端连接了 27019，而27019 又是仲裁节点，所以出现了这个错误。解决方法，客户端连接 其他节点，不连接 设置为仲裁的节点。 12.执行：rs.status()，查看状态。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849JoeSet:OTHER&gt; rs.status()&#123; &quot;set&quot; : &quot;JoeSet&quot;, &quot;date&quot; : ISODate(&quot;2017-07-26T22:09:44.940Z&quot;), &quot;myState&quot; : 2, &quot;members&quot; : [ &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.80.128:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, # SECONDARY 表示从节点 &quot;uptime&quot; : 38, &quot;optime&quot; : Timestamp(1501106974, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-26T22:09:34Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.80.128:27018&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, # PRIMARY 表示主节点 &quot;uptime&quot; : 10, &quot;optime&quot; : Timestamp(1501106974, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-26T22:09:34Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-26T22:09:44.560Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-26T22:09:44.579Z&quot;), &quot;pingMs&quot; : 1, &quot;electionTime&quot; : Timestamp(1501106981, 1), &quot;electionDate&quot; : ISODate(&quot;2017-07-26T22:09:41Z&quot;), &quot;configVersion&quot; : 1 &#125;, &#123; &quot;_id&quot; : 3, &quot;name&quot; : &quot;192.168.80.128:27019&quot;, &quot;health&quot; : 1, &quot;state&quot; : 7, &quot;stateStr&quot; : &quot;ARBITER&quot;, # ARBITER 表示仲裁节点 &quot;uptime&quot; : 10, &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-26T22:09:44.559Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-26T22:09:44.585Z&quot;), &quot;pingMs&quot; : 0, &quot;configVersion&quot; : 1 &#125; ], &quot;ok&quot; : 1&#125; 13.为了验证副本集搭建成功，我们 在node2（主节点） 中插入几条数据，然后在 node1 （从节点）中查看，因为 上面配置了 node2 为主节点， node1为从节点，node3为仲裁节点。 1) 在node2中： 123456789101112131415JoeSet:PRIMARY&gt; show dbslocal 0.078GBJoeSet:PRIMARY&gt; use testdbswitched to db testdbJoeSet:PRIMARY&gt; db.createCollection(&quot;testCon&quot;)&#123; &quot;ok&quot; : 1 &#125;JoeSet:PRIMARY&gt; show collectionssystem.indexestestConJoeSet:PRIMARY&gt; for(var i=0; i&lt;3;i++) db.testCon.insert(&#123;name:&quot;Joe&quot;,index:i&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)JoeSet:PRIMARY&gt; db.testCon.find()&#123; &quot;_id&quot; : ObjectId(&quot;597a553d7db09ad9f77f1353&quot;), &quot;name&quot; : &quot;Joe&quot;, &quot;index&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;597a553d7db09ad9f77f1354&quot;), &quot;name&quot; : &quot;Joe&quot;, &quot;index&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;597a553d7db09ad9f77f1355&quot;), &quot;name&quot; : &quot;Joe&quot;, &quot;index&quot; : 2 &#125; 2) 在node1中： 1234567891011121314151617181920JoeSet:SECONDARY&gt; show dbs2017-07-27T14:06:14.672-0700 E QUERY Error: listDatabases failed:&#123; &quot;note&quot; : &quot;from execCommand&quot;, &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master&quot; &#125; at Error (&lt;anonymous&gt;) at Mongo.getDBs (src/mongo/shell/mongo.js:47:15) at shellHelper.show (src/mongo/shell/utils.js:630:33) at shellHelper (src/mongo/shell/utils.js:524:36) at (shellhelp2):1:1 at src/mongo/shell/mongo.js:47JoeSet:SECONDARY&gt; rs.slaveOk()JoeSet:SECONDARY&gt; show dbslocal 0.078GBtestdb 0.078GBJoeSet:SECONDARY&gt; use testdbswitched to db testdbJoeSet:SECONDARY&gt; show collectionssystem.indexestestConJoeSet:SECONDARY&gt; db.testCon.find()&#123; &quot;_id&quot; : ObjectId(&quot;597a553d7db09ad9f77f1353&quot;), &quot;name&quot; : &quot;Joe&quot;, &quot;index&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;597a553d7db09ad9f77f1354&quot;), &quot;name&quot; : &quot;Joe&quot;, &quot;index&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;597a553d7db09ad9f77f1355&quot;), &quot;name&quot; : &quot;Joe&quot;, &quot;index&quot; : 2 &#125; 在node1（从节点）中 可以查看到 node2（主节点）插入的数据，说明 副本集搭建成功。 注意：从节点中是不允许执行写操作的。 三、模拟主节点故障及恢复1) 模拟node2（主节点）挂了，kiil 掉node2（主节点）的进程 2) 客户端连接 node1（从节点）：mongo –host 192.168.80.128 –port 27017 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950JoeSet:PRIMARY&gt; rs.status()&#123; &quot;set&quot; : &quot;JoeSet&quot;, &quot;date&quot; : ISODate(&quot;2017-07-27T21:20:45.629Z&quot;), &quot;myState&quot; : 1, &quot;members&quot; : [ &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.80.128:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 4209, &quot;optime&quot; : Timestamp(1501189437, 10), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T21:03:57Z&quot;), &quot;electionTime&quot; : Timestamp(1501190187, 1), &quot;electionDate&quot; : ISODate(&quot;2017-07-27T21:16:27Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.80.128:27018&quot;, &quot;health&quot; : 0, &quot;state&quot; : 8, &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;, &quot;uptime&quot; : 0, &quot;optime&quot; : Timestamp(0, 0), &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T21:20:43.773Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T21:16:24.292Z&quot;), &quot;pingMs&quot; : 1, &quot;lastHeartbeatMessage&quot; : &quot;Failed attempt to connect to 192.168.80.128:27018; couldn&apos;t connect to server 192.168.80.128:27018 (192.168.80.128), connection attempt failed&quot;, &quot;configVersion&quot; : -1 &#125;, &#123; &quot;_id&quot; : 3, &quot;name&quot; : &quot;192.168.80.128:27019&quot;, &quot;health&quot; : 1, &quot;state&quot; : 7, &quot;stateStr&quot; : &quot;ARBITER&quot;, &quot;uptime&quot; : 4191, &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T21:20:45.475Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T21:20:44.876Z&quot;), &quot;pingMs&quot; : 0, &quot;configVersion&quot; : 1 &#125; ], &quot;ok&quot; : 1&#125; 发现原来的从节点（node1）变为了 主节点，而原来的主节点显示的状态是 不可达、不健康的。这对于整个副本集的使用是没有影响的。 3) 重新启动node2，即 原来的主节点。如果启动node2失败，就删除掉 db文件夹下的mongod.lock文件。 4) 客户端连接node1，mongo –host 192.168.80.128 –port 27017，发现node2 重新启动后，node1 又变为 从节点，而 node2又变为 原来的主节点，这是因为 仲裁节点 和 设置的优先级的原因。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849JoeSet:SECONDARY&gt; rs.status()&#123; &quot;set&quot; : &quot;JoeSet&quot;, &quot;date&quot; : ISODate(&quot;2017-07-27T21:31:06.197Z&quot;), &quot;myState&quot; : 2, &quot;members&quot; : [ &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.80.128:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 4830, &quot;optime&quot; : Timestamp(1501189437, 10), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T21:03:57Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.80.128:27018&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 87, &quot;optime&quot; : Timestamp(1501189437, 10), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T21:03:57Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T21:31:05.209Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T21:31:05.506Z&quot;), &quot;pingMs&quot; : 0, &quot;electionTime&quot; : Timestamp(1501190981, 1), &quot;electionDate&quot; : ISODate(&quot;2017-07-27T21:29:41Z&quot;), &quot;configVersion&quot; : 1 &#125;, &#123; &quot;_id&quot; : 3, &quot;name&quot; : &quot;192.168.80.128:27019&quot;, &quot;health&quot; : 1, &quot;state&quot; : 7, &quot;stateStr&quot; : &quot;ARBITER&quot;, &quot;uptime&quot; : 4811, &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T21:31:06.085Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T21:31:05.633Z&quot;), &quot;pingMs&quot; : 0, &quot;configVersion&quot; : 1 &#125; ], &quot;ok&quot; : 1&#125; 当仲裁节点挂掉后，达不到高可用了，即当 主节点挂了后，从节点在没有仲裁节点的情况下，不会切换为 主节点了。所以，推荐使用没有仲裁节点的副本集，如下。 四、搭建没有仲裁节点的副本集，推荐使用1.复制node1 为 node6、node7、node8： 123cp -r node1 node6cp -r node1 node7cp -r node1 node8 2.清空node6、node7、node8的db文件夹 和 log 文件夹 12rm -rf data/db/*rm -rf log/* 3.修改node6、node7、node8 的datapath、logpath和port、replSet node6 的port 为 27021，node7 的port 为 27022，node8 的port 为 27023，replSet 为 XbqSet 4.启动三个节点，分别进入 对应的node文件夹中： mongod –config mongodb.conf 5.连接node6节点（27021）：mongo –host 192.168.80.128 –port 27021 6.执行初始化命令：rs.initiate({“_id”:”XbqSet”,members:[{“_id”:1,”host”:”192.168.80.128:27021”},{“_id”:2,”host”:”192.168.80.128:27022”},{“_id”:3,”host”:”192.168.80.128:27023” }]}) 7.查看状态，发现有一个主节点，两个从节点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051XbqSet:PRIMARY&gt; rs.status()&#123; &quot;set&quot; : &quot;XbqSet&quot;, &quot;date&quot; : ISODate(&quot;2017-07-27T22:02:28.188Z&quot;), &quot;myState&quot; : 1, &quot;members&quot; : [ &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.80.128:27021&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 483, &quot;optime&quot; : Timestamp(1501192872, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T22:01:12Z&quot;), &quot;electionTime&quot; : Timestamp(1501192876, 1), &quot;electionDate&quot; : ISODate(&quot;2017-07-27T22:01:16Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.80.128:27022&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 75, &quot;optime&quot; : Timestamp(1501192872, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T22:01:12Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T22:02:26.973Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T22:02:26.973Z&quot;), &quot;pingMs&quot; : 0, &quot;configVersion&quot; : 1 &#125;, &#123; &quot;_id&quot; : 3, &quot;name&quot; : &quot;192.168.80.128:27023&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 75, &quot;optime&quot; : Timestamp(1501192872, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T22:01:12Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T22:02:26.973Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T22:02:26.973Z&quot;), &quot;pingMs&quot; : 0, &quot;configVersion&quot; : 1 &#125; ], &quot;ok&quot; : 1&#125; 8.kill 掉node6，即端口 27021 9.然后连接到node7上：mongo –host 192.168.80.128 –port 27022，查看状态，发现node7为从节点，node8为主节点了，原来的node6 不可达、不健康的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152XbqSet:SECONDARY&gt; rs.status()&#123; &quot;set&quot; : &quot;XbqSet&quot;, &quot;date&quot; : ISODate(&quot;2017-07-27T22:08:24.894Z&quot;), &quot;myState&quot; : 2, &quot;members&quot; : [ &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.80.128:27021&quot;, &quot;health&quot; : 0, &quot;state&quot; : 8, &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;, &quot;uptime&quot; : 0, &quot;optime&quot; : Timestamp(0, 0), &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T22:08:23.748Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T22:07:43.464Z&quot;), &quot;pingMs&quot; : 0, &quot;lastHeartbeatMessage&quot; : &quot;Failed attempt to connect to 192.168.80.128:27021; couldn&apos;t connect to server 192.168.80.128:27021 (192.168.80.128), connection attempt failed&quot;, &quot;configVersion&quot; : -1 &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.80.128:27022&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 833, &quot;optime&quot; : Timestamp(1501192872, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T22:01:12Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true &#125;, &#123; &quot;_id&quot; : 3, &quot;name&quot; : &quot;192.168.80.128:27023&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 431, &quot;optime&quot; : Timestamp(1501192872, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T22:01:12Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T22:08:23.409Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T22:08:23.521Z&quot;), &quot;pingMs&quot; : 0, &quot;electionTime&quot; : Timestamp(1501193266, 1), &quot;electionDate&quot; : ISODate(&quot;2017-07-27T22:07:46Z&quot;), &quot;configVersion&quot; : 1 &#125; ], &quot;ok&quot; : 1&#125; 10.重新启动node6：mongod –config mongodb.conf 11.继续连接node7：mongo –host 192.168.80.128 –port 27022，查看状态，原来的node6 变为了 从节点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051XbqSet:SECONDARY&gt; rs.status()&#123; &quot;set&quot; : &quot;XbqSet&quot;, &quot;date&quot; : ISODate(&quot;2017-07-27T22:12:41.275Z&quot;), &quot;myState&quot; : 2, &quot;members&quot; : [ &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.80.128:27021&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 54, &quot;optime&quot; : Timestamp(1501192872, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T22:01:12Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T22:12:40.382Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T22:12:41.213Z&quot;), &quot;pingMs&quot; : 0, &quot;configVersion&quot; : 1 &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.80.128:27022&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 1090, &quot;optime&quot; : Timestamp(1501192872, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T22:01:12Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true &#125;, &#123; &quot;_id&quot; : 3, &quot;name&quot; : &quot;192.168.80.128:27023&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 688, &quot;optime&quot; : Timestamp(1501192872, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-27T22:01:12Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-27T22:12:39.673Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-27T22:12:39.820Z&quot;), &quot;pingMs&quot; : 0, &quot;electionTime&quot; : Timestamp(1501193266, 1), &quot;electionDate&quot; : ISODate(&quot;2017-07-27T22:07:46Z&quot;), &quot;configVersion&quot; : 1 &#125; ], &quot;ok&quot; : 1&#125; 五、增加、删除节点1.cp -r node8 node10，修改node10的datapath、logpath的port，并将port 改为 27024，启动 node10。 2.进入到主节点中，mongo –host 192.168.80.128 –port 27023，一定要在主节点中进行操作。 3.增加节点：rs.add(“192.168.80.128:27024”) 12XbqSet:PRIMARY&gt; rs.add(&quot;192.168.80.128:27024&quot;)&#123; &quot;ok&quot; : 1 &#125; 4.查看状态，rs.status()，发现新增加的节点 为从节点，即 SECONDARY。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465XbqSet:PRIMARY&gt; rs.status()&#123; &quot;set&quot; : &quot;XbqSet&quot;, &quot;date&quot; : ISODate(&quot;2017-07-29T14:31:44.872Z&quot;), &quot;myState&quot; : 1, &quot;members&quot; : [ &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.80.128:27021&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 630, &quot;optime&quot; : Timestamp(1501338697, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-29T14:31:37Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-29T14:31:43.555Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-29T14:31:43.457Z&quot;), &quot;pingMs&quot; : 0, &quot;configVersion&quot; : 2 &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.80.128:27022&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 1854, &quot;optime&quot; : Timestamp(1501338697, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-29T14:31:37Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-29T14:31:43.555Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-29T14:31:43.186Z&quot;), &quot;pingMs&quot; : 0, &quot;configVersion&quot; : 2 &#125;, &#123; &quot;_id&quot; : 3, &quot;name&quot; : &quot;192.168.80.128:27023&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 1953, &quot;optime&quot; : Timestamp(1501338697, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-29T14:31:37Z&quot;), &quot;electionTime&quot; : Timestamp(1501338022, 1), &quot;electionDate&quot; : ISODate(&quot;2017-07-29T14:20:22Z&quot;), &quot;configVersion&quot; : 2, &quot;self&quot; : true &#125;, &#123; &quot;_id&quot; : 4, &quot;name&quot; : &quot;192.168.80.128:27024&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 7, &quot;optime&quot; : Timestamp(1501338697, 1), &quot;optimeDate&quot; : ISODate(&quot;2017-07-29T14:31:37Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-07-29T14:31:43.566Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-07-29T14:31:43.579Z&quot;), &quot;pingMs&quot; : 6, &quot;configVersion&quot; : 2 &#125; ], &quot;ok&quot; : 1&#125; 5.删除节点：rs.remove(hostportstr) 12XbqSet:PRIMARY&gt; rs.remove(&quot;192.168.80.128:27024&quot;)&#123; &quot;ok&quot; : 1 &#125; 查看状态，发现 端口为27024的节点 没有了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MongoDB（四）-- 主从配置]]></title>
    <url>%2F2019%2F06%2F27%2FMongoDB%EF%BC%88%E5%9B%9B%EF%BC%89-%E4%B8%BB%E4%BB%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[一、前言虽然MongoDB官方已经不建议使用主从模式了，但是 熟悉下 也是有用的，替代方案是采用副本集的模式。slave默认情况下是不支持读写的，但是master会把数据同步到slave，不支持客户端读写。客户端连接slave时用命令支持读：rs.slaveOk()。 二、主从配置1.进入到 /usr/java 中，cd /usr/java 2.新建mongodbMaster-slave，mkdir mongodbMaster-slave 3.进入到 mongodbMaster-slave 文件夹中，新建 master 和 slave 文件夹，mkdir master slave 4.进入到 master 和 slave 中，新建 data 和 log 文件件，mkdir data log；进入到 data中，新建 db文件件，mkdir db 5.进入到 mongodbMaster-slave 中，配置 临时的环境变量，export PATH=/usr/java/mongodb/bin:$PATH 6.查看临时的环境变量是否配置成功：echo $PATH 7.执行 mongod –help ，查看帮助信息 –master：指定master节点； –slave：指定slave节点； –source ：指向服务的端口。 8.将 mongodb 的配置文件 拷贝到 master 和 slave 中： 12cp mongodb.conf ../mongodbMaster-slave/master/mongodb.confcp mongodb.conf ../mongodbMaster-slave/slave/mongodb.conf 9.修改 master中的 mongodb.conf ，vim mongodb.conf 12345678dbpath=/usr/java/mongodbMaster-slave/master/data/dblogpath=/usr/java/mongodbMaster-slave/master/log/mongodb.loglogappend=truefork=truebind_ip=192.168.80.128 # master IPport=27020 # master 端口master=true # 设置为mastersource=192.168.80.128:27021 # slave的IP 和 端口 同理修改 slave 中的mongodb.conf ，vim mongodb.conf 12345678dbpath=/usr/java/mongodbMaster-slave/slave/data/dblogpath=/usr/java/mongodbMaster-slave/slave/log/mongodb.loglogappend=truefork=truebind_ip=192.168.80.128 # slave IPport=27021 # slave 端口slave=true # 设置为 slavesource=192.168.80.128:27020 # master的IP 和 端口 10.启动 master，进入到 master 文件夹中： mongod –config mongodb.conf 11.启动slave，进入到 slave 文件夹中： mongod –config mongodb.conf 12.验证主从是否配置成功 1）客户端连接 slave： mongo –host 192.168.80.128 –port 27021 方法一：执行：db.printSlaveReplicationInfo() 查看 是否是从，source 指向的是 27020，即 master。 2）客户端连接 slave，通过 查看 local 数据库中的 表的信息 了解 对应的 master，我们 来看下 所有的 db，即 show dbs，此时，发现报错，如下： 1234562017-07-23T14:07:13.196-0700 E QUERY Error: listDatabases failed:&#123; &quot;note&quot; : &quot;from execCommand&quot;, &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master&quot; &#125; at Error (&lt;anonymous&gt;) at Mongo.getDBs (src/mongo/shell/mongo.js:47:15) at shellHelper.show (src/mongo/shell/utils.js:630:33) at shellHelper (src/mongo/shell/utils.js:524:36) at (shellhelp2):1:1 at src/mongo/shell/mongo.js:47 解决方法，执行 rs.slaveOK()， 进入到local 数据库，查看 所有的表：show collections，发现有一个 sources ，我们 查询 sources 表中的数据，db.sources.find()： 查询结果：{ “_id” : ObjectId(“59750d91828f18cb9f133ef2”), “host” : “192.168.80.128:27020”, “source” : “main”, “syncedTo” : Timestamp(1500844337, 1) }，发现 host对应的是 master 的IP和端口，说明 主从配置成功。 3）通过 在 master 中 插入几条数据，然后 在 slave中 查看 的方法，来看 主从 是否配置成功。 在master中执行 如下，插入 一条数据： 在slave中 执行如下，查询数据： 可以看到 在 master 中 插入的数据，说明主从配置成功。 4）也可以 通过 db.isMaster() 查看，在master中执行： 在 slave中 执行：]]></content>
  </entry>
  <entry>
    <title><![CDATA[MongoDB（三）-- 执行JS、界面工具]]></title>
    <url>%2F2019%2F06%2F27%2FMongoDB%EF%BC%88%E4%B8%89%EF%BC%89-%E6%89%A7%E8%A1%8CJS%E3%80%81%E7%95%8C%E9%9D%A2%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[一、执行Js脚本1.开启mongod服务 2.连接mongodb客户端，./mongo –host 192.168.80.128 –port 27017 3.创建数据库：use testdb1 4.创建表：db.createCollection(“testTable1”) 5.执行JS脚本插入数据：for(var i=0;i&lt;100;i++) db.testTable1.insert({name:”Joe”,age:20,index:i}) 6.查询插入的数据：db.testTable1.find() 二、使用界面工具​ ​ ​ 这个工具很简单，有相应的提示，在灰色框中输入 命令执行即可。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MongoDB（二）-- Java API 实现增删改查]]></title>
    <url>%2F2019%2F06%2F27%2FMongoDB%EF%BC%88%E4%BA%8C%EF%BC%89-Java-API-%E5%AE%9E%E7%8E%B0%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[一、下载jar包http://central.maven.org/maven2/org/mongodb/mongo-java-driver/ 二、代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278package com.xbq.mongodb;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;import org.bson.types.ObjectId;import com.mongodb.BasicDBObject;import com.mongodb.DB;import com.mongodb.DBCollection;import com.mongodb.DBCursor;import com.mongodb.DBObject;import com.mongodb.Mongo;import com.mongodb.util.JSON;/** * @ClassName: MongoDBTest * @Description: TODO MongoDB 增删改查 操作，包含批量操作 * @author xbq * @version 1.0 * @date 2017-4-5 上午11:50:06 */public class MongoDBTest &#123; private static final String HOST = &quot;192.168.242.129&quot;; private static final int PORT = 27017; private static final String DB_NAME = &quot;testDB&quot;; private static Mongo mongo; private static DB db; static &#123; // 连接到MongoDB mongo = new Mongo(HOST, PORT); // 打开数据库 testDB db = mongo.getDB(DB_NAME); &#125; public static void main(String[] args) &#123; // 获取集合 xbqTable，若该集合不存在，mongoDB将自动创建该集合 DBCollection dbCollection = db.getCollection(&quot;testTable&quot;); // 查询该数据库所有的集合名 for(String name : mongo.getDatabaseNames())&#123; System.out.println(name); &#125; // addOne(dbCollection);// addList(dbCollection);// addByJson(dbCollection); // deleteOne(dbCollection);// deleteByIn(dbCollection);// deleteAll(dbCollection); // updateOne(dbCollection);// updateMulti(dbCollection); // queryOne(dbCollection);// queryPage(dbCollection);// queryRange(dbCollection); queryList(dbCollection); &#125; // ====================================查询开始============================================== /** * @Title: queryOne * @Description: TODO 查询 name为 张三的 一条记录 * @param dbCollection * @return: void */ public static void queryOne(DBCollection dbCollection)&#123; DBObject documents = new BasicDBObject(&quot;name&quot;,&quot;张三&quot;); DBObject result = dbCollection.findOne(documents); System.out.println(result); &#125; /** * @Title: queryPage * @Description: TODO 分页查询 ， 查询 跳过前2条 后的 3条 数据 * @param dbCollection * @return: void */ public static void queryPage(DBCollection dbCollection)&#123; DBCursor cursor = dbCollection.find().skip(2).limit(3); while (cursor.hasNext()) &#123; System.out.println(cursor.next()); &#125; &#125; /** * @Title: queryRange * @Description: TODO 范围查询，查询 第3条 到 第5条 之间的记录 * @param dbCollection * @return: void */ public static void queryRange(DBCollection dbCollection) &#123; DBObject range = new BasicDBObject(); range.put(&quot;$gte&quot;, 50); range.put(&quot;$lte&quot;, 52); DBObject dbObject = new BasicDBObject(); dbObject.put(&quot;age&quot;, range); DBCursor cursor = dbCollection.find(dbObject); while (cursor.hasNext()) &#123; System.out.println(cursor.next()); &#125; &#125; /**&apos; * @Title: queryList * @Description: TODO 查询出全部的 记录 * @param dbCollection * @return: void */ public static void queryList(DBCollection dbCollection) &#123; DBCursor cursor = dbCollection.find(); DBObject dbObject = null; while(cursor.hasNext())&#123; dbObject = cursor.next(); System.out.println(dbObject); &#125; &#125; // ====================================增加开始============================================== /** * @Title: addOne * @Description: TODO 新增 一条记录 * @param dbCollection * @return: void */ public static void addOne(DBCollection dbCollection)&#123; DBObject documents = new BasicDBObject(&quot;name&quot;,&quot;张三&quot;).append(&quot;age&quot;, 45).append(&quot;sex&quot;, &quot;男&quot;).append(&quot;address&quot;, new BasicDBObject(&quot;postCode&quot;, 100000).append(&quot;street&quot;, &quot;深南大道888号&quot;).append(&quot;city&quot;, &quot;深圳&quot;)); dbCollection.insert(documents); &#125; /** * @Title: addList * @Description: TODO 批量新增 记录 , 增加的记录 中 可以使用各种数据类型 * @param dbCollection * @return: void */ public static void addList(DBCollection dbCollection)&#123; List&lt;DBObject&gt; listdbo= new ArrayList&lt;DBObject&gt;(); DBObject dbObject = new BasicDBObject(); dbObject.put(&quot;name&quot;, &quot;老王&quot;); // 可以直接保存List类型 List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;非隔壁老王&quot;); dbObject.put(&quot;remark&quot;, list); listdbo.add(dbObject); dbObject = new BasicDBObject(); // 可以直接保存map Map&lt;String,List&lt;String&gt;&gt; map = new HashMap&lt;String,List&lt;String&gt;&gt;(); List&lt;String&gt; hobbys = new ArrayList&lt;String&gt;(); hobbys.add(&quot;看花&quot;); hobbys.add(&quot;采花&quot;); map.put(&quot;爱好&quot;, hobbys); dbObject.put(&quot;hobby&quot;, map); listdbo.add(dbObject); dbObject = new BasicDBObject(); dbObject.put(&quot;name&quot;, &quot;老张&quot;); dbObject.put(&quot;age&quot;, 52); dbObject.put(&quot;job&quot;, &quot;看守老王&quot;); dbObject.put(&quot;remark&quot;, new BasicDBObject(&quot;address&quot;, &quot;广东省深圳市&quot;).append(&quot;street&quot;, &quot;深南大道888号&quot;)); listdbo.add(dbObject); dbCollection.insert(listdbo); &#125; /** * @Title: addByJson * @Description: TODO json转对象后 ，执行新增 * @param dbCollection * @return: void */ public static void addByJson(DBCollection dbCollection)&#123; String json = &quot;&#123; \&quot;name\&quot; : \&quot;王五\&quot; , \&quot;age\&quot; : 66 , \&quot;job\&quot; : \&quot;看守老王\&quot; , \&quot;remark\&quot; : &#123; \&quot;address\&quot; : \&quot;广东省深圳市\&quot; , \&quot;street\&quot; : \&quot;深南大道888号\&quot;&#125;&#125;&quot;; DBObject dbObject = (DBObject) JSON.parse(json); dbCollection.insert(dbObject); &#125; // ====================================修改开始============================================== /** * @Title: update * @Description: TODO 修改指定记录 * @param dbCollection * @return: void */ public static void updateOne(DBCollection dbCollection) &#123; // 先根据id查询将 这条 记录查询出来 DBObject qryResult = dbCollection.findOne(new ObjectId(&quot;58e4a11c6c166304f0635958&quot;)); // 修改指定的值 qryResult.put(&quot;age&quot;, 55); DBObject olddbObject = new BasicDBObject(); olddbObject.put(&quot;_id&quot;, new ObjectId(&quot;58e4a11c6c166304f0635958&quot;)); dbCollection.update(olddbObject, qryResult); &#125; /** * @Title: updateMulti * @Description: TODO 修改 多条记录 * @param dbCollection * @return: void */ public static void updateMulti(DBCollection dbCollection) &#123; DBObject newdbObject = new BasicDBObject(); newdbObject.put(&quot;name&quot;, &quot;张三&quot;); newdbObject.put(&quot;address&quot;, &quot;广东深圳&quot;); newdbObject.put(&quot;remark&quot;, &quot;张三是一个NB的Coder&quot;); DBObject olddbObject = new BasicDBObject(); olddbObject.put(&quot;name&quot;, &quot;张三&quot;); // 需要加上这个 DBObject upsertValue = new BasicDBObject(&quot;$set&quot;, newdbObject); // 后面的两个参数：1.若所更新的数据没有，则插入 ; 2、同时更新多个符合条件的文档(collection) dbCollection.update(olddbObject, upsertValue, true, true); &#125; // ====================================删除开始============================================== /** * @Title: deleteFirst * @Description: TODO 删除第一个 * @param * @return: void */ public static void deleteFirst(DBCollection dbCollection)&#123; DBObject dbObject = dbCollection.findOne(); dbCollection.remove(dbObject); &#125; /** * @Title: deleteOne * @Description: TODO 删除指定的一条记录 * @param dbCollection * @return: void */ public static void deleteOne(DBCollection dbCollection)&#123; DBObject dbObject = new BasicDBObject(); dbObject.put(&quot;_id&quot;, new ObjectId(&quot;58e49c2d6c166309e0d50484&quot;)); dbCollection.remove(dbObject); &#125; /** * @Title: deleteByIn * @Description: TODO 删除多条记录 例如：select * from tb where name in(&apos;12&apos;,&apos;34&apos;) * @param dbCollection * @return: void */ public static void deleteByIn(DBCollection dbCollection) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;老张&quot;); list.add(&quot;老王&quot;); list.add(&quot;张三&quot;); DBObject dbObject = new BasicDBObject(&quot;$in&quot;, list); DBObject delObject = new BasicDBObject(); delObject.put(&quot;name&quot;, dbObject); dbCollection.remove(delObject); &#125; /** * @Title: deleteAll * @Description: TODO 删除全部的记录 * @param dbCollection * @return: void */ public static void deleteAll(DBCollection dbCollection)&#123; DBCursor cursor = dbCollection.find(); while(cursor.hasNext())&#123; dbCollection.remove(cursor.next()); &#125; &#125;&#125; 三、源码下载]]></content>
  </entry>
  <entry>
    <title><![CDATA[MongoDB（一）-- 简介、安装、CRUD]]></title>
    <url>%2F2019%2F06%2F26%2FMongoDB%EF%BC%88%E4%B8%80%EF%BC%89-%E7%AE%80%E4%BB%8B%E3%80%81%E5%AE%89%E8%A3%85%E3%80%81CRUD%2F</url>
    <content type="text"><![CDATA[一、Mongodb简介MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。 在高负载的情况下，添加更多的节点，可以保证服务器性能。 MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。 MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。 二、主要特点 MongoDB的提供了一个面向文档存储，操作起来比较简单和容易。 你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Sameer”,Address=”8 Gandhi Road”)来实现更快的排序。 你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。 如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。 Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。 MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。 Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。 Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。 Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。 GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。 MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。 MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。 MongoDB安装简单。 三、安装1.下载 根据自己Linux的版本下载对应的mongodb，下载地址：https://www.mongodb.com/download-center#community，下载到 /usr/java 路径下。 2.创建数据库文件路径和数据库日志路径 1) 进入到java路径下，cd /usr/java/ 2) 创建mongodbNode文件夹，mkdir mongodbNode 3) 在mongodbNode下，创建data和log文件夹，其中data是存mongodb数据的文件夹，log是存mongodb日志的文件夹 mkdir data log 4) 进入data文件，在data下创建db文件夹，mongodb会将数据文件写到db文件夹下， mkdir db 3.启动mongodb服务端，有两种方式 1) 命令行方式启动（启动方便），进入到bin目录下（可以通过 ./mongod –help 来查看 mongodb 服务端启动的参数）： ./mongod --dbpath /usr/java/mongoNode/data/db --logpath /usr/java/mongoNode/log/mongodb.log --fork --bind_ip 192.168.242.129 --port 27017 --journal 当看到以下信息的时候，则说明启动成功： 当然，我们也可以通过查看后台进程的方式 检查mongodb是否启动成功：netstat -anp|grep mongod 2) 配置文件方式启动（管理起来规范） a. 在cd /usr/java/mongoNode 下新建一个文件：touch mongodb.conf b. 编辑文件，vi mongodb.conf c. 在mongodb.conf中添加如下内容： 123456dbpath=/usr/java/mongoNode/data/dblogpath=/usr/java/mongoNode/log/mongodb.loglogappend=truefork=truebind_ip=192.168.242.129port=27017 d. 将mongodb设置成环境变量：export PATH=/usr/java/mongodb/bin:$PATH e. 查看环境变量是否生效：echo $PATH f. 启动：mongod --config mongodb.conf 4.启动mongodb客户端 命令行方式：./mongo –host 192.168.242.129 –port 27017 配置文件方式：mongo –host 192.168.242.129 –port 27017 四、配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 日志文件位置logpath=/var/log/mongo/mongod.log# 以追加方式写入日志logappend=true# 是否以守护进程方式运行fork = true# 默认27017#port = 27017# 数据库文件位置dbpath=/var/lib/mongo# 启用定期记录CPU利用率和 I/O 等待#cpu = true# 是否以安全认证方式运行，默认是不认证的非安全方式#noauth = true#auth = true# 详细记录输出#verbose = true# Inspect all client data for validity on receipt (useful for# developing drivers)用于开发驱动程序时验证客户端请求#objcheck = true# Enable db quota management# 启用数据库配额管理#quota = true# 设置oplog记录等级# Set oplogging level where n is# 0=off (default)# 1=W# 2=R# 3=both# 7=W+some reads#diaglog=0# Diagnostic/debugging option 动态调试项#nocursors = true# Ignore query hints 忽略查询提示#nohints = true# 禁用http界面，默认为localhost：28017#nohttpinterface = true# 关闭服务器端脚本，这将极大的限制功能# Turns off server-side scripting. This will result in greatly limited# functionality#noscripting = true# 关闭扫描表，任何查询将会是扫描失败# Turns off table scans. Any query that would do a table scan fails.#notablescan = true# 关闭数据文件预分配# Disable data file preallocation.#noprealloc = true# 为新数据库指定.ns文件的大小，单位:MB# Specify .ns file size for new databases.# nssize = # Replication Options 复制选项# in replicated mongo databases, specify the replica set name here#replSet=setname# maximum size in megabytes for replication operation log#oplogSize=1024# path to a key file storing authentication info for connections# between replica set members#指定存储身份验证信息的密钥文件的路径#keyFile=/path/to/keyfile 五、数据库一个mongodb中可以建立多个数据库。 MongoDB的默认数据库为”db”，该数据库存储在data目录中。 MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。 “show dbs” 命令可以显示所有数据的列表。 运行”use”命令，可以连接到一个指定的数据库。 六、文档文档是一个键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。 需要注意的是： 1、文档中的键/值对是有序的。 2、文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。 3、MongoDB区分类型和大小写。 4、MongoDB的文档不能有重复的键。 5、文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 七、集合集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。 集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。 八、操作（CURD）1.连接到客户端 2.查看数据库：show dbs 3.新建数据库:use testXbq，此时，show dbs不会有刚刚新建的数据库，当向数据库中 新建连接时 或者 直接插入文档 时，才会出现 testXbq数据库。 1) 查看当前使用的数据库： db.getName() 或者 db 2) 显示当前数据库的信息：db.stats() 3) 修复当前使用的数据库：db.repairDatabase() 4) 删除当前使用的数据库：db.dropDatabase() 5) 从指定的机器上复制指定数据库数据到某个数据库：db.copyDatabase(“mydb”, “temp”, “127.0.0.1”); –将本机的mydb的数据复制到temp数据库中 6) 查看当前db的链接机器地址：db.getMongo() 7) 当前db版本：db.version() 8) 查看当前数据库中的所有连接：show collections 4.插入文档，向刚刚创建的连接中 插入文档：db.xbqTable.insert({name:”xbq”,sex:”男”})，也可以使用 save方法 5.查询总的的文档：db.xbqTable.find() ，若想要 格式化查询结果，在find()后面加上 .pretty() 即可，如：db.xbqTable.find().pretty() 6.更新文档，修改刚刚插入的文档：db.xbqTable.update({ “_id” : ObjectId(“58e40c02aee532ffbea0b89d”)},{$set:{name:”徐邦启”}}) multi : 可选，mongodb 默认是false，只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。 重新插入几条文档： 更新name为xbq的文档，并将age更新为 24，db.xbqTable.update({name:”xbq”},{$set:{age:24}},{multi:true}) 7.删除文档，删除name为 徐邦启的文档：db.xbqTable.remove({name:”徐邦启”}) 8.and查询，查询 name 为xbq，并且 address 为 广东深圳 的 文档：db.xbqTable.find({name:”xbq”,address:”广东深圳”}) 9.or查询，查询 name 为joe 或者 name为 java的文档：db.xbqTable.find({$or:[{name:”joe”},{name:”java”}]}) 10.&lt;，&lt;=，&gt; ，&gt;= ，&lt;&gt; 总文档有： 1) 查询age &lt; 15的文档：db.xbqTable.find({age:{$lt:15}}) 2) 查询age &lt;= 15的文档：db.xbqTable.find({age:{$lte:15}}) 3) 查询age&gt;15的文档：db.xbqTable.find({age:{$gt:15}}) 4) 查询年age&gt;=15的文档：db.xbqTable.find({age:{$gte:15}}) 5) 查询age != 15的文档：db.xbqTable.find({age:{$ne:15}}) 11.模糊查询，查询 name 中包含 o 的文档：db.xbqTable.find({name:/o/}); 相当于 select * from xbqTable where name like ‘%o%’; 12.模糊查询，查询name中以 t 开头的文档：db.xbqTable.find({name:/^t/}) 相当于 select * from xbqTable where name like ‘t%’; 13.查询指定列 name 和 sex 的数据：db.xbqTable.find({},{name:1,sex:1}) 或者 db.xbqTable.find({},{name:true,sex:true}) 相当于 select name,sex from xbqTable; 写 0 或者 false ，则为 排除此列，查询 除了此列的 其他数据 14.查询指定列 name 和 sex的数据，并且 age &gt; 16 的文档：db.xbqTable.find({age:{$gt:16}},{name:1,sex:1}) 15.查询去重：db.xbqTable.distinct(“sex”) 相当于：select distinct(sex) from xbqTable; 16.按照age 排序： 升序：db.xbqTable.find().sort({age:1}) 降序：db.xbqTable.find().sort({age:-1}) 17.查询前2条的文档：db.xbqTable.find().limit(2) 18.查询3条后的文档（跳过前3条的文档）：db.xbqTable.find().skip(3) 19.查询在2–4之间的文档（即先跳过前2条，然后再查询2条）（用于分页查询，limit是pageSize，skip是 第几页 * pageSize）： db.xbqTable.find().skip(2).limit(2) 20.查询第1条的文档：db.xbqTable.findOne() 或者 db.xbqTable.find().limit(1) 21.查询 name 为 xbq的文档数目：db.xbqTable.find({name:”xbq”}).count(); 22.分组 九、索引索引通常能够极大的提高查询的效率，如果没有索引，MongoDB在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。 索引是特殊的数据结构，索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构。 建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 “background” 可选参数。 “background” 默认值为false。 例如：db.xbqTable.ensureIndex({name:1},{background:true})，意思是 在name字段上建立一个升序的索引。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis（七）-- Redis分布式锁实现]]></title>
    <url>%2F2019%2F06%2F24%2FRedis%EF%BC%88%E4%B8%83%EF%BC%89-Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[一、使用分布式锁要满足的几个条件 系统是一个分布式系统（关键是分布式，单机的可以使用ReentrantLock或者synchronized代码块来实现） 共享资源（各个系统访问同一个资源，资源的载体可能是传统关系型数据库或者NoSQL） 同步访问（即有很多个进程同事访问同一个共享资源。没有同步访问，谁管你资源竞争不竞争） 二、应用的场景例子管理后台的部署架构（多台tomcat服务器+redis【多台tomcat服务器访问一台redis】+mysql【多台tomcat服务器访问一台服务器上的mysql】）就满足使用分布式锁的条件。多台服务器要访问redis全局缓存的资源，如果不使用分布式锁就会出现问题。 看如下伪代码： 123456long N=0L;//N从redis获取值if(N&lt;5)&#123; N++；//N写回redis&#125; 上面的代码主要实现的功能： 从redis获取值N，对数值N进行边界检查，自加1，然后N写回redis中。 这种应用场景很常见，像秒杀，全局递增ID、IP访问限制等。以IP访问限制来说，恶意攻击者可能发起无限次访问，并发量比较大，分布式环境下对N的边界检查就不可靠，因为从redis读的N可能已经是脏数据。传统的加锁的做法（如java的synchronized和Lock）也没用，因为这是分布式环境，这个同步问题的救火队员也束手无策。在这危急存亡之秋，分布式锁终于有用武之地了。 分布式锁可以基于很多种方式实现，比如zookeeper、redis…。不管哪种方式，他的基本原理是不变的：用一个状态值表示锁，对锁的占用和释放通过状态值来标识。 三、使用redis的setNX命令实现分布式锁 1、实现的原理 Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系。redis的SETNX命令可以方便的实现分布式锁。 2、基本命令解析 1）setNX（SET if Not eXists） 语法： 1SETNX key value 将 key 的值设为 value ，当且仅当 key 不存在。 若给定的 key 已经存在，则 SETNX 不做任何动作。 SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写 返回值： 设置成功，返回 1 。 设置失败，返回 0 。 例子： 1234567891011redis&gt; EXISTS job # job 不存在(integer) 0redis&gt; SETNX job &quot;programmer&quot; # job 设置成功(integer) 1redis&gt; SETNX job &quot;code-farmer&quot; # 尝试覆盖 job ，失败(integer) 0redis&gt; GET job # 没有被覆盖&quot;programmer&quot; 所以我们使用执行下面的命令 1SETNX lock.foo &lt;current Unix time + lock timeout + 1&gt; 如返回1，则该客户端获得锁，把lock.foo的键值设置为时间值表示该键已被锁定，该客户端最后可以通过DEL lock.foo来释放该锁。 如返回0，表明该锁已被其他客户端取得，这时我们可以先返回或进行重试等对方完成或等待锁超时。 2）getSET 语法： 1GETSET key value 将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 当 key 存在但不是字符串类型时，返回一个错误。 返回值： 返回给定 key 的旧值。 当 key 没有旧值时，也即是， key 不存在时，返回 nil 。 3）get 语法： 1GET key 返回值： 当 key 不存在时，返回 nil ，否则，返回 key 的值。 如果 key 不是字符串类型，那么返回一个错误 四、解决死锁上面的锁定逻辑有一个问题：如果一个持有锁的客户端失败或崩溃了不能释放锁，该怎么解决？ 1我们可以通过锁的键对应的时间戳来判断这种情况是否发生了，如果当前的时间已经大于lock.foo的值，说明该锁已失效，可以被重新使用。 发生这种情况时，可不能简单的通过DEL来删除锁，然后再SETNX一次（讲道理，删除锁的操作应该是锁拥有这执行的，这里只需要等它超时即可），当多个客户端检测到锁超时后都会尝试去释放它，这里就可能出现一个竞态条件,让我们模拟一下这个场景： 123456C0操作超时了，但它还持有着锁，C1和C2读取lock.foo检查时间戳，先后发现超时了。 C1 发送DEL lock.foo C1 发送SETNX lock.foo 并且成功了。 C2 发送DEL lock.foo C2 发送SETNX lock.foo 并且成功了。 这样一来，C1，C2都拿到了锁！问题大了！ 幸好这种问题是可以避免的，让我们来看看C3这个客户端是怎样做的： 123456C3发送SETNX lock.foo 想要获得锁，由于C0还持有锁，所以Redis返回给C3一个0 C3发送GET lock.foo 以检查锁是否超时了，如果没超时，则等待或重试。 反之，如果已超时，C3通过下面的操作来尝试获得锁： GETSET lock.foo &lt;current Unix time + lock timeout + 1&gt; 通过GETSET，C3拿到的时间戳如果仍然是超时的，那就说明，C3如愿以偿拿到锁了。 如果在C3之前，有个叫C4的客户端比C3快一步执行了上面的操作，那么C3拿到的时间戳是个未超时的值，这时，C3没有如期获得锁，需要再次等待或重试。留意一下，尽管C3没拿到锁，但它改写了C4设置的锁的超时值，不过这一点非常微小的误差带来的影响可以忽略不计。 注意：为了让分布式锁的算法更稳键些，持有锁的客户端在解锁之前应该再检查一次自己的锁是否已经超时，再去做DEL操作，因为可能客户端因为某个耗时的操作而挂起，操作完的时候锁因为超时已经被别人获得，这时就不必解锁了。 五、代码实现expireTimeMsg 锁过期时间，防止线程在入锁之后 无限等待waitTimeMsg 锁等待时间（或者 叫 尝试获得锁的时间），防止线程饥饿 1.Jedis工具类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package com.xbq.redis;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;/** * Jedis工具类 * @author xbq * @created：2017-4-19 */public class JedisUtil &#123; private JedisPool pool; private static String URL = &quot;192.168.242.130&quot;; private static int PORT = 6379; private static String PASSWORD = &quot;xbq123&quot;; // ThreadLocal，给每个线程 都弄一份 自己的资源 private final static ThreadLocal&lt;JedisPool&gt; threadPool = new ThreadLocal&lt;JedisPool&gt;(); private final static ThreadLocal&lt;Jedis&gt; threadJedis = new ThreadLocal&lt;Jedis&gt;(); private final static int MAX_TOTAL = 100; // 最大分配实例 private final static int MAX_IDLE = 50; // 最大空闲数 private final static int MAX_WAIT_MILLIS = -1; // 最大等待数 /** * 获取 jedis池 * @return */ public JedisPool getPool()&#123; JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); // 控制一个pool可分配多少个jedis实例，通过pool.getResource()来获取，如果赋值为-1，则表示不限制； // 如果pool已经分配了maxActive个jedis实例，则此时pool的状态为exhausted(耗尽) jedisPoolConfig.setMaxTotal(MAX_TOTAL); // 控制一个pool最多有多少个状态为idle(空闲的)的jedis实例 jedisPoolConfig.setMaxIdle(MAX_IDLE); // 表示当borrow(引入)一个jedis实例时，最大的等待时间，如果超过等待时间，则直接抛出JedisConnectionException jedisPoolConfig.setMaxWaitMillis(MAX_WAIT_MILLIS); final int timeout = 60 * 1000; pool = new JedisPool(jedisPoolConfig, URL, PORT, timeout); return pool; &#125; /** * 在jedis池中 获取 jedis * @return */ public Jedis common()&#123; // 从 threadPool中取出 jedis连接池 pool = threadPool.get(); // 为空，则重新产生 jedis连接池 if(pool == null)&#123; pool = this.getPool(); // 将jedis连接池维护到threadPool中 threadPool.set(pool); &#125; // 在threadJedis中获取jedis实例 Jedis jedis = threadJedis.get(); // 为空，则在jedis连接池中取出一个 if(jedis == null)&#123; jedis = pool.getResource(); // 验证密码 jedis.auth(PASSWORD); // 将jedis实例维护到threadJedis中 threadJedis.set(jedis); &#125; return jedis; &#125; /** * 释放资源 */ public void closeAll()&#123; Jedis jedis = threadJedis.get(); if(jedis != null)&#123; threadJedis.set(null); JedisPool pool = threadPool.get(); if(pool != null)&#123; // 释放连接，归还给连接池 pool.returnResource(jedis); &#125; &#125; &#125;&#125; 2.分布式锁实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146package com.xbq.redis;import org.apache.log4j.Logger;import redis.clients.jedis.Jedis;/** * Redis实现分布式锁 * @author xbq */public class RedisLock &#123; private static final Logger logger = Logger.getLogger(RedisLock.class); // 获取jedis实例 private Jedis jedis; // 锁 的key private String lockKey; // 锁过期时间，防止线程在入锁之后 无限等待 private int expireTimeMsg = 60 * 1000; // 锁等待时间（或者 叫 尝试获得锁的时间），防止线程饥饿 private int waitTimeMsg = 10 * 1000; // 系统时间偏移量5秒，服务器间的系统时间差不可以超过5秒，避免由于时间差造成错误的解锁 private final static int offsetTime = 5 * 1000; // 用毫秒表示 // 默认减去的时间 private static final int DEFAULT_ACQUIRY_RESOLUTION_MILLIS = 100; // 锁状态 private volatile boolean lock = false; public RedisLock(Jedis jedis, String lockKey)&#123; this.jedis = jedis; this.lockKey = lockKey + &quot;_lock&quot;; &#125; public RedisLock(Jedis jedis, String lockKey, int waitTimeMsg)&#123; this(jedis, lockKey); this.waitTimeMsg = waitTimeMsg; &#125; public RedisLock(Jedis jedis, String lockKey, int waitTimeMsg, int expireTimeMsg)&#123; this(jedis, lockKey, waitTimeMsg); this.expireTimeMsg = expireTimeMsg; &#125; /** * 获取 锁 的key * @return */ public String getLockKey()&#123; return lockKey; &#125; /** * 获取 key 对应的value * @param key * @return */ private String get(String key)&#123; return jedis.get(key); &#125; /** * 设置 key value，不存在 key，设置值 成功，返回1；存在key，设置值 失败，返回0 * @param key * @param value * @return */ private long setNx(String key, String value)&#123; return jedis.setnx(key, value); &#125; /** * 获取旧值，设置 新的 值 * @param key * @param value * @return */ private String getSet(String key, String value)&#123; return jedis.getSet(key, value); &#125; /** * 获取锁 * 实现思路: 主要是使用了redis 的setnx命令,缓存了锁 * reids缓存的key是锁的key,所有的共享, value是锁的到期时间(注意:这里把过期时间放在value了,没有时间上设置其超时时间) * 执行过程: * 1.通过setnx尝试设置某个key的值,成功(当前没有这个锁)则返回,成功获得锁 * 2.锁已经存在则获取锁的到期时间,和当前时间比较,超时的话,则设置新的值 * @return * @throws InterruptedException */ public boolean lock() throws InterruptedException&#123; int waitTime = waitTimeMsg; // 循环为了多次争夺锁 while (waitTime &gt;= 0) &#123; // 过期时间 long exquires = System.currentTimeMillis() + expireTimeMsg + 1; logger.info(Thread.currentThread().getName() + &quot;尝试获取锁！&quot;); // 得到了 锁 if(this.setNx(lockKey, String.valueOf(exquires)) == 1)&#123; logger.info(Thread.currentThread().getName() + &quot;获得了锁，锁 过期时间为：&quot; + exquires); lock = true; return true; &#125; // 存在原来的锁，就获取原来锁的过期时间 String lastLockTime = this.get(lockKey); // 判断redis中的时间是否为空，获取出的 时间 过期了，则进行下面操作 if(lastLockTime != null &amp;&amp; System.currentTimeMillis() - Long.valueOf(lastLockTime) &gt; (expireTimeMsg + offsetTime))&#123; // 获取上一个锁的过期时间，并设置现在的锁的过期时间（只有一个线程才能获取上一个线程的设置时间，因为jedis.getSet是同步的） String oldValue = this.getSet(lockKey, String.valueOf(exquires)); // 防止误删（覆盖，因为key是相同的）了他人的锁——这里达不到效果，这里值会被覆盖，但是因为相差了很少的时间，所以可以接受 if(oldValue != null &amp;&amp; oldValue.equals(lastLockTime))&#123; // [分布式的情况下]:如果这个时候，多个线程恰好都到了这里，但是只有一个线程的设置值和当前值相同，他才有权利获取锁 logger.info(&quot;------&quot; + Thread.currentThread().getName() + &quot;获得了锁！------&quot;); lock = true; return true; &#125; &#125; // 循环一次减去一次 waitTime = waitTime - DEFAULT_ACQUIRY_RESOLUTION_MILLIS; // 使用随机的等待时间可以一定程度上保证公平性 Thread.sleep((long)(Math.random() * 100)); &#125; logger.error(&quot;--------&quot; + Thread.currentThread().getName() + &quot;获取锁失败！！&quot;); return false; &#125; /** * 释放锁 */ public void unLock()&#123; // 判断加锁了，才进行删除操作 if(lock)&#123; jedis.del(lockKey); System.out.println(Thread.currentThread().getName() + &quot;解锁成功！--------------&quot;); // 恢复默认值 lock = false; &#125; &#125; &#125; 3.模拟并发测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.xbq.redis;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Semaphore;import redis.clients.jedis.Jedis;/** * 模拟并发环境下 获取锁 * @author xbq */public class Main &#123; public static void main(String[] args) &#123; // 定义线程池 ExecutorService service = Executors.newCachedThreadPool(); // 只能有10个线程同时访问，用来模拟并发 final Semaphore semaphore = new Semaphore(10); // 模拟20个客户端访问 for (int i = 0; i &lt; 20; i++) &#123; Runnable runnable = new Runnable() &#123; String lockKey = &quot;TestLock33&quot;; @Override public void run() &#123; try &#123; // 获取许可 semaphore.acquire(); // 获取jedis实例 Jedis jedis = new JedisUtil().common(); RedisLock redisLock = new RedisLock(jedis, lockKey, 10000); if(redisLock.lock())&#123; // 获取到了锁，然后进行 业务处理 // 业务代码 Thread.sleep(3000); &#125; // 释放锁 redisLock.unLock(); // 访问完后，释放 ，如果屏蔽下面的语句，则在控制台只能打印5条记录，之后线程一直阻塞 semaphore.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; // 执行线程 service.execute(runnable); &#125; // 退出线程池 service.shutdown(); &#125;&#125; 六、一些问题1、为什么不直接使用expire设置超时时间，而将时间的毫秒数其作为value放在redis中？ 如下面的方式，把超时的交给redis处理： 12345lock(key, expireSec)&#123; isSuccess = setnx keyif (isSuccess) expire key expireSec&#125; 这种方式貌似没什么问题，但是假如在setnx后，redis崩溃了，expire就没有执行，结果就是死锁了。锁永远不会超时。 2、为什么前面的锁已经超时了，还要用getSet去设置新的时间戳的时间获取旧的值，然后和外面的判断超时时间的时间戳比较呢？ 因为是分布式的环境下，可以在前一个锁失效的时候，有两个进程进入到锁超时的判断。如： C0超时了，还持有锁,C1/C2同时请求进入了方法里面 C1/C2获取到了C0的超时时间 C1使用getSet方法 C2也执行了getSet方法 假如我们不加 oldValueStr.equals(currentValueStr) 的判断，将会C1/C2都将获得锁，加了之后，能保证C1和C2只能一个能获得锁，一个只能继续等待。 注意：这里可能导致超时时间不是其原本的超时时间，C1的超时时间可能被C2覆盖了，但是他们相差的毫秒及其小，这里忽略了。 七、源码下载https://gitee.com/xbq168/DistributedLockByRedis]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis（六）-- SpringMVC整合Redis]]></title>
    <url>%2F2019%2F06%2F19%2FRedis%EF%BC%88%E5%85%AD%EF%BC%89-SpringMVC%E6%95%B4%E5%90%88Redis%2F</url>
    <content type="text"><![CDATA[一、pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.xbq.demo&lt;/groupId&gt; &lt;artifactId&gt;SpringRedisDemo&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;SpringRedisDemo Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;!-- Spring版本号 --&gt; &lt;properties&gt; &lt;org.springframework.version&gt;4.2.3.RELEASE&lt;/org.springframework.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.16&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;1.8.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;org.springframework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;org.springframework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;org.springframework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;org.springframework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;$&#123;org.springframework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;org.springframework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;$&#123;org.springframework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;org.springframework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;SpringRedisDemo&lt;/finalName&gt; &lt;/build&gt;&lt;/project&gt; 二、applicaltionContext.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:context="http://www.springframework.org/schema/context" xmlns:jee="http://www.springframework.org/schema/jee" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:cache="http://www.springframework.org/schema/cache" xsi:schemaLocation=" http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd "&gt; &lt;!-- 自动扫描 --&gt; &lt;context:component-scan base-package="com.xbq.demo.util"/&gt; &lt;!-- 加载文件 --&gt; &lt;context:property-placeholder location="classpath:redis.properties" /&gt; &lt;!-- jedis 配置 --&gt; &lt;bean id="poolConfig" class="redis.clients.jedis.JedisPoolConfig"&gt; &lt;property name="maxIdle" value="$&#123;redis.maxIdle&#125;" /&gt; &lt;property name="minIdle" value="$&#123;redis.minIdle&#125;" /&gt; &lt;property name="maxWaitMillis" value="$&#123;redis.maxWait&#125;" /&gt; &lt;property name="testOnBorrow" value="$&#123;redis.testOnBorrow&#125;" /&gt; &lt;/bean&gt; &lt;!-- redis服务器中心 --&gt; &lt;bean id="connectionFactory" class="org.springframework.data.redis.connection.jedis.JedisConnectionFactory"&gt; &lt;property name="poolConfig" ref="poolConfig" /&gt; &lt;property name="port" value="$&#123;redis.port&#125;" /&gt; &lt;property name="hostName" value="$&#123;redis.host&#125;" /&gt; &lt;property name="password" value="$&#123;redis.pass&#125;" /&gt; &lt;property name="timeout" value="$&#123;redis.timeout&#125;" /&gt; &lt;/bean&gt; &lt;!-- redis操作模板，面向对象的模板 --&gt; &lt;bean id="redisTemplate" class="org.springframework.data.redis.core.StringRedisTemplate"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;!-- 如果不配置Serializer，那么存储的时候只能使用String，如果用对象类型存储，那么会提示错误 --&gt; &lt;property name="keySerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.StringRedisSerializer" /&gt; &lt;/property&gt; &lt;property name="valueSerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.JdkSerializationRedisSerializer" /&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 三、redis.properties123456789redis.host=192.168.242.130redis.port=6379redis.pass=xbq123redis.timeout=-1 redis.maxIdle=100redis.minIdle=8redis.maxWait=-1redis.testOnBorrow=true 四、spring-mvc.xml123456789101112131415161718192021222324252627282930313233&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:jee=&quot;http://www.springframework.org/schema/jee&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd&quot;&gt; &lt;!-- 视图解析器 --&gt; &lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/&quot; /&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;property name=&quot;defaultEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;10000000&quot;/&gt; &lt;/bean&gt; &lt;!-- 使用注解的包，包括子集 --&gt; &lt;context:component-scan base-package=&quot;com.xbq.demo.controller&quot;/&gt;&lt;/beans&gt; 五、web.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:web=&quot;http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; id=&quot;WebApp_ID&quot; version=&quot;3.0&quot;&gt; &lt;display-name&gt;SpringDemoTest&lt;/display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;!-- Spring配置文件 --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; classpath*:applicationContext.xml, &lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- 编码过滤器 --&gt; &lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!--spring监听器--&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 防止spring内存溢出监听器--&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.util.IntrospectorCleanupListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 添加对springmvc的支持 --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springMVC&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 六、封装一个操作hash基本数据类型的工具类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package com.xbq.demo.util;import javax.annotation.Resource;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.stereotype.Component;/** * 操作 hash 的基本操作 * @author xbq */@Component(&quot;redisCache&quot;)public class RedisCacheUtil &#123; @Resource private StringRedisTemplate redisTemplate; /** * 向Hash中添加值 * @param key 可以对应数据库中的表名 * @param field 可以对应数据库表中的唯一索引 * @param value 存入redis中的值 */ public void hset(String key, String field, String value) &#123; if(key == null || &quot;&quot;.equals(key))&#123; return ; &#125; redisTemplate.opsForHash().put(key, field, value); &#125; /** * 从redis中取出值 * @param key * @param field * @return */ public String hget(String key, String field)&#123; if(key == null || &quot;&quot;.equals(key))&#123; return null; &#125; return (String) redisTemplate.opsForHash().get(key, field); &#125; /** * 判断 是否存在 key 以及 hash key * @param key * @param field * @return */ public boolean hexists(String key, String field)&#123; if(key == null || &quot;&quot;.equals(key))&#123; return false; &#125; return redisTemplate.opsForHash().hasKey(key, field); &#125; /** * 查询 key中对应多少条数据 * @param key * @return */ public long hsize(String key) &#123; if(key == null || &quot;&quot;.equals(key))&#123; return 0L; &#125; return redisTemplate.opsForHash().size(key); &#125; /** * 删除 * @param key * @param field */ public void hdel(String key, String field) &#123; if(key == null || &quot;&quot;.equals(key))&#123; return; &#125; redisTemplate.opsForHash().delete(key, field); &#125;&#125; 七、测试类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.xbq.demo;import org.junit.Before;import org.junit.Test;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.xbq.demo.util.RedisCacheUtil;/** * 测试 */public class RedisTest &#123; private RedisCacheUtil redisCache; private static String key; private static String field; private static String value; @Before public void setUp() throws Exception &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); context.start(); redisCache = (RedisCacheUtil) context.getBean(&quot;redisCache&quot;); &#125; // 初始化 数据 static &#123; key = &quot;tb_student&quot;; field = &quot;stu_name&quot;; value = &quot;一系列的关于student的信息！&quot;; &#125; // 测试增加数据 @Test public void testHset() &#123; redisCache.hset(key, field, value); System.out.println(&quot;数据保存成功！&quot;); &#125; // 测试查询数据 @Test public void testHget()&#123; String re = redisCache.hget(key, field); System.out.println(&quot;得到的数据：&quot; + re); &#125; // 测试数据的数量 @Test public void testHsize()&#123; long size = redisCache.hsize(key); System.out.println(&quot;数量为：&quot; + size); &#125;&#125; 八、控制层 StudentController1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.xbq.demo.controller;import java.io.PrintWriter;import javax.annotation.Resource;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import com.xbq.demo.util.RedisCacheUtil;/** * 控制层 * @author xbq */@Controller@RequestMapping(&quot;/student/&quot;)public class StudentController &#123; @Resource private RedisCacheUtil redisCache; // 查询数据 @RequestMapping(&quot;list&quot;) public String list(HttpServletResponse response, HttpServletRequest request)&#123; String re = redisCache.hget(&quot;tb_student&quot;, &quot;stu_id&quot;); try &#123; this.write(response, re); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 回写到页面上 * @param response * @param o * @throws Exception */ private void write(HttpServletResponse response, Object o) throws Exception&#123; response.setContentType(&quot;text/html;charset=utf-8&quot;); PrintWriter out=response.getWriter(); out.println(o.toString()); out.flush(); out.close(); &#125;&#125; 十、源码下载点击阅读原文可以下载源码哦~~]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis（五）-- Java API]]></title>
    <url>%2F2019%2F06%2F18%2FRedis%EF%BC%88%E4%BA%94%EF%BC%89-Java-API%2F</url>
    <content type="text"><![CDATA[一、pox.xml123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 二、Java代码，Jedis工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326package om.xbq.redis;import java.util.List;import java.util.Set;import org.junit.Test;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;/** * Jedis工具类 * @author xbq * @created：2017-4-19 */public class JedisUtil &#123; private JedisPool pool; private static String URL = "192.168.242.130"; private static int PORT = 6379; private static String PASSWORD = "xbq123"; // ThreadLocal，给每个线程 都弄一份 自己的资源 private final static ThreadLocal&lt;JedisPool&gt; threadPool = new ThreadLocal&lt;JedisPool&gt;(); private final static ThreadLocal&lt;Jedis&gt; threadJedis = new ThreadLocal&lt;Jedis&gt;(); private final static int MAX_TOTAL = 100; // 最大分配实例 private final static int MAX_IDLE = 50; // 最大空闲数 private final static int MAX_WAIT_MILLIS = -1; // 最大等待数 /** * 获取 jedis池 * @return */ public JedisPool getPool()&#123; JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); // 控制一个pool可分配多少个jedis实例，通过pool.getResource()来获取，如果赋值为-1，则表示不限制； // 如果pool已经分配了maxActive个jedis实例，则此时pool的状态为exhausted(耗尽) jedisPoolConfig.setMaxTotal(MAX_TOTAL); // 控制一个pool最多有多少个状态为idle(空闲的)的jedis实例 jedisPoolConfig.setMaxIdle(MAX_IDLE); // 表示当borrow(引入)一个jedis实例时，最大的等待时间，如果超过等待时间，则直接抛出JedisConnectionException jedisPoolConfig.setMaxWaitMillis(MAX_WAIT_MILLIS); final int timeout = 60 * 1000; pool = new JedisPool(jedisPoolConfig, URL, PORT, timeout); return pool; &#125; /** * 在jedis池中 获取 jedis * @return */ private Jedis common()&#123; // 从 threadPool中取出 jedis连接池 pool = threadPool.get(); // 为空，则重新产生 jedis连接池 if(pool == null)&#123; pool = this.getPool(); // 将jedis连接池维护到threadPool中 threadPool.set(pool); &#125; // 在threadJedis中获取jedis实例 Jedis jedis = threadJedis.get(); // 为空，则在jedis连接池中取出一个 if(jedis == null)&#123; jedis = pool.getResource(); // 验证密码 jedis.auth(PASSWORD); // 将jedis实例维护到threadJedis中 threadJedis.set(jedis); &#125; return jedis; &#125; /** * 释放资源 */ public void closeAll()&#123; Jedis jedis = threadJedis.get(); if(jedis != null)&#123; threadJedis.set(null); JedisPool pool = threadPool.get(); if(pool != null)&#123; // 释放连接，归还给连接池 pool.returnResource(jedis); &#125; &#125; &#125; /** * 判断key是否存在 * @param key * @return */ public boolean existsKey(String key)&#123; Jedis jedis = this.common(); return jedis.exists(key); &#125; /** * 删除 * @param key * @return */ public Long delValue(String key)&#123; Jedis jedis = this.common(); return jedis.del(key); &#125; // ----------------------------对String类型的操作----------------------------------------- /** * 增加 修改 * @param key * @param value */ public String setValue(String key, String value) &#123; Jedis jedis = this.common(); return jedis.set(key, value); &#125; /** * 查询 * @param key * @return */ public String getValue(String key)&#123; Jedis jedis = this.common(); return jedis.get(key); &#125; /** * 追加数据 * @param key * @param value */ public void appendValue(String key, String value)&#123; Jedis jedis = this.common(); jedis.append(key, value); &#125; /** * 测试 String */ @Test public void testString()&#123; if(this.existsKey("name"))&#123; System.out.println("这一个key存在了！"); this.appendValue("name", "xbq6666"); String name = this.getValue("name"); System.out.println("name===" + name); long flag = this.delValue("name"); System.out.println(flag); &#125;else &#123; this.setValue("name", "javaCoder"); String name = this.getValue("name"); System.out.println("name===" + name); &#125; &#125; // ----------------------------对List类型的操作------------------------------------------ /** * 保存到链表 * @param key * @param keys * @return */ public long lpush(String key, String ...keys)&#123; Jedis jedis = this.common(); return jedis.lpush(key, keys); &#125; /** * 取出链表中的全部元素 * @param key * @return */ public List&lt;String&gt; lrange(String key) &#123; Jedis jedis = this.common(); return jedis.lrange(key, 0, -1); &#125; /** * 查询出链表中的元素个数 * @param key * @return */ public long llen(String key)&#123; Jedis jedis = this.common(); return jedis.llen(key); &#125; /** * 取出链表中的头部元素 * @param key * @return */ public String lpop(String key)&#123; Jedis jedis = this.common(); return jedis.lpop(key); &#125; // ----------------------------对Hash类型的操作------------------------------------------ /** * 添加 * @param key * @param field * @param value * @return */ public long hset(String key, String field, String value) &#123; Jedis jedis = this.common(); return jedis.hset(key, field, value); &#125; /** * 查询 * @param key * @param field * @return */ public String hget(String key, String field)&#123; Jedis jedis = this.common(); return jedis.hget(key, field); &#125; /** * 判断 key 中的field 是否存在 * @param key * @param field * @return */ public boolean hexists(String key, String field)&#123; Jedis jedis = this.common(); return jedis.hexists(key, field); &#125; /** * 删除 * @param key * @param fields * @return */ public long hdel(String key, String ...fields)&#123; Jedis jedis = this.common(); return jedis.hdel(key, fields); &#125; // ----------------------------对Set类型的操作-------------------------------------------- /** * 添加元素 * @param key * @param members * @return */ public long sadd(String key, String ...members)&#123; Jedis jedis = this.common(); return jedis.sadd(key, members); &#125; /** * 查询出set中的所有元素 * @param key * @return */ public Set&lt;String&gt; sMember(String key)&#123; Jedis jedis = this.common(); return jedis.smembers(key); &#125; /** * 查询出 set中元素的个数 * @param key * @return */ public long scard(String key)&#123; Jedis jedis = this.common(); return jedis.scard(key); &#125; // ----------------------------对ZSet类型的操作-------------------------------------------- /** * 在zset中添加元素 * @param key * @param score * @param member * @return */ public long zadd(String key, double score ,String member)&#123; Jedis jedis = this.common(); return jedis.zadd(key, score, member); &#125; /** * 查询所有元素 * @param key * @return */ public Set&lt;String&gt; zrange(String key)&#123; Jedis jedis = this.common(); return jedis.zrange(key, 0, -1); &#125; /** * 查询zset中的元素个数 * @param key * @return */ public long zcard(String key)&#123; Jedis jedis = this.common(); return jedis.zcard(key); &#125; /** * 删除zset中的一个 或者多个元素 * @param key * @param members * @return */ public long zrem(String key, String ...members)&#123; Jedis jedis = this.common(); return jedis.zrem(key, members); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis（四）-- 集群]]></title>
    <url>%2F2019%2F06%2F17%2FRedis%EF%BC%88%E5%9B%9B%EF%BC%89-%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[一、Redis适合做企业级分布式缓存集群的条件1.Redis内置哈希槽，有16384个哈希槽（0~16383），根据CRC16算法来确定这个集群中属于哪一个服务器来处理这个请求。 2.Redis提供一个集群管理工具：redis-trib，调度这些集群搭建，将这个集群中的每一组redis服务器联系起来。 二、Redis集群原理 Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。集群的每个节点负责一部分hash槽。这种结构很容易添加或者删除节点，并且无论是添加删除或者修改某一个节点，都不会造成集群不可用的状态。使用哈希槽的好处就在于可以方便的添加或移除节点。当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了；当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了；在这一点上，我们以后新增或移除节点的时候不用先停掉所有的 redis 服务。 三、集群搭建1.在/usr/java中新建cluster文件夹，然后在cluster文件夹中新建 1000 1001 2000 2001 3000 3001文件夹 1234cd /usr/java mkdir clustercd clustermkdir 1000 1001 2000 2001 3000 3001 2.将redis.conf分别拷贝到1000 1001 2000 2001 3000 3001文件夹中 3.修改redis.conf文件中的 123456789port 1000 //端口1000 1001 2000 2001 3000 3001bind 本机ip //默认ip为127.0.0.1，需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群daemonize yes //redis后台运行pidfile /usr/java/cluster/1000/redis.pid //pidfile文件的位置，1000文件夹可以更换logfile /usr/java/cluster/1000/redis.log //日志位置,1000文件夹可以更换cluster-enabled yes //开启集群 cluster-config-file /usr/java/cluster/1000/nodes.conf //集群的配置 cluster-node-timeout 15000 //请求超时 默认15秒，可自行设置appendonly yes //aof日志开启 有需要就开启，它会每次写操作都记录一条日志 4.安装ruby，都说必须要安装ruby2.0以上的，2.0以上自带rubygems包管理器，但我这里安装的是1.8.7，一样也可以。 1)yum install ruby2)yum install rubygems 安装rubygems包管理器3)gem install redis gem命令安装redis包，增加redis-trib.rb调用redis的接口包 5.将redis中的src文件夹下的 redis-trib.rb 拷贝到 /usr/local/bin 中，启动redis集群的时候，就不需要 进入到redis/src目录中了 6.启动redis集群， redis-trib.rb create –replicas 1 192.168.242.130:1000 192.168.242.130:2000 192.168.242.130:3000 192.168.242.130:1001 192.168.242.130:2001 192.168.242.130:3001. 其中，–replicas 1 设置从节点个数，前三个是master，后三个是slave. 7.查看redis进程，ps -ef|grep redis 8.客户端连接，这里连接redis集群 要使用 redis-cli -c -h –p，其中 -c代表集群模式 redis-cli -c -h 192.168.242.130 -p 1000 9.查看集群是否成功：cluster info 10.当在1000中set值后，会看到自动切换到 3000，这是因为 刚刚 set的数据 落到了 3000中。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis（三）-- 主从同步]]></title>
    <url>%2F2019%2F06%2F17%2FRedis%EF%BC%88%E4%B8%89%EF%BC%89-%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[一、主从同步的工作原理redis主从复制过程： 当配置好slave后，slave与master建立连接，然后发送sync命令。无论是第一次连接还是重新连接，master都会启动一个后台进程，将 数据库快照保存到文件中，同时master主进程会开始收集新的写命令并缓存。后台进程完成写文件后，master就发送文件给slave，slave将 文件保存到硬盘上，再加载到内存中，接着master就会把缓存的命令转发给slave，后续master将收到的写命令发送给slave。如果master同时收到多个slave发来的同步连接命令，master只会启动一个进程来写数据库镜像，然后发送给所有的slave。master同步数据时是非阻塞式的，可以接收用户的读写请求。然而在slave端是阻塞模式的，slave在同步master数据时，并不能够响应客户端的查询。 可以在master禁用数据持久化，只需要注释掉master 配置文件中的所有save配置，然后只在slave上配置数据持久化。 如果Master和Slave之间的链接出现断连现象，Slave可以自动重连Master，但是在连接成功之后，一次完全同步将被自动执行。 slave连接到master slave发送SYNC命令 master服务器备份数据库到.rdb文件 master服务器把.rdb文件传输给slave服务器 slave服务器把.rdb文件数据导入到数据库 上面的这5步是同步的第一阶段,接下来在master服务器上调用每个命令都在使用replicationFeedSlaves()来同步到slave服务器。 二、配置主从同步1.新建文件夹master-slave，在master-slave中新建master和slave，将redis.conf到master和slave文件夹中。 2.修改master和slave的配置文件：端口号、进程文件和日志文件的位置。 3.修改slave的配置文件： 123slaveof &lt;masterip&gt; &lt;masterport&gt; # 指定master的ip和portmasterauth &lt;master-password&gt; # master有验证的情况下slave-read-only yes # 设置slave为只读模式，默认的 4.分别进入到master和slave文件夹，启动 redis服务：redis-server redis.conf 5.客户端连接 1) 连接master：redis-cli -h 192.168.99.207 -p 6379 查看master的信息，输入 info，看到角色是master 2) 连接slave：redis-cli -h 192.168.99.207 -p 6380 查看slave的信息，输入info，看到的角色是slave，且 master_link_status:up master_repl_offset 和 slave_repl_offset相等 master_last_io_seconds_ago 在 10 秒内，则说明配置成功 6.测试 1) 在master中添加数据：set name xbq 2) 在slave中查询数据：get name，结果和在master中插入的数据一样。 3) 测试下 在slave中添加数据：set address shenzheng，发现报错，(error) READONLY You can’t write against a read only slave. 这是因为 我们设置了 slave为只读，不可以操作写。 三、主从同步的应用1.用于对 数据的热备份 2.用于读写分离]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis（二）-- 发布订阅、事务、安全、持久化]]></title>
    <url>%2F2019%2F06%2F14%2FRedis%EF%BC%88%E4%BA%8C%EF%BC%89-%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%E3%80%81%E4%BA%8B%E5%8A%A1%E3%80%81%E5%AE%89%E5%85%A8%E3%80%81%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[一、Redis发布订阅Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。 打开两个窗口：session1 和 session2 在session1中订阅消息： ​ subscribe xbqChannel 客户端订阅消息，xbqChannel 为相应的频道 在session2中发布消息： ​ publish xbqChannel testMessge 发布消息，同时订阅该频道的客户端能收到该消息 二、Redis事务和众多其它数据库一样，Redis作为NoSQL数据库也同样提供了事务机制。在Redis中，MULTI/EXEC/DISCARD/WATCH这四个命令是我们实现事务的基石。 Redis 事务带有以下重要的特征： 在事务中的所有命令都将会被串行化的顺序执行，事务执行期间，Redis不会再为其它客户端的请求提供任何服务，从而保证了事物中的所有命令被原子的执行。 和关系型数据库中的事务相比，在Redis事务中如果有某一条命令执行失败，其后的命令仍然会被继续执行。 我们可以通过MULTI命令开启一个事务，有关系型数据库开发经验的人可以将其理解为”BEGIN TRANSACTION”语句。在该语句之后执行的命令都将被视为事务之内的操作，最后我们可以通过执行EXEC/DISCARD命令来提交/回滚该事务内的所有操作。这两个Redis命令可被视为等同于关系型数据库中的COMMIT/ROLLBACK语句。 在事务开启之前，如果客户端与服务器之间出现通讯故障并导致网络断开，其后所有待执行的语句都将不会被服务器执行。然而如果网络中断事件是发生在客户端执行EXEC命令之后，那么该事务中的所有命令都会被服务器执行。 当使用Append-Only模式时，Redis会通过调用系统函数write将该事务内的所有写操作在本次调用中全部写入磁盘。然而如果在写入的过程中出现系统崩溃，如电源故障导致的宕机，那么此时也许只有部分数据被写入到磁盘，而另外一部分数据却已经丢失。Redis服务器会在重新启动时执行一系列必要的一致性检测，一旦发现类似问题，就会立即退出并给出相应的错误提示。此时，我们就要充分利用Redis工具包中提供的redis-check-aof工具，该工具可以帮助我们定位到数据不一致的错误，并将已经写入的部分数据进行回滚。修复之后我们就可以再次重新启动Redis服务器了。 一个事务从开始到执行会经历以下三个阶段：开始事务、命令入队、执行事务。 三、安全1.查看redis的密码：config get requirepass 2.为redis设置密码的方法： 在redis.conf中进行配置：requirepass xbqpass 通过命令行进行设置：redis&gt; config set requirepass xbqpass 3.当对redis进行操作时，需要授权： redis&gt; auth xbqpass 四、持久化1.RDB（Snapshotting快照持久化） 快照是默认的持久化方式。这种方式是就是将内存中数据以快照的方式写入到二进制文件中,默认的文件名为dump.rdb。可以通过配置设置自动做快照持久化的方式。我们可以配置redis在n秒内如果超过m个key被修改就自动做快照，下面是默认的快照保存配置： save 900 1 #900秒内如果超过1个key被修改，则发起快照保存 save 300 10 #300秒内容如超过10个key被修改，则发起快照保存 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照 client 也可以使用save或者bgsave命令通知redis做一次快照持久化，每次快照持久化都是将内存数据完整写入到磁盘一次，并不是增量的只同步脏数据。如果数据量大的话，而且写操作比较多，必然会引起大量的磁盘io操作，可能会严重影响性能。另外由于快照方式是在一定间隔时间做一次的，所以如果redis意外down掉的话，就会丢失最后一次快照后的所有修改。 2.AOF（Append-only） redis会将每一个收到的写命令都通过write函数追加到文件中(默认是appendonly.aof)。当redis重启时会通过重新执行文件中保存的写命令来在内存中重建整个数据库的内容。 appendonly yes #启用aof持久化方式 # appendfsync always #每次收到写命令就立即强制写入磁盘，最慢的，但是保证完全的持久化，不推荐使用 appendfsync everysec #每秒钟强制写入磁盘一次，在性能和持久化方面做了很好的折中，推荐 # appendfsync no #完全依赖os，性能最好,持久化没保证 3.RDB机制的优势和劣势 1.RDB优势: 一旦采用该方式，那么整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 2.RDB劣势： 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 4.AOF机制的优势和劣势 1.AOF优势： 1). 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。 2). 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。 3). 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。 4). AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 2.AOF劣势： 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。 3.如何修复坏损的AOF文件： 1). 将现有已经坏损的AOF文件额外拷贝出来一份。2). 执行”redis-check-aof –fix “命令来修复坏损的AOF文件。3). 用修复后的AOF文件重新启动Redis服务器。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis（一）-- 基础]]></title>
    <url>%2F2019%2F06%2F14%2FRedis%EF%BC%88%E4%B8%80%EF%BC%89-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[一、Redis 简介Redis 是完全开源免费的，是一个高性能的key-value数据库。 Redis 与其他 key - value 缓存产品有以下三个特点： Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s。 二、Redis的安装1.下载到/usr/java/redis：wget http://download.redis.io/releases/redis-3.2.4.tar.gz 2.解压：tar –-zxvf redis-stable.tar.gz 解压 3.make 编译 4.编译成功后，进入src文件夹，执行make install进行Redis安装： make install –&gt; /usr/local/bin目录下有：redis-server,redis-cli,redis-check-aof,redis-check-dump 5.redis-server -v 检查是否安装成功 6.修改redis.conf中的 daemonize 改为 yes（修改为以后台程序启动），pidfile 改为 /usr/java/redisFile/redis.pid，bind 改为 IP地址，port 改为 6379，logfile 改为 /usr/java/redis/redisFile/redis.log，loglevel 改为 debug 7.启动：redis-server redis.conf 8.客户端连接：redis-cli -h IP -p PORT 三、redis配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768691. Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程 daemonize yes 2. 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定 pidfile /var/run/redis.pid 3. 指定Redis监听端口，默认端口为6379，作者在自己的一篇博文中解释了为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字 port 6379 4. 绑定的主机地址 bind 127.0.0.1 这个Ip要设置成你服务器的Ip 5.当 客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能 timeout 300 6. 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose loglevel verbose 7. 日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null logfile stdout 8. 设置数据库的数量，默认数据库为0，可以使用SELECT &lt;dbid&gt;命令在连接上指定数据库id databases 16 9. 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 save &lt;seconds&gt; &lt;changes&gt; Redis默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。 10. 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大 rdbcompression yes 11. 指定本地数据库文件名，默认值为dump.rdb dbfilename dump.rdb 12. 指定本地数据库存放目录 dir ./ 13. 设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步 slaveof &lt;masterip&gt; &lt;masterport&gt; 14. 当master服务设置了密码保护时，slav服务连接master的密码 masterauth &lt;master-password&gt; 15. 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH &lt;password&gt;命令提供密码，默认关闭 requirepass foobared 16. 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息 maxclients 128 17. 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区 maxmemory &lt;bytes&gt; 18. 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no appendonly no 19. 指定更新日志文件名，默认为appendonly.aof appendfilename appendonly.aof 20. 指定更新日志条件，共有3个可选值： no：表示等操作系统进行数据缓存同步到磁盘（快） always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec 21. 指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析Redis的VM机制） vm-enabled no 22. 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享 vm-swap-file /tmp/redis.swap 23. 将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0 vm-max-memory 0 24. Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值 vm-page-size 32 25. 设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，，在磁盘上每8个pages将消耗1byte的内存。 vm-pages 134217728 26. 设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4 vm-max-threads 4 27. 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启 glueoutputbuf yes 28. 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法 hash-max-zipmap-entries 64 hash-max-zipmap-value 512 29. 指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍） activerehashing yes 30. 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件 include /path/to/local.conf 注意：protected-mode参数是为了禁止外网访问redis，如果需要外网访问，需要设置为 no 四、基本命令1.返回redis的相关信息：info 2.清空所有数据库中的数据库：flushall 3.清空当前数据库： flushdb 4.切换数据库：select 0~15（默认配置16个数据库） 对String数据类型而言 1.添加String数据：set name xbq 2.取出String数据：get name 3.判断key为name的键 是否存在：exists name，返回0则没有 4.重命名key：rename name name1（将name重命名为name1） 5.查看key为name1的数据类型：type name1 6.在指定key的value后拼接指定数据：append name1 666（在name1对应的value后拼接666） 7.获取key对应的value的长度：strlen name1 8.删除指定的key：del name1 9.查看指定数据库的全部key：keys * 10.查看当前数据库的key的数量：dbsize 对list数据类型而言 1.添加数据（插入顺序是从尾部向头部）：lpush mykey a b c d 2.查询全部数据：lrange mykey 0 -1 3.查询第一个到第三个数据：lrange mykey 0 2 4.查询索引为2（第三个）的数据：lindex mykey 2 5.查看链表mykey中的元素个数：llen mykey 6.取出链表的位于头部的数据：lpop mykey（取出之后链表中 就不存在此元素了） 7.删除两个等于a的元素：lrem mykey 2 a 8.将索引值为1的元素改为xbq：lset mykey 1 xbq 9.查询是否设置成功（在8的基础上）：lindex mykey 1 10.删除指定链表：del mykey 11.添加数据：lpush mykey1 a b c d e f 12.获取索引值1到3之间的元素：ltrim mykey1 1 3 13.在元素d前面插入xbq：linsert mykey1 before d xbq 14.在元素c后面插入fy：linsert mykey1 after c fy 15.在链表mykey2中添加元素（插入顺序是从头部向尾部）：rpush mykey2 a b c d e 16.取出链表mykey2中的尾部元素：rpop mykey2 ，即e 17.在链表mykey1中取出位于尾部的元素 放到链表mykey2的头部位置：rpoplpush mykey2 mykey3（重要） 对hash数据类型而言 1.添加数据：hset myhash name xbq 2.查询数据：hget myhash name 3.获取myhash键的字段数量：hlen myhash 4.判断myhash键中是否存在字段名为name的字段：hexists myhash name 5.删除该键：del myhash 6.添加数据，一次性添加多个字段：hmset myhash name xbq age 20 address shenzhen 7.查询数据，可以查询多个字段：hmget myhash name age 8.查询指定键中的所有字段和值，逐对出现：hgetall myhash 9.查询指定键中的所有的字段：hkeys myhash 10.查询指定键中的所有值：hvals myhash 对set数据类型而言 1.添加数据：sadd myset a b c d e 2.查询数据，结果是无序的，并不是插入的顺序：smembers myset 3.判断b是否在集合中存在：sismember myset b 4.查询集合中的数量：scard myset 5.随机查询集合中的元素：srandmember myset [count] 6.取出集合尾部的元素：spop myset 7.取出集合myset中的元素 放到集合myset1中：smove myset myset1 a 8.清空该数据库：flushdb 9.数据准备： sadd myset a b c d sadd myset2 c sadd myset3 a c e 10.myset和myset2相比，a、b和d三个成员是两者之间的差异成员。再用这个结果继续和myset3进行差异比较，b和d是myset3不存在的成员： sdiff myset myset2 myset3 11.将3个集合的差异成员存在在diffkey关联的Set中，并返回插入的成员数量：sdiffstore mysetdiff myset myset2 myset3 12.取出3个集合中的交集部分：sinter myset myset2 myset3 13.将3个集合中的交集成员存储到与interkey关联的Set中： sinterstore interkey myset myset2 myset3 14.取出3个集合中的并集部分：sunion myset myset2 myset3 15.将3个集合中成员的并集存储到unionkey关联的set中：sunionstore unionkey myset myset2 myset3 对zset数据类型而言 1.添加数据：zadd myzset 1 a 格式：zadd 集合名 序号 集合元素 zadd myzset 1 a zadd myzset 3 b 2 c 2.查询集合中的元素：zrange myzset 0 -1 （这里是有序的，按序号的大小 排序的），查询结果为 a c b 3.查询集合中的元素和序号：myzset 0 -1 withscores 4.查询集合中的个数：zcard myzset 5.查询元素对应的下标（下标是从0开始的）：zrank myzset b 6.查询下标1~2之间的元素个数：zcount myzset 1 2 7.查询元素对应的序号：zscore myzset a 8.将c元素的序号增加4：zincrby myzset 4 c 9.查询序号满足表达式1 &lt; score &lt;= 2的成员：zrangebyscore myzset 1 2 10.删除指定的一个元素或多个元素：zrem myzset a b 五、五种数据类型的应用场景1.String 字符串类型是Redis中最为基础的数据存储类型，它在Redis中是二进制安全的，这便意味着该类型可以接受任何格式的数据，如JPEG图像数据或Json对象描述信息等。在Redis中字符串类型的Value最多可以容纳的数据长度是512M。 2.list 在Redis中，List类型是按照插入顺序排序的字符串链表。和数据结构中的普通链表一样，我们可以在其头部(left)和尾部(right)添加新的元素。在插入时，如果该键并不存在，Redis将为该键创建一个新的链表。与此相反，如果链表中所有的元素均被移除，那么该键也将 会被从数据库中删除。List中可以包含的最大元素数量是4294967295。 从元素插入和删除的效率视角来看，如果我们是在链表的两头插入或删除元素，这将会是非常高效的操作，即使链表中已经存储了百万条记录，该操作也可以在常量时间内完成。然而需要说明的是，如果元素插入或删除操作是作用于链表中间，那将会是非常低效的。相信 对于有良好数据结构基础的开发者而言，这一点并不难理解。 Redis链表经常会被用于消息队列的服务，以完成多程序之间的消息交换。假设一个应用程序正在执行LPUSH操作向链表中添加新的元素，我们通常将这样的程序称之为”生产者(Producer)”，而另外一个应用程序正在执行RPOP操作从链表中取出元素，我们称这样的程序为”消费者(Consumer)”。如果此时，消费者程序在取出消息元素后立刻崩溃，由于该消息已经被取出且没有被正常处理，那么我们就可以认为该消息已经丢失，由此可能会导致业务数据丢失，或业务状态的不一致等现象的发生。然而通过使用RPOPLPUSH命令，消费者程序在从主消息队列中取出消息之后再将其插入到备份队列中，直到消费者程序完成正常的处理逻辑后再将该消息从备份队列中删除。同时我们还可以提供一个守护进程，当发现备份队列中的消息过期时，可以重新将其再放回到主消息队列中，以便其它的消费者程序继续处理。 3.hash 将Redis中的Hashes类型看成具有String Key和String Value的map容器。所以该类型非常适合于存储值对象的信息。如Username、Password和Age等。如果Hash中包含很少的字段，那么该类型的数据也将仅占用很少的磁盘空间。每一个Hash可以存储4294967295个键值对。 4.set 在Redis中，我们可以将Set类型看作为没有排序的字符集合，和List类型一样，我们也可以在该类型的数据值上执行添加、删除或判断某一元素是否存在等操作。需要说明的是，这些操作的时间复杂度为O(1)，即常量时间内完成次操作。Set可包含的最大元素数量是4294967295。 和List类型不同的是，Set集合中不允许出现重复的元素，这一点和C++标准库中的set容器是完全相同的。换句话说，如果多次添加相同元素，Set中将仅保留该元素的一份拷贝。和List类型相比，Set类型在功能上还存在着一个非常重要的特性，即在服务器端完成多个Sets之间的聚合计算操作，如unions、intersections和differences。由于这些操作均在服务端完成，因此效率极高，而且也节省了大量的网络IO开销。 可以使用Redis的Set数据类型跟踪一些唯一性数据，比如访问某一博客的唯一IP地址信息。对于此场景，我们仅需在每次访问该博客时将访问者的IP存入Redis中，Set数据类型会自动保证IP地址的唯一性。 充分利用Set类型的服务端聚合操作方便、高效的特性，可以用于维护数据对象之间的关联关系。比如所有购买某一电子设备的客户ID被存储在一个指定的Set中，而购买另外一种电子产品的客户ID被存储在另外一个Set中，如果此时我们想获取有哪些客户同时购买了这两种商品时，Set的intersections命令就可以充分发挥它的方便和效率的优势了。 5.zset Sorted-Sets和Sets类型极为相似，它们都是字符串的集合，都不允许重复的成员出现在一个Set中。它们之间的主要差别是Sorted-Sets中的每一个成员都会有一个分数(score)与之关联，Redis正是通过分数来为集合中的成员进行从小到大的排序。然而需要额外指出的是，尽管Sorted-Sets中的成员必须是唯一的，但是分数(score)却是可以重复的。 在Sorted-Set中添加、删除或更新一个成员都是非常快速的操作，其时间复杂度为集合中成员数量的对数。由于Sorted-Sets中的成员在集合中的位置是有序的，因此，即便是访问位于集合中部的成员也仍然是非常高效的。事实上，Redis所具有的这一特征在很多其它类型的数据库中是很难实现的，换句话说，在该点上要想达到和Redis同样的高效，在其它数据库中进行建模是非常困难的。 可以用于一个大型在线游戏的积分排行榜。每当玩家的分数发生变化时，可以执行ZADD命令更新玩家的分数，此后再通过ZRANGE命令获取积分TOP TEN的用户信息。当然我们也可以利用ZRANK命令通过username来获取玩家的排行信息。最后我们将组合使用ZRANGE和ZRANK命令快速的获取和某个玩家积分相近的其他用户的信息。 Sorted-Sets类型还可用于构建索引数据。]]></content>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper（八）-- Curator实现分布式锁]]></title>
    <url>%2F2019%2F06%2F02%2FZooKeeper%EF%BC%88%E5%85%AB%EF%BC%89-Curator%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[1.pom.xml123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.JAVA代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package com.xbq.zookeeper.curator;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;import org.apache.curator.framework.CuratorFramework;import org.apache.curator.framework.CuratorFrameworkFactory;import org.apache.curator.framework.recipes.locks.InterProcessMutex;import org.apache.curator.retry.RetryNTimes;/** * 使用Curator来实现分布式锁 * @author xbq */public class LockByCurator &#123; // 此demo使用的集群，所以有多个ip和端口 private static String CONNECT_SERVER = "192.168.242.129:2181,192.168.242.129:2182,192.168.242.129:2183"; // session过期时间 private static int SESSION_TIMEOUT = 3000; // 连接超时时间 private static int CONNECTION_TIMEOUT = 3000; // 锁节点 private static final String CURATOR_LOCK = "/curatorLock"; /** * 获取锁操作 * @param cf */ public static void doLock(CuratorFramework cf)&#123; System.out.println(Thread.currentThread().getName() + " 尝试获取锁！"); // 实例化 zk分布式锁 InterProcessMutex mutex = new InterProcessMutex(cf, CURATOR_LOCK); try &#123; // 判断是否获取到了zk分布式锁 if(mutex.acquire(5, TimeUnit.SECONDS))&#123; System.out.println(Thread.currentThread().getName() + " 获取到了锁！-------"); // 业务操作 Thread.sleep(5000); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; // 释放锁 mutex.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 测试 * @param args */ public static void main(String[] args) &#123; // 定义线程池 ExecutorService service = Executors.newCachedThreadPool(); // 定义信号灯，只能允许10个线程并发操作 final Semaphore semaphore = new Semaphore(10); // 模拟10个客户端 for(int i=0; i &lt; 10 ;i++)&#123; Runnable runnable = new Runnable() &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); // 连接 ZooKeeper CuratorFramework framework = CuratorFrameworkFactory. newClient(CONNECT_SERVER, SESSION_TIMEOUT, CONNECTION_TIMEOUT, new RetryNTimes(10,5000)); // 启动 framework.start(); doLock(framework); semaphore.release(); &#125; catch (Exception e) &#123; &#125; &#125; &#125;; service.execute(runnable); &#125; service.shutdown(); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper（七）-- ZK原生API实现分布式锁]]></title>
    <url>%2F2019%2F06%2F02%2FZooKeeper%EF%BC%88%E4%B8%83%EF%BC%89-ZK%E5%8E%9F%E7%94%9FAPI%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[一、使用场景在分布式应用，往往存在多个进程提供同一服务。这些进程有可能在相同的机器上，也有可能分布在不同的机器上。 如果这些进程共享了一些资源，可能就需要分布式锁来锁定对这些资源的访问。 二、实现分布式锁结构图 三、代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185package com.xbq.zookeeper.javaApi;import java.util.Collections;import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;/** * 使用Zookeeper原生API实现分布式锁 * @author xbq */public class ZookeeperLock implements Watcher&#123; // 声明zk对象 private ZooKeeper zk = null; // 此demo使用的集群，所以有多个ip和端口 private static String CONNECT_SERVER = "192.168.242.130:2181,192.168.242.130:2182,192.168.242.130:2183"; // session过期时间 private static int SESSION_TIMEOUT = 3000; // 根节点 private String root = "/locks"; // 当前等待的节点 private String waitNode; // 等待的时间 private int waitTime; // 锁节点 private String myzkNode; // 计数器 private CountDownLatch latch; /** * 构造函数 初始化 */ public ZookeeperLock()&#123; try &#123; zk = new ZooKeeper(CONNECT_SERVER, SESSION_TIMEOUT, this); // 判断是否存在根节点，不需要监听根节点 Stat stat = zk.exists(root, false); // 为空，说明不存在 if(stat == null)&#123; // 添加一个永久节点，即添加一个根节点 zk.create(root, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 尝试获取锁 * @return */ private boolean tryLock()&#123; String splitStr = "lock_"; // 格式 lock_000000001 try &#123; // 创建一个临时有序节点，并赋值给 myzkNode myzkNode = zk.create(root + "/" + splitStr, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // 获取根节点下的所有子节点 List&lt;String&gt; children = zk.getChildren(root, false); // 对子节点 排序 Collections.sort(children); // 如果刚刚创建的节点 等于 获取最小的一个子节点，则说明 获取到锁 if(myzkNode.equals(root + "/" + children.get(0)))&#123; return true; &#125; // 如果刚刚创建的节点 不等于 获取到的最小的一个子节点，则 监控比自己小的一个节点 // 获取刚刚建立的子节点（不包含根节点的子节点） String childNode = myzkNode.substring(myzkNode.lastIndexOf("/") + 1); // 获取比自己小的节点 waitNode = children.get(Collections.binarySearch(children, childNode) - 1); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 等待锁释放 * @param waitNode * @param waidTime * @return * @throws KeeperException * @throws InterruptedException */ private boolean waitLock(String waitNode, int waidTime) throws KeeperException, InterruptedException&#123; // 判断比自己小的节点是否存在，并添加监听 Stat stat = zk.exists(root + "/" + waitNode, true); // 如果存在 比自己小的节点 if(stat != null)&#123; this.latch = new CountDownLatch(1); this.latch.await(waidTime, TimeUnit.MILLISECONDS); this.latch = null; &#125; return true; &#125; /** * 获取锁 */ public void lock()&#123; // 如果尝试获取锁成功 if(tryLock())&#123; // 获取 成功 System.out.println("Thread" + Thread.currentThread().getName() + " -- hold lock!"); return; &#125; // 等待并获取锁 try &#123; waitLock(waitNode, waitTime); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; /** * 解锁 */ public void unLock()&#123; try &#123; zk.delete(myzkNode, -1); zk.close(); System.out.println("Thread" + Thread.currentThread().getName() +" unlock success! 锁节点：" + myzkNode); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; &#125; /** * 删除的时候 触发事件 */ @Override public void process(WatchedEvent event) &#123; // 如果计数器不为空的话，释放计数器锁 if(this.latch != null)&#123; this.latch.countDown(); &#125; &#125; /** * 测试 * @param args */ public static void main(String[] args) &#123; // 定义线程池 ExecutorService service = Executors.newCachedThreadPool(); // 只能10个线程同时运行，即模拟并发数为10 final Semaphore semaphore = new Semaphore(10); // 10个客户端连接 for(int i=0;i&lt;10;i++)&#123; Runnable runnable = new Runnable() &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); ZookeeperLock zkLock = new ZookeeperLock(); zkLock.lock(); // 业务操作代码 Thread.sleep(3000); zkLock.unLock(); semaphore.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;; service.execute(runnable); &#125; service.shutdown(); &#125;&#125; 四、源码下载点击阅读原文下载源码哦]]></content>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper（六）-- CAP和BASE理论、ZAB协议]]></title>
    <url>%2F2019%2F06%2F02%2FZooKeeper%EF%BC%88%E5%85%AD%EF%BC%89-CAP%E5%92%8CBASE%E7%90%86%E8%AE%BA%E3%80%81ZAB%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[一、CAP理论和BASE理论1.CAP理论 CAP理论，指的是在一个分布式系统中，不可能同时满足Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性）这三个基本需求，最多只能满足其中的两项。 对于分布式系统而言，分区容错性是一个最基本的要求，因为分布式系统中的组件必然需要部署到不通的节点，必然会出现子网络，在分布式系统中，网络问题是必定会出现的异常。因此分布式系统只能在C（一致性）和A（可用性）之间进行权衡。 Consistency（一致性）：所有节点在同一时间具有相同的数据。 Availability（可用性）：保证每个请求不管成功或者失败都有响应。 Partition Tolerance（分区容错性）：系统中任意信息的丢失或失败不会影响系统的继续运作。 CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。 2.BASE理论 BASE理论是指，Basically Available（基本可用）、Soft-state（ 软状态/柔性事务）、Eventual Consistency（最终一致性）。是基于CAP定理演化而来，是对CAP中一致性和可用性权衡的结果。 核心思想：即使无法做到强一致性，但每个业务根据自身的特点，采用适当的方式来使系统达到最终一致性。 Basically Available：基本可用，指分布式系统在出现故障的时候，允许损失部分可用性，保证核心可用。但不等价于不可用。 Soft-state：软状态/柔性事务，即状态可以有一段时间的不同步。 Eventual consistency：最终一致性，系统中的所有数据副本经过一定时间后，最终能够达到一致的状态，不需要实时保证系统数据的强一致性。最终一致性是弱一致性的一种特殊情况。 BASE是反ACID的，它完全不同于ACID模型，牺牲强一致性，获得基本可用性和柔性可靠性并要求达到最终一致性。 BASE理论面向的是大型高可用可扩展的分布式系统，通过牺牲强一致性来获得可用性。 二、ZAB协议1.ZAB协议概述 1.ZooKeeper并没有完全采用Paxos算法，而是使用了一种称为ZooKeeper Atomic Broadcast(ZAB,zookeeper原子消息广播协议)的协议作为其数据一致性的核心算法。 2.ZAB协议是为分布式协调服务zookeeper专门设计的一种支持崩溃恢复的原子广播协议。 3.在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。 2.ZAB协议的两种基本模式 ZAB协议包含两种基本的模式，即崩溃恢复和消息广播。 当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时， ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。当选举产生了新的Leader 服务器同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。 当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播 ， 那么新加人的服务器就会自觉地进人数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。]]></content>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper（五）-- Curator使用]]></title>
    <url>%2F2019%2F06%2F02%2FZooKeeper%EF%BC%88%E4%BA%94%EF%BC%89-Curator%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言Curator是Netflix开源的一套ZooKeeper客户端框架： 1.封装ZooKeeper client与ZooKeeper server之间的连接处理; 提供了一套Fluent风格的操作API; 提供ZooKeeper各种应用场景(recipe, 比如共享锁服务, 集群领导选举机制)的抽象封装。 Curator几个组成部分： Client：是ZooKeeper客户端的一个替代品, 提供了一些底层处理和相关的工具方法 Framework： 用来简化ZooKeeper高级功能的使用, 并增加了一些新的功能, 比如管理到ZooKeeper集群的连接, 重试处理 Recipes：实现了通用ZooKeeper的recipe, 该组件建立在Framework的基础之上 Utilities：各种ZooKeeper的工具类 Errors： 异常处理, 连接, 恢复等 Extensions： recipe扩展 Curator内部实现的几种重试策略： ExponentialBackoffRetry：重试指定的次数, 且每一次重试之间停顿的时间逐渐增加 RetryNTimes：指定最大重试次数的重试策略 RetryOneTime：仅重试一次 RetryUntilElapsed：一直重试直到达到规定的时间 正文1.项目使用maven工程，在pom.xml中添加依赖 12345678910111213141516 &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt; 2.下面代码从增删改查、事务、事件订阅/监听器来实现的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213package om.xbq.demo;import java.util.Collection;import org.apache.curator.framework.CuratorFramework;import org.apache.curator.framework.CuratorFrameworkFactory;import org.apache.curator.framework.api.CuratorWatcher;import org.apache.curator.framework.api.transaction.CuratorTransactionResult;import org.apache.curator.framework.recipes.cache.PathChildrenCache;import org.apache.curator.framework.recipes.cache.PathChildrenCacheEvent;import org.apache.curator.framework.recipes.cache.PathChildrenCacheListener;import org.apache.curator.retry.ExponentialBackoffRetry;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.data.Stat;public class CuratorDemo &#123; // 此demo使用的集群，所以有多个ip和端口 private static String CONNECT_SERVER = "192.168.242.129:2181,192.168.242.129:2182,192.168.242.129:2183"; private static int SESSION_TIMEOUT = 3000; private static int CONNECTION_TIMEOUT = 3000; public static void main(String[] args) &#123; // 连接 ZooKeeper CuratorFramework framework = CuratorFrameworkFactory. newClient(CONNECT_SERVER, SESSION_TIMEOUT, CONNECTION_TIMEOUT, new ExponentialBackoffRetry(1000,10)); // 启动 framework.start(); Stat stat = ifExists(framework); if(stat != null)&#123;// update(framework);// delete(framework);// query(framework); // 监听事件，只监听一次，不推荐// listener1(framework); &#125;else &#123;// add(framework); &#125; // 事务// transaction(framework); // 持久监听，推荐使用 listener2(framework); &#125; /** * 判断节点是否存在 * @param cf * @return */ public static Stat ifExists(CuratorFramework cf)&#123; Stat stat = null; try &#123; stat = cf.checkExists().forPath("/node_curator/test");; System.out.println(stat); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return stat; &#125; /** * @Title: add * @Description: TODO(增加节点 ， 可以增加 多级节点) * @param @param cf 设定文件 * @return void 返回类型 * @throws */ public static void add(CuratorFramework cf)&#123; try &#123; String rs = cf.create().creatingParentsIfNeeded() .withMode(CreateMode.PERSISTENT).forPath("/node_curator/test","xbq".getBytes()); System.out.println(rs); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; cf.close(); &#125; &#125; /** * @Title: update * @Description: TODO(修改指定节点的值) * @param @param cf 设定文件 * @return void 返回类型 * @throws */ public static void update(CuratorFramework cf)&#123; try &#123; Stat stat = cf.setData().forPath("/node_curator/test", "javaCoder".getBytes()); System.out.println(stat); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; cf.close(); &#125; &#125; /** * @Title: delete * @Description: TODO(删除节点或者删除包括子节点在内的父节点) * @param @param cf 设定文件 * @return void 返回类型 * @throws */ public static void delete(CuratorFramework cf)&#123; try &#123; // 递归删除的话，则输入父节点 cf.delete().deletingChildrenIfNeeded().forPath("/node_curator"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; cf.close(); &#125; &#125; /** * @Title: query * @Description: TODO(查询节点的值) * @param @param cf 设定文件 * @return void 返回类型 * @throws */ public static void query(CuratorFramework cf)&#123; try &#123; byte[] value = cf.getData().forPath("/node_curator/test"); System.out.println(new String(value)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; cf.close(); &#125; &#125; /** * @Title: transaction * @Description: TODO(一组crud操作同生同灭) * @param @param cf 设定文件 * @return void 返回类型 * @throws */ public static void transaction(CuratorFramework cf)&#123; try &#123; // 事务处理， 事务会自动回滚 Collection&lt;CuratorTransactionResult&gt; results = cf.inTransaction() .create().withMode(CreateMode.PERSISTENT).forPath("/node_xbq1").and() .create().withMode(CreateMode.PERSISTENT).forPath("/node_xbq2").and().commit(); // 遍历 for(CuratorTransactionResult result:results)&#123; System.out.println(result.getResultStat() + "-&gt;" + result.getForPath() + "-&gt;" + result.getType()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; cf.close(); &#125; &#125; /** * @Title: listener1 * @Description: TODO(监听 事件 -- 通过 usingWatcher 方法) * 注意：通过CuratorWatcher 去监听指定节点的事件， 只监听一次 * @param @param cf 设定文件 * @return void 返回类型 * @throws */ public static void listener1(CuratorFramework cf)&#123; try &#123; cf.getData().usingWatcher(new CuratorWatcher() &#123; @Override public void process(WatchedEvent event) throws Exception &#123; System.out.println("触发事件：" + event.getType()); &#125; &#125;).forPath("/javaCoder"); System.in.read(); // 挂起，在控制台上输入 才停止 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; cf.close(); &#125; &#125; /** * @Title: listener2 * @Description: TODO(监听 子节点的事件，不监听 自己 -- 通过 PathChildrenCacheListener 方法，推荐使用) * @param @param cf 设定文件 * @return void 返回类型 * @throws */ public static void listener2(CuratorFramework cf) &#123; // 节点node_xbq不存在 会新增 PathChildrenCache cache = new PathChildrenCache(cf, "/node_xbq", true); try &#123; cache.start(PathChildrenCache.StartMode.POST_INITIALIZED_EVENT); cache.getListenable().addListener(new PathChildrenCacheListener() &#123; @Override public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception &#123; System.out.println("触发事件：" + event.getType()); &#125; &#125;); System.in.read(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; cf.close(); &#125; &#125;&#125; 源码下载点击阅读原文下载源码哦]]></content>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper（四）-- 第三方客户端 ZkClient的使用]]></title>
    <url>%2F2019%2F06%2F02%2FZooKeeper%EF%BC%88%E5%9B%9B%EF%BC%89-%E7%AC%AC%E4%B8%89%E6%96%B9%E5%AE%A2%E6%88%B7%E7%AB%AF-ZkClient%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言zkClient主要做了两件事情: 一件是在session loss和session expire时自动创建新的ZooKeeper实例进行重连。 另一件是将一次性watcher包装为持久watcher。后者的具体做法是简单的在watcher回调中，重新读取数据的同时再注册相同的watcher实例。 zkClient目前已经运用到了很多项目中，知名的有Dubbo、Kafka、Helix。 zkClient jar包下载，或者直接添加maven依赖： http://mvnrepository.com/artifact/com.101tec/zkclient 正文直接上代码： 1.测试类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145package com.xbq.demo;import java.io.IOException;import org.I0Itec.zkclient.ZkClient;import org.I0Itec.zkclient.IZkDataListener;/** * @ClassName: ZkClientDemo * @Description: TODO(zkClient 测试) * @author xbq * @date 2017-3-26 上午11:49:39 */public class ZkClientDemo &#123; // 此demo使用的集群，所以有多个ip和端口 private static String CONNECT_SERVER = "192.168.99.138:2181,192.168.99.138:2182,192.168.99.138:2183"; private static int SESSION_TIMEOUT = 3000; private static int CONNECTION_TIMEOUT = 3000; private static ZkClient zkClient ; static &#123; zkClient = new ZkClient(CONNECT_SERVER, SESSION_TIMEOUT,CONNECTION_TIMEOUT,new MyZkSerializer()); &#125; public static void main(String[] args) &#123; add(zkClient);// update(zkClient);// delete(zkClient); // addDiGui(zkClient);// deleteDiGui(zkClient); // subscribe(zkClient); &#125; /** * @Title: add * @Description: TODO(增加一个指定节点) * @param @param zkClient 设定文件 * @return void 返回类型 * @throws */ public static void add(ZkClient zkClient)&#123; // 如果不存在节点，就新建一个节点 if(!zkClient.exists("/config"))&#123; zkClient.createPersistent("/config","javaCoder"); &#125; // 查询一下，看是否增加成功 String value = zkClient.readData("/config"); System.out.println("value===" + value); &#125; /** * @Title: addSequential * @Description: TODO(递归创建节点) * @param @param zkClient 设定文件 * @return void 返回类型 * @throws */ public static void addDiGui(ZkClient zkClient)&#123; // 递归创建节点 zkClient.createPersistent("/xbq/java/coder", true); if(zkClient.exists("/xbq/java/coder"))&#123; System.out.println("增加成功！"); &#125;else &#123; System.out.println("增加失败！"); &#125; &#125; /** * @Title: delete * @Description: TODO(删除指定节点) * @param @param zkClient 设定文件 * @return void 返回类型 * @throws */ public static void delete(ZkClient zkClient)&#123; // 存在节点才进行删除 if(zkClient.exists("/config"))&#123; boolean flag = zkClient.delete("/config"); System.out.println("删除" + (flag == true ? "成功！" : "失败！")); &#125; &#125; /** * @Title: deleteDiGui * @Description: TODO(递归删除) * @param @param zkClient 设定文件 * @return void 返回类型 * @throws */ public static void deleteDiGui(ZkClient zkClient)&#123; // 存在节点才进行删除 if(zkClient.exists("/xbq"))&#123; // 递归删除的时候 只传入 父节点就可以，如果传入 全部的节点，虽然返回的是true，但是依然是没有删除的， // 因为zkClient将异常封装好了，进入catch的时候，会返回true，这是一个坑 boolean flag = zkClient.deleteRecursive("/xbq"); System.out.println("删除" + (flag == true ? "成功！" : "失败！")); &#125; &#125; /** * @Title: update * @Description: TODO(修改节点的值) * @param @param zkClient 设定文件 * @return void 返回类型 * @throws */ public static void update(ZkClient zkClient)&#123; if(zkClient.exists("/config"))&#123; zkClient.writeData("/config", "testUpdate"); // 查询一下，看是否修改成功 String value = zkClient.readData("/config"); System.out.println("value===" + value); &#125; &#125; /** * @Title: subscribe * @Description: TODO(事件订阅, 可用于配置管理) * 先订阅，再 操作增删改。（可多个 客户端订阅） * @param @param zkClient 设定文件 * @return void 返回类型 * @throws */ public static void subscribe(ZkClient zkClient)&#123; zkClient.subscribeDataChanges("/config/userName", new IZkDataListener() &#123; @Override public void handleDataDeleted(String arg0) throws Exception &#123; System.out.println("触发了删除事件：" + arg0); &#125; @Override public void handleDataChange(String arg0, Object arg1) throws Exception &#123; System.out.println("触发了改变事件：" + arg0 + "--&gt;" + arg1); &#125; &#125;); try &#123; System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2.自定义序列化类 12345678910111213141516171819202122232425262728293031323334package com.xbq.demo;import org.I0Itec.zkclient.exception.ZkMarshallingError;import org.I0Itec.zkclient.serialize.ZkSerializer;import java.io.UnsupportedEncodingException;/** * @ClassName: MyZkSerializer * @Description: TODO(实现序列化接口，转为UTF-8编码) * @author xbq * @date 2017-3-26 上午11:56:22 */public class MyZkSerializer implements ZkSerializer&#123; @Override public byte[] serialize(Object data) throws ZkMarshallingError &#123; try &#123; return String.valueOf(data).getBytes("UTF-8"); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; return null; &#125; @Override public Object deserialize(byte[] bytes) throws ZkMarshallingError &#123; try &#123; return new String(bytes, "UTF-8"); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 源码下载点击阅读原文下载源码哦]]></content>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper（三）-- JAVA原生API]]></title>
    <url>%2F2019%2F06%2F02%2FZookeeper%EF%BC%88%E4%B8%89%EF%BC%89-JAVA%E5%8E%9F%E7%94%9FAPI%2F</url>
    <content type="text"><![CDATA[一、前提jar包：zookeeper-3.4.9.jar，slf4j-api-1.6.1.jar，slf4j-log4j12-1.6.1.jar，log4j-1.2.15.jar 二、Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185package com.xbq.zookeeper;import java.security.NoSuchAlgorithmException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.ACL;import org.apache.zookeeper.data.Id;import org.apache.zookeeper.server.auth.DigestAuthenticationProvider;/** * @ClassName: ZookeeperDemo * @Description: TODO zookeeper测试 * @author xbq * @version 1.0 * @date 2017-3-10 下午5:05:16 */public class ZookeeperDemo &#123; private static final int SESSION_TIMEOUT = 3000; public static void main(String[] args) &#123; Zookeeper zooKeeper = null; try &#123; zooKeeper = new ZooKeeper(&quot;192.168.242.128:2183&quot;, SESSION_TIMEOUT, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; System.out.println(&quot;触发事件：&quot; + event.getType()); &#125; &#125;); String path = &quot;/node_xbq&quot;; String data = &quot;TestxbqCoder&quot;; create(zooKeeper, path, data); // update(zooKeeper, path, data); // delete(zooKeeper, path); // aclOper(zooKeeper, path, data); // aclOper2(zooKeeper, path, data); // doWatch(zooKeeper, path, data); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if(zooKeeper ! = null)&#123; try&#123; zooKeeper.close(); &#125;catch(Exception e)&#123; &#125; &#125; &#125; &#125; /** * @Title: create * @Description: TODO 增加操作 * @param zooKeeper * @param path * @param data * @throws KeeperException * @throws InterruptedException * @return: void */ public static void create(ZooKeeper zooKeeper, String path, String data) throws KeeperException, InterruptedException&#123; if(zooKeeper.exists(path, false) == null)&#123; // 如果不存在节点，就新建 // 第三个参数是 权限，第四个参数 代表持久节点 // 权限分类：OPEN_ACL_UNSAFE：对所有用户开放 READ_ACL_UNSAFE：只读 CREATOR_ALL_ACL： 创建者可以做任何操作 zooKeeper.create(path, data.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; System.out.println(new String(zooKeeper.getData(path, true, null))); &#125; /** * @Title: update * @Description: TODO 修改操作 * @param zooKeeper * @return: void * @throws InterruptedException * @throws KeeperException */ public static void update(ZooKeeper zooKeeper, String path, String data) throws KeeperException, InterruptedException &#123; zooKeeper.setData(path, data.getBytes(), -1); // -1标识任何版本号 都可以 System.out.println(new String(zooKeeper.getData(path, true, null))); &#125; /** * @Title: delete * @Description: TODO 删除 * @param zooKeeper * @param path * @throws KeeperException * @throws InterruptedException * @return: void */ public static void delete(ZooKeeper zooKeeper, String path) throws KeeperException, InterruptedException&#123; if(zooKeeper.exists(path, false) != null)&#123; zooKeeper.delete(path, -1); &#125; if(zooKeeper.exists(path, false) != null)&#123; System.out.println(new String(zooKeeper.getData(path, true, null))); &#125; &#125; /** * @Title: aclOper * @Description: TODO 权限测试 先创建一个只读权限节点，然后更新该节点 * @param zooKeeper * @param path * @param data * @throws KeeperException * @throws InterruptedException * @return: void */ public static void aclOper(ZooKeeper zooKeeper, String path, String data) throws KeeperException, InterruptedException&#123; // 首先创建一个只读的节点。第三个参数 代表只读权限 zooKeeper.create(path, data.getBytes(), Ids.READ_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(new String(zooKeeper.getData(path, true, null))); // 测试更新节点，因为增加的是 只读的，所以 应该是不可以修改的。发现报错：org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /node_xbq zooKeeper.setData(path, &quot;testRead_ACL&quot;.getBytes(), -1); &#125; /** * @Title: aclOper2 * @Description: TODO 自定义权限验证 * 自定义schema权限类型:digest,world,auth,ip，这里用 digest举例 * @param zooKeeper * @param path * @param data * @throws KeeperException * @throws InterruptedException * @throws NoSuchAlgorithmException * @return: void */ public static void aclOper2(ZooKeeper zooKeeper, String path, String data) throws KeeperException, InterruptedException, NoSuchAlgorithmException&#123; // 第一个参数是 所有的权限，第二个参数是 通过 用户名和密码 验证 ACL acl = new ACL(ZooDefs.Perms.ALL, new Id(&quot;digest&quot;, DigestAuthenticationProvider.generateDigest(&quot;root:root&quot;))); List&lt;ACL&gt; acls = new ArrayList&lt;ACL&gt;(); acls.add(acl); if(zooKeeper.exists(path, false) == null)&#123; zooKeeper.create(path, data.getBytes(), acls, CreateMode.PERSISTENT); &#125; zooKeeper.addAuthInfo(&quot;digest&quot;, &quot;root:root&quot;.getBytes()); // 通过下面的方式 是可以取到值的，因为 加了 用户名和 密码 验证 ，需要 加上 zooKeeper.addAuthInfo(&quot;digest&quot;, &quot;root:root&quot;.getBytes()); System.out.println(new String(zooKeeper.getData(path, true, null))); &#125; /** * @Title: doWatch * @Description: TODO 监听 * @param zooKeeper * @param path * @param data * @throws KeeperException * @throws InterruptedException * @return: void */ public static void doWatch(ZooKeeper zooKeeper, String path, String data) throws KeeperException, InterruptedException&#123; if(zooKeeper.exists(path, false) == null)&#123; zooKeeper.create(path, data.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; // 监听 path节点 zooKeeper.getData(path, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; System.out.println(&quot;触发了节点变更事件：&quot; + event.getType()); &#125; &#125;, null); // 用更新操作触发 监听事件 zooKeeper.setData(path, &quot;updateTest&quot;.getBytes(), -1); &#125;&#125; 三、源码下载点击阅读原文可下载源码]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F31%2FZookeeper%EF%BC%88%E4%BA%8C%EF%BC%89-%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[title: Zookeeper（二）– 客户端操作命令date: 2019-05-31 21:41:23tags:categories: 一、前提开启zookeeper服务端，用客户端连接。输入help，查看可使用命令，如下图所示： 操作无非就是增删改查等。 二、增加格式：create [-s] [-e] path data acl path：即路径，由于zookeeper是一个树形结构，所以创建的时候就是一个Path节点，就是路径 data：节点对应的值，保存的少量数据 [-s]有序节点 [-e]临时节点 注：节点临时有序节点、临时节点、持久有序节点 和 持久节点。 1.创建节点 eg：create /node_1 1 2.创建有序的持久化节点 eg: create -s /node_1/node_1_1 2 3.创建临时节点（基于会话级别） eg: create -e /node_1/node_1_2 3 三、查询get path [watch] eg:get /node_1 四、修改set path data [version] eg: set /node_1 abc 五、删除delete path [vsersion] eg:delete /node_1 发现报：Node not empty:/node_1，说明node_1下面存在节点，不可以删除。 所以，删除其中一个子节点： delete /node_1/node_1_1 查看node_1 下面的节点： ls /node_1 六、查看ls 和 ls2 和 stat s /node_1 ls2 /node_1 stat /node_1 ls2和stat命令输出的信息更加详细，结果一样。 七、给节点设置限制setquota -n|-b val path -n的时候，val表示创建的子节点数量，path为指定的节点 -b的时候，val表示创建节点的数据量，path为指定的节点 1.首先查看/node_1下子节点的数量，使用 : ls /node_1，发现/node_1下面有两个子节点： 2.这里我们设置/node_1下面允许有3个子节点 setquota -n 3 /node_1 3.我们继续添加一个子节点 create /node_1/node_1_1 aaa 查看/node_1下面的节点数，有3个，正确的。 4.再继续添加一个子节点 create /node_1/node_1_4 bbb 发现也是可以创建的，虽然设置了限制，但是仍然是可以创建成功。同时，会bin/zookeeper.out 输出警告信息。这个时候我们看下日志， 命令： tail -f zookeeper.out。 八、给节点删除限制（很少用到）delquota [-n|-b] path eg: delquota -n /node_1 ，使用这个就删除了刚刚在node_1上加上的限制 通过get /node_1/node_1_4，可以看到 count=-1，则说明没有限制 九、查看事务日志退出客户端，进入到 cd /usr/java/zookeeper/server3/data/log/version-2，查看事务日志： 1java -cp ../../../zookeeper-3.4.9/zookeeper-3.4.9.jar:../../../zookeeper-3.4.9/lib/slf4j-api-1.6.1.jar org.apache.zookeeper.server.LogFormatter log.200000001]]></content>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper（一）-- 简介以及单机部署和集群部署]]></title>
    <url>%2F2019%2F05%2F31%2FZookeeper%EF%BC%88%E4%B8%80%EF%BC%89-%E7%AE%80%E4%BB%8B%E4%BB%A5%E5%8F%8A%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2%E5%92%8C%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、分布式系统由多个计算机组成解决同一个问题的系统，提高业务的并发，解决高并发问题。 二、分布式环境下常见问题 节点失效 配置信息的创建及更新 分布式锁 三、Zookeeper1.定义 Zookeeper是一个高性能，分布式的，开源分布式应用协调服务。所谓的分布式协调服务，就是在集群的节点中进行可靠的消息传递，来协调集群的工作。 Zookeeper之所以能够实现分布式协调服务，靠的就是它能够保证分布式数据一致性。所谓的分布式数据一致性，指的就是可以在集群中保证数据传递的一致性。 Zookeeper能够提供的分布式协调服务包括：数据发布订阅、负载均衡、命名服务、分布式协调/通知、集群管理、分布式锁、分布式队列等功能 2.应用场景 配置中心 负载均衡 统一命名服务 共享锁 四、Zookeeper的特点Zookeeper工作在集群中，对集群提供分布式协调服务，它提供的分布式协调服务具有如下的特点： 顺序一致性 从同一个客户端发起的事务请求，最终将会严格按照其发起顺序被应用到zookeeper中 原子性 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的 单一视图 无论客户端连接的是哪个zookeeper服务器，其看到的服务端数据模型都是一致的。 可靠性 一旦服务端成功的应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会一直保留下来，除非有另一个事务又对其进行了改变。 实时性 zookeeper并不是一种强一致性，只能保证顺序一致性和最终一致性，只能称为达到了伪实时性。 五、基本概念1.数据模型 zookeepei中可以保存数据，正是利用zookeeper可以保存数据这一特点，我们的集群通过在zookeeper里存取数据来进行消息的传递。 zookeeper中保存数据的结构非常类似于文件系统。都是由节点组成的树形结构。不同的是文件系统是由文件夹和文件来组成的树，而zookeeper中是由znode来组成的树。 每一个ZNODE里都可以存放一段数据，znode下还可以挂载零个或多个子znode节点，从而组成一个树形结构。（结构如下所示） 2.集群角色 （1）Leader：接受所有Follower的提案请求并统一协调发起提案的投票，负责与所有的Follower进行内部数据交换（同步） （2）Follower：直接为客户端服务并参与提案的投票，同时与Leader进行数据交换（同步） （3）Observer：直接为客户端服务但不参与提案的投票，同时也与Leader进行数据交换（同步） 3.会话 4.版本号 5.Acl权限控制 六、单机部署1.下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/，通过 1wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/zookeeper-3.4.9.tar.gz 下载到Linux 2.将/usr/java/zookeeper/server1/zookeeper-3.4.9/conf下的zoo_sample.cfg文件复制为zoo.cfg，修改zoo.cfg，修改dataDir( 数据快照存储目录) 和 增加dataLogDir（保存Log的目录） 3.通过bin/zkServer.sh start|stop|restart|status 启动、停止、重启、查询状态 4.通过bin/zkCli.sh [-timeout 0 -r] -server host:port（host即IP地址，peot即端口号） 七、集群部署（这里使用伪集群）1.在/usr/java中新建zookeeper目录，然后在zookeeper目录下新建 server1,server2,server3目录 123mkdir /usr/java/zookeeper/server1mkdir /usr/java/zookeeper/server1mkdir /usr/java/zookeeper/server1 2.将下载的tar文件复制到新建的server1,server2,server3中，然后解压到这三个目录下 3.修改zoo.cfg文件 第一个端口号是Leader和Follower之间通讯的端口号，即数据同步的端口号，第二个端口号是Leader选举的端口号 4.分别复制zoo.cfg到server2/zookeeper-3.4.9/conf 和 server3/zookeeper-3.4.9/conf下 5.修改server2/zookeeper-3.4.9/conf下的zoo.cfg，只修改dataDir、dataLogDir和clientPort即可，下图server点后面的数字就是myid，即进程号，用于选举 6.同理，修改server3/zookeeper-3.4.9/conf下的zoo.cfg，将dataDir改为server3下面的，将dataLogDir改为server3下的，将clientPort改为2183 7.在server1，server2，server3中新建data目录，并且在data目录下新建data和log目录 8.在server1/data/data下编辑myid文件，即 vi myid，在里面写1， 在server2/data/data下编辑myid文件，即 vi myid，在里面写2， 在server3/data/data下编辑myid文件，即 vi myid，在里面写3， 这里对应上图中server点后面的数字，即进程号。 9.启动server1， 12cd /usr/java/zookeeper/server1/zookeeper-3.4.9/bin ./zkServer.sh start ，启动成功后，然后通过客户端连接：./zkCli.sh -server localhost:2181，可以看到如下： 说明错误了。在集群中，必须有一半的服务正常，客户端才可以正常连接。 10.先关闭客户端，然后启动server2，再重新连接 123cd /usr/java/zookeeper/server2/zookeeper-3.4.9/bin （进入server2的目录）./zkServer start （启动server2）./zkCli.sh -server localhost:2181（客气客户端，连接服务端） 可以看到如下，则说明成功。 11.再启动server3，用客户端测试连接。OK，则集群成功。 12.查看哪一个是Leader，哪一个是Follower，进入zookeeper的bin目录下，输入zkServer.sh status即可查看]]></content>
  </entry>
  <entry>
    <title><![CDATA[面试题 - equals和hashcode为什么要一起重写]]></title>
    <url>%2F2019%2F05%2F30%2F%E9%9D%A2%E8%AF%95%E9%A2%98-equals%E5%92%8Chashcode%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%B8%80%E8%B5%B7%E9%87%8D%E5%86%99%2F</url>
    <content type="text"><![CDATA[object对象中的 public boolean equals(Object obj)，对于任何非空引用值 x 和 y，当且仅当 x 和 y 引用同一个对象时，此方法才返回 true； 注意：当此方法被重写时，通常有必要重写 hashCode 方法，以维护 hashCode 方法的常规协定，该协定声明相等对象必须具有相等的哈希码。 如下： (1)当obj1.equals(obj2)为true时，obj1.hashCode() == obj2.hashCode()必须为true (2)当obj1.hashCode() == obj2.hashCode()为false时，obj1.equals(obj2)必须为false 如果不重写equals，那么比较的将是对象的引用是否指向同一块内存地址，重写之后目的是为了比较两个对象的value值是否相等。特别指出利用equals比较八大包装对象 （如int，float等）和 String类（因为该类已重写了equals和hashcode方法）对象时，默认比较的是值，在比较其它自定义对象时都是比较的引用地址 hashcode是用于散列数据的快速存取，如利用HashSet/HashMap/Hashtable类来存储数据时，都是根据存储对象的hashcode值来进行判断是否相同的。 这样如果我们对一个对象重写了euqals，意思是只要对象的成员变量值都相等那么euqals就等于true，但不重写hashcode，那么我们再new一个新的对象， 当原对象.equals（新对象）等于true时，两者的hashcode却是不一样的，由此将产生了理解的不一致，如在存储散列集合时（如Set类），将会存储了两个值一样的对象， 导致混淆，因此，就也需要重写hashcode() 举例说明： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.HashSet;import java.util.Set;/** * @author: Joe * @date: 2019/2/13 16:47 * @description: */public class XbqTest &#123; public static void main(String[] args) &#123; Name n1 = new Name(&quot;01&quot;); Name n2 = new Name(&quot;01&quot;); Set&lt;Name&gt; c = new HashSet&lt;Name&gt;(); c.add(n1); System.out.println(&quot;------1------&quot;); c.add(n2); System.out.println(&quot;------2------&quot;); System.out.println(n1.equals(n2)); System.out.println(&quot;------3------&quot;); System.out.println(n1.hashCode()); System.out.println(n2.hashCode()); System.out.println(c); &#125;&#125;class Name &#123; private String id; public Name(String id) &#123; this.id = id; &#125; public String toString()&#123; return this.id; &#125; /** * 重写 equals 方法 * @param obj * @return */ public boolean equals(Object obj) &#123; if (obj instanceof Name) &#123; Name name = (Name) obj; System.out.println(&quot;equal&quot;+ name.id); return (id.equals(name.id)); &#125; return super.equals(obj); &#125; /** * 重写 hashCode 方法 * @return */ public int hashCode() &#123; Name name = (Name) this; System.out.println(&quot;Hash&quot; + name.id); return id.hashCode(); &#125;&#125; 运行结果： 12345678910111213Hash01------1------Hash01equal01------2------equal01true------3------Hash011537Hash011537[01] 当注释掉 Name类中 equals 方法时，运行结果如下： 1234567891011Hash01------1------Hash01------2------false------3------Hash011537Hash011537[01, 01] 当注释掉 Name类中 hashCode 方法时，运行结果如下： 12345678------1------------2------equal01true------3------942731712971848845[01, 01] 就这个程序进行分析，在第一次添加时，调用了hashcode()方法，将hashcode存入对象中，第二次也一样，然后对hashcode进行比较。hashcode也只用于HashSet/HashMap/Hashtable类存储数据，所以会用于比较，需要重写 总结，自定义类要重写equals方法来进行等值比较，自定义类要重写compareTo方法来进行不同对象大小的比较，重写hashcode方法为了将数据存入HashSet/HashMap/Hashtable类时进行比较。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（十一）-- 使用Merge存储引擎实现MySQL分表]]></title>
    <url>%2F2019%2F05%2F28%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89-%E4%BD%BF%E7%94%A8Merge%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E5%AE%9E%E7%8E%B0MySQL%E5%88%86%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[一、使用场景Merge表有点类似于视图。使用Merge存储引擎实现MySQL分表，这种方法比较适合那些没有事先考虑分表，随着数据的增多，已经出现了数据查询慢的情况。 这个时候如果要把已有的大数据量表分开比较痛苦，最痛苦的事就是改代码。所以使用Merge存储引擎实现MySQL分表可以避免改代码。 Merge引擎下每一张表只有一个MRG文件。MRG里面存放着分表的关系，以及插入数据的方式。它就像是一个外壳，或者是连接池，数据存放在分表里面。 merge合并表的要求： 合并的表使用的必须是MyISAM引擎 表的结构必须一致，包括索引、字段类型、引擎和字符集 对于增删改查，直接操作总表即可。 二、建表1.用户1表 123456CREATE TABLE `user1` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(50) DEFAULT NULL, `sex` int(1) NOT NULL DEFAULT &apos;0&apos;, PRIMARY KEY (`id`)) ENGINE=MyISAM DEFAULT CHARSET=utf8; 2.用户2表 1create table user2 like user1; 3.主表 123456CREATE TABLE `alluser` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(50) DEFAULT NULL, `sex` int(1) NOT NULL DEFAULT &apos;0&apos;, KEY `id` (`id`)) ENGINE=MRG_MyISAM DEFAULT CHARSET=utf8 INSERT_METHOD=LAST UNION=(`user1`,`user2`); 1) ENGINE = MERGE 和 ENGINE = MRG_MyISAM是一样的意思，都是代表使用的存储引擎是 Merge。 2) INSERT_METHOD，表示插入方式，取值可以是：0 和 1，0代表不允许插入，1代表可以插入； 3) FIRST插入到UNION中的第一个表，LAST插入到UNION中的最后一个表。 三、操作1.先在user1表中增加一条数据，然后再在user2表中增加一条数据，查看 alluser中的数据。 123insert into user1(name,sex) values (&apos;张三&apos;,1);insert into user2(name,sex) values (&apos;李四&apos;,2);select * from alluser; 发现是刚刚插入的数据如下： 这就出现了一个id重复，这就造成了当删除和修改的时候异常，解决办法是给 alluser的id赋唯一值。 我们解决方法是，重新建立一张表tb_ids(id int)，用来专门存一个id的，并插入一条初始数据，同时删除掉user1和user2中的数据。 1234create table tb_ids(id int);insert into tb_ids values(1);delete from user1;delete from user2; 然后在user1和user2表中分别建立一个触发器(tr_seq和tr_seq2)，触发器的功能是 当在user1或者user2表中增加一条记录时，取出tb_ids中的id值，赋给user1和user2的id，然后将tb_ids的id值加1， user1表的触发器内容如下（user2表的触发器修要修改 触发器的名字 和 表名，如下红字标注）： 123456789DELIMITER $$CREATE TRIGGER tr_seqBEFORE INSERT on user1FOR EACH ROW BEGIN select id into @testid from tb_ids limit 1; update tb_ids set id = @testid + 1;set new.id = @testid;END$$DELIMITER; 2.在user1和user2表中分别增加一条数据， 12insert into user1(name,sex) values(&apos;王五&apos;,1);insert into user2(name,sex) values(&apos;赵六&apos;,2); 3.查询user1和user2中的数据： 4.查询总表alluser中的数据，发现id没有重复的： ​ 搞定。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（十）-- 主从复制]]></title>
    <url>%2F2019%2F05%2F24%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E5%8D%81%EF%BC%89-%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[一、概念 Mysql复制（replication）是一个异步的复制，从一个Mysql 实例（Master）复制到另一个Mysql 实例（Slave）。实现整个主从复制，需要由Master服务器上的IO进程，和Slave服务器上的Sql进程和IO进程共从完成。要实现主从复制，首先必须打开Master端的binary log（bin-log）功能，因为整个 MySQL 复制过程实际上就是Slave从Master端获取相应的二进制日志，然后再在自己slave端完全顺序的执行日志中所记录的各种操作。 （二进制日志几乎记录了除select以外的所有针对数据库的sql操作语句） 二、复制解决的问题(1) 数据分布 (Data distribution ) (2) 负载平衡(load balancing) (3) 备份(Backups) (4) 高可用性和容错行 High availability and failover 三、复制的原理 (1) master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； (2) slave将master的binary log events拷贝到它的中继日志(relay log)； (3) slave重做中继日志中的事件，将更改应用到自己的数据上。 下图描述了复制的过程： 该过程的第一部分就是master记录二进制日志。在每个事务更新数据完成之前，master在二日志记录这些改变。MySQL将事务串行的写入二进制日志，即使事务中的语句都是交叉执行的。在事件写入二进制日志完成后，master通知存储引擎提交事务。 下一步就是slave将master的binary log拷贝到它自己的中继日志。首先，slave开始一个工作线程——I/O线程。I/O线程在master上打开一个普通的连接，然后开始binlog dump process。Binlog dump process从master的二进制日志中读取事件，如果已经跟上master，它会睡眠并等待master产生新的事件。I/O线程将这些事件写入中继日志。 SQL slave thread（SQL从线程）处理该过程的最后一步。SQL线程从中继日志读取事件，并重放其中的事件而更新slave的数据，使其与master中的数据一致。只要该线程与I/O线程保持一致，中继日志通常会位于OS的缓存中，所以中继日志的开销很小。 此外，在master中也有一个工作线程：和其它MySQL的连接一样，slave在master中打开一个连接也会使得master开始一个线程。复制过程有一个很重要的限制——复制在slave上是串行化的，也就是说master上的并行更新操作不能在slave上并行操作。 四、配置 创建一个数据库 ： create database testXbq; 创建表：create table student(id int primary key auto_increment,name varchar(20)); 先将master上的数据复制到slave上（基于二进制日志备份（默认是关闭的）） （1）先开启二进制日志： 编辑配置文件：vi /etc/my.cnf，在里面增加 123log-bin=mysql-bin // 二进制存放的目录和名字binlog_format=mixed // server-id=1 // 标志 master的唯一标识 （2）重启mysql： service mysql restart （3）查看master的状态：show master status; （4）查看目录下的二进制文件： ll /var/lib/mysql 其中，mysql-bin.index存的是： （5）基于 位置 备份 1mysqldump -uroot -p --single-transaction --master-data=2 --triggers --routines --all-databases &gt; xbq.sql // 基于独立的事务 // =2 注释掉 =1 不注释 // 备份到当前目录下xbq.sql 查看 xbq.sql，会发现如下（或者 直接重启master后 登录mysql，执行 show master status也可以看到 从 mysql-bin.000001的579位置开始复制）： 后续的复制工作会从 mysql-bin.000001的579位置开始复制。 （6）master需要授权 一个用户给 slave，即建立一个复制账号，在master上执行（可以通过 ? grand 查看 grand的语法）： grant replication slave on . to ‘reppc‘@’%’ identified by ‘xbq123’; // 一般不用root帐号，% 表示所有客户端都可能连，只要帐号，密码正确，此处可用具体客户端IP代替，如192.168.1.112，加强安全。 需要刷新权限 才可以生效： flush privileges;（此时，在salve上连接 master的数据库） （7）重启master （8）在slave上应用 xbq.sql，有两种方式：（master中执行下面命令,ip换成slave的ip） 方法一：远程拷贝到 slaver， scp root@192.168.242.129:/usr/xbq.sql /usr/xbq.sql ，机器上需安装openssh-client，yum install -y openssh-client 方法二：远程连接 执行sql， mysql -h192.168.242.129 -uroot -p &lt; /usr/xbq.sql，前提，需要在slave上给master远程连接的权限，如下： 12grant all privileges on *.* to &apos;root&apos;@&apos;master的IP&apos; identified by &apos;master的mysql密码&apos; with grant option;FLUSH PRIVILEGES; 4.slave 应用sql（前提：slave中已经有了master导出的sql文件，以下命令在slave中执行。如果使用 (8)中的方法二的话，忽略此步骤） 方法一： source /usr/xbq.sql 方法二： mysql -uroot -p &lt; /usr/xbq.sql 经过上面的操作 master上的数据 就到了 slave中 5.复制的过程： （1）slave开始二进制日志，和 master一样的配置 ，但是 server-id 需要 =2。然后重启salve上的mysql。 验证在slave上是否可以登录master，mysql -u root -h master的IP -p ，输入密码后，可以登录 则证明可以登录。否则，需要在 master上给slave授权，如下： 12grant all privileges on *.* to &apos;root&apos;@&apos;slave的IP&apos; identified by &apos;slave的mysql密码&apos; with grant option;FLUSH PRIVILEGES; （2）让slave连接master，并开始重做master二进制日志中的事件（可以通过 ? change master to 查看语法）： 1change master to master_host=&apos;192.168.242.129&apos;, master_user=&apos;reppc&apos;,master_password=&apos;xbq123&apos;, master_port=3306,master_log_file=&apos;mysql-bin.000001&apos;,master_log_pos=579; 从 服务器的哪一个日志的哪一个位置 复制。 其中，master_host是master的IP，master_user是在master中赋予复制权限的用户，master_log_file是master中二进制文件，master_log_pos是开始复制的位置。 （3）查看IOThread和SQLThread是否成功，只有 这两个状态 都是 YES才可以复制。 （4）查看maste上的用户表 select * from mysql.user \G，看到多了一个 我们刚刚指定的用户，看到 此记录 的 reppc：Y ，即说明该用户 具有复制的权限 此时，master已经配置ok了，查看状态：show master status; （5）先重启slave: (6) 查看salve的状态 show slave status ： 出现 Slave-IO-Running：YES，Slave-SQL-Running :YES ，则主从配置成功！ 五.测试（1）在matser中的 student 表中新增加一条记录： insert into student(name) values(‘徐邦启’); （2）在slave中查看student表： OK！ 六、常用命令1.查看二进制日志： 方法一：mysql&gt; show binlog events in ‘日志文件’ from 位置; 方法二：使用mysql工具查看日志文件（二进制日志中包含中各种DML和DDL） #mysqlbinlog –no-defaults mysql-bin.000001 查看指定文件binlog #mysqlbinlog –start-position=位置 日志路径，例如： mysqlbinlog -start-position=120 /var/lib/mysql/mysql-bin.000001 1234567891011# 查看某个时间段的二进制日志，并且输出到指定的文件mysqlbinlog --no-defaults --start-datetime=&quot;2018-12-12 13:00:00&quot; --stop-datetime=&quot;2018-12-12 14:40:00&quot; mysql-bin.000085 -vv --base64-output=decode-rows | more &gt;&gt; target.txt# 查看某个二进制日志中的指定命令，并且输出到指定的文件mysqlbinlog --no-defaults --base64-output=decode-rows -v -v mysql-bin.000085 | sed -n &apos;/### DELETE FROM `数据库名`.`表名`/,/COMMIT/p&apos; &gt; target.txt# 将@1、@2等一系列看不懂的符号转换为SQL语句cat target.txt | sed -n &apos;/###/p&apos; | sed &apos;s/### //g;s/\/\*.*/,/g;s/DELETE FROM/INSERT INTO/g;&apos; | sed -r &apos;s/(@4.*),/\1;/g&apos; | sed &apos;s/@[0-9]*\=//g&apos; &gt; test.sql# 查看某个二进制日志的某个时间段内 DELETE 语句出现的次数mysqlbinlog --no-defaults --start-datetime=&quot;2018-12-12 13:00:00&quot; --stop-datetime=&quot;2018-12-12 14:40:00&quot; mysql-bin.000085 -d 数据库名 mysql-bin.000085 -v|grep UPDATE | wc -l 2.查看中继日志（和查看二进制日志是同一个命令）： mysql&gt; show relaylog events in ‘日志文件’ from 位置; 3.查看mysql线程列表： mysql&gt; processlist; 4.查看mysql的配置文件（my.cnf）目录：whereis my.cnf 5.查看mysql的配置文件（my.cnf）目录：mysql –verbose –help | grep -C 1 ‘Default opt’ 6.清空master：reset master; 七、说明本文是一主一从，当然，一主多从也是一样的道理，在增加一台slave，然后在my.cnf修改，然后 change master to，在master上 修改 change master to master_host= ‘192.168.242.%’。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（九）-- 锁机制之行锁]]></title>
    <url>%2F2019%2F05%2F24%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E4%B9%9D%EF%BC%89-%E9%94%81%E6%9C%BA%E5%88%B6%E4%B9%8B%E8%A1%8C%E9%94%81%2F</url>
    <content type="text"><![CDATA[一、行锁概念及特点1.概念：给单独的一行记录加锁，主要应用于innodb表存储引擎 2.特点：在innodb存储引擎中应用比较多，支持事务、开销大、加锁慢；会出现死锁；锁的粒度小，并发情况下，产生锁等待的概率比较低，所以支持的并发数比较高。 二、数据库事务1.概念：事务是一系列操作组成的工作单元，该工作单元内的操作是不可分割的，也就是说要么全部都执行，要么全部不执行。 2.特性：ACID 原子性：事务是最小的工作单元，不可分割，要么都做，要么都不做 一致性：事务执行前和执行后的数据要保证正确性，数据完整性没有被破坏。 隔离性：在并发事务执行的时候，一个事务对其他事务不会产生影响。 持久性：一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的 三、多个事务并发执行 问题及解决方案1.问题 丢失更新：在没有事务隔离的情况下，两个事务同时更新一条数据，后一个事务 会 覆盖前面事务的更新，导致前面的事务丢失更新。 脏读：事务A先更新数据，但是没有提交，事务B读到了事务A没有提交的数据。 不可重复读：事务A中，先读到一条数据，事务A还没有结束，此时，事务B对该条数据进行了修改操作，事务A又读到了这条数据，事务A两次读到的数据不同。 幻读：事务A先读到一批数据，假设读到10条，事务B插入了一条数据，此时，事务A又读这一批数据，发现多了一条，好像幻觉一样。 注：不可重复读的重点是修改，同样的条件，你读取过的数据，再次读取出来发现值不一样。 幻读的重点在于新增或者删除，同样的条件，第 1 次和第 2 次读出来的记录数不一样。 2.解决方案–数据库隔离机制 1) 未提交读（read uncommitted）：这是数据库最低的隔离级别，允许一个事务读另一个事务未提交的数据。 解决了丢失更新，但是会出现脏读、不可重复读、幻读。 2) 提交读（read committed）：一个事务更新的数据 在提交之后 才可以被另一个事务读取，即一个事务不可以读取到另一个事务未提交的数据。 解决了丢失更新和脏读，但是会出现不可重复读和幻读。 3) 可重复读（repeatale read）：这是数据库默认的事务隔离级别，保证一个事务在相同条件下前后两次读取的数据是一致的。 解决了丢失更新、脏读和不可重复读，但是会出现幻读。 4) 序列化（serializable）：这是数据库最高的隔离级别。事务串行执行，不会交叉执行。 解决了所有的问题。 注：乐观所可以解决幻读。 四、行锁的特性查看mysql事务隔离级别：show variables like ‘tx_iso%’; 前提：set autocommit=0; // 设置自动提交事务为手动提交 123456789/* 行锁案例*/create table lock_two( id int, col int)engine=innodb;insert into lock_two(id,col) values (1,1);insert into lock_two(id,col) values (2,2);insert into lock_two(id,col) values (3,3); 1.在session1中执行update : update lock_two set col=11 where id=1; （1）分别在session1和session2中查询lock_two，看id为1的记录的col是否修改了。 发现session1 的记录修改了，session2中的记录没有被修改。 （2）在session1中执行commite后，然后再在session2中查询： 发现session2中的表数据改变了。 2.在session1中执行update：update lock_two set col=11 where id=1，不执行commit; 在session2中执行uodate ：update lock_two set col=11 where id=1，不执行commit; 发现session2中的update发生阻塞，并且超过一段时间报错。 3.在session1中执行update：update lock_two set col=22 where id = 2; 不执行commit 在session2中执行另一条update：update lock_two set col=33 where id = 3; 此时，session2中的update发生阻塞，在没发生错误的情况下，session1执行commit，session2中的update会马上执行。 4.在lock_two中创建索引， 12create index idx_id on lock_two(id);create index idx_col on lock_two(col); 然后重复第3步， 发现session2可以更新，不会产生阻塞。因为用上了索引，相当于行锁。 结论：如果没有用上索引，行锁变成表锁 五、手动锁一行记录格式begin; 1select * from lock_two where id=2 for update; 在session1中执行上面语句，在ssesion2中可以查看，但是不可以修改 sesion1中的for update 的记录。 当session1中执行commit后，seesion2中的update立刻执行。 六、间隙锁1.定义 在范围查找的情况下， innodb会给范围条件中的数据加上锁，无论数据是否是否真实存在。 2.例子 在session1中update：update lock_two set col=666 where id&gt;2 and id&lt;8; 1) 在session2中执行insert：insert into lock_two values(9,99); 插入执行成功！ 2) 在session2中执行insert：insert into lock_two values(7,77); 插入阻塞，一段时间后报错！ 执行select：select * from lock_two where id=4; 查询成功！ 建议：在innodb中，因为有间隙锁的存在，最好在where中少使用这种范围查找。 七、查看行锁的信息show status like &#39;innodb_row_lock%&#39;; 说明： Innodb_row_lock_current_waits ：当前正在等待的数量 Innodb_row_lock_time: 从启动到现在锁定的总时长，单位是ms Innodb_row_lock_time_avg :锁等待的平均时长 Innodb_row_lock_time_max：等待锁时间最长的一个时间 Innodb_row_lock_waits：总共的等待次数]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（八）-- 锁机制之表锁]]></title>
    <url>%2F2019%2F05%2F23%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E5%85%AB%EF%BC%89-%E9%94%81%E6%9C%BA%E5%88%B6%E4%B9%8B%E8%A1%A8%E9%94%81%2F</url>
    <content type="text"><![CDATA[数据库的锁主要用来保证数据的一致性的。MyISAM存储引擎只支持表锁，InnoDB存储引擎既支持行锁，也支持表锁，但默认情况下是采用行锁。 一、锁分类1.按照对数据操作的类型分：读锁，写锁 读锁：也称为共享锁。 针对同一资源，多个并发读操作可以并行执行，并且互不影响，但是不能写 写锁：也称排它锁。当前线程写数据的时候，会阻塞其它线程来读取数据 或者 写数据 注：读锁和写锁都是阻塞锁。 2.按照数据操作的粒度：表锁，行锁，页锁 表锁：开销小，加锁快，主要在myisam存储引擎中出现。特点：锁住整个表，开销小，加锁快，无死锁情况， 锁的粒度大，在并发情况下，产生锁等待的概率比较高，所以说，支持的并发数比较低，一般用于查找 行锁：开销大，加锁慢，锁定单独的某个表中的某一行记录，主要用于innodb存储引擎。特点：有死锁情况，锁定粒度最小，发生锁冲突的概率最低，支持的并发数也最高 页锁：开销和加锁时间界于表锁和行锁之间。会出现死锁，锁定粒度界于表锁和行锁之间，并发度一般 二、加锁与解锁1.手动增加表锁 lock table 表名 [read|write]，表名 [read|write]… 2.解锁 unlock tables; 3.查看哪些表被锁 show open tables; 三、表锁案例1.读锁 12345678create table lock_one( id int primary key auto_increment, col int)engine=myisam;insert into lock_one(col) values (1);insert into lock_one(col) values (2);insert into lock_one(col) values (3); 下面我们模拟两个用户，即两个线程连接数据库，开启两个xsheel窗口，连接到mysql： 1) 在会话1中对lock_one表增加读锁 1lock table lock_one read; 2) 在当前会话（会话1）中是否可以select该表呢，也就是说对 lock_one增加了读锁后，在当前会话中是否可以读呢？ 1select * from lock_one; 答案是可以的。 3) 在另一个会话中（会话2）是否可以select该表呢？ 答案也是可以的。 4) 那么在会话1中是否可以查询其他表呢？ 例如，查询 users表：select * from users; 我们发现是不可以查询其他表的，这是因为当前会话已经对lock_one表加上了锁，即当前线程锁住了lock_one表，只可以操作lock_one表，就不可以查询其他的表。 5) 问题来了，会话2是否可以查询其他表呢？ 1select * from users; 我们发现是可以的。因为会话2和会话1是没有关系的，会话2查询会话1锁住的表都可以，查询没有锁住的 肯定是可以的。 6) 在会话1中是否可以更新（增删改）锁住的lock_one表呢？ 1update lock_one set col=66 where id=1; 发现是不可以的，因为我们对 lock_one表加了 读锁，所以是不可以 进行写操作的。 7) 在会话2中是否可以更新（增删改）会话1中锁住的lock_one表呢？ 我们发现是没有执行结果的，也就是说 正在等待更新，在阻塞等待中。因为我们在会话1中对lock_one中增加了读锁，其他人只有读的操作，没有写的操作。 8) 在会话1中 对lock_one进行解锁时，会话2中的更新（增删改）操作 就会立即执行。 2.写锁 1) 在会话1中对lock_one表增加写锁 1lock table lock_one write; 2) 在会话1中查询该表 1select * from lock_one; 我们发现是可以的。 3) 在会话2中查询该表 我们发现是没有执行结果的，也就是说 处于阻塞状态。因为写锁是排它锁，其他用户线程不可以读取当前锁住的表，只有解锁之后 其他用户线程才可以执行select 4) 在会话1中对lock_one进行写锁后，会话1会否可以查询其他表呢？ 1select * from users; 我们发现是不可以的。道理和读锁的时候一样，当前会话已经对lock_one表加上了锁，即当前线程锁住了lock_one表，只可以操作lock_one表，就不可以查询其他的表。 5) 那么在会话2中是否可以查询其他表呢？ 答案肯定是可以的。因为之和锁的表有关系，和其他表没有任何关系。 6) 在会话1中是否可以进行写（增删改）操作呢？ 答案一定是可以的。因为会话1对lock_one表进行了写锁操作，也就是只可以写。 7) 在会话2中是否可以进行写（增删改）操作呢？ 我们发现是不可以的。因为写锁是排它锁，也就是只可以当前线程操作锁住的表，其他用户线程需要等到解锁之后才可以操作该表。 3.总结 1) 甲对表A加了读锁 甲对表A可以执行读（查询）操作，但不可以执行写（增删改）操作 甲对其他表不可以执行读写（增删改查）操作 乙对表A可以执行读（查询）操作，但不可以执行写（增删改）操作 乙对其他表可以执行读写（增删改查）操作 2) 甲对表A加了写锁 甲对表A可以执行读写（增删改查）操作 甲对其他表不可以执行读写（增删改查）操作 乙对表A不可以执行读写（增删改查）操作 乙对其他表可以执行读写（增删改查）操作 四、MyISAM存储引擎中锁特点 执行select语句的时候，会自动给涉及的表加上表锁，在执行更新操作时，会自动给表加上写锁 MyISAM存储引擎比较适合作为以查询为主的表存储引擎，不适合写为主的表存储引擎，因为加写锁后，是锁住整个表，其他用户线程不能做任何操作，这样会导致大量用户线程阻塞的情况。 五、表锁的状态查询1.查询指令 show status like ‘table_lock%’; 说明： Table_locks_immediate：表示可以立即获取锁的查询次数，每获取一次锁就增加1 Table_locks_waited：锁等待的次数（重要，如果这个值的大，则说明锁表的次数多，需要优化，通过 show open tables，查看哪些表锁了，然后分析为什么会锁）。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（七）-- 慢查询]]></title>
    <url>%2F2019%2F05%2F23%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%83%EF%BC%89-%E6%85%A2%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[1.慢查询的用途 它能记录下所有执行超过long_query_time时间的SQL语句，帮我们找到执行慢的SQL，方便我们对这些SQL进行优化。 2.查看是否开启慢查询 show variables like ‘slow_query%’; slow_query_log = off，表示没有开启慢查询 slow_query_log_file 表示慢查询日志存放的目录 3.开启慢查询（需要的时候才开启，因为很耗性能，建议使用即时性的） 方式一：（即时性的，重启mysql之后失效，常用的） set global slow_query_log=1; 或者 set global slow_query_log=ON; 开启之后 我们会发现 /var/lib/mysql下已经存在 localhost-slow.log了，未开启的时候默认是不存在的。 方式二：（永久性的） 在/etc/my.cfg文件中的[mysqld]中加入： 12slow_query_log=ONslow_query_log_file=/var/lib/mysql/localhost-slow.log 4.设置慢查询记录的时间 查询慢查询记录的时间：show variables like ‘long_query%’，默认是10秒钟，意思是大于10秒才算慢查询。 我们现在设置慢查询记录时间为1秒：set long_query_time=1; 5.执行select count(1) from order o where o.user_id in (select u.id where users); 因为我们开启了慢查询，且设置了超过1秒钟的就为慢查询，此sql执行了24秒，所以属于慢查询。 我们在日志中查看： more /var/lib/mysql/localhost-slow.log， 我们可以看到查询的时间，用户，花费的时间，使用的数据库，执行的sql语句等信息。在生产上我们就可以使用这种方式来查看 执行慢的sql。 6.查询慢查询的次数：show status like ‘slow_queries’; 在我们重新执行刚刚的查询sql后，查询慢查询的次数会变为8 当然，用 more /var/lib/mysql/localhost-slow.log 也是可以看到详细结果的。 在生产中，我们会分析查询频率高的，且是慢查询的sql，并不是每一条查询慢的sql都需要分析。 7.慢查询日志分析工具Mysqldumpslow 由于在生产上会有很多慢查询，所以采用上述的方法查看慢查询sql会很麻烦，还好MySQL提供了慢查询日志分析工具Mysqldumpslow。 其功能是, 统计不同慢sql的出现次数(Count)，执行最长时间(Time)，累计总耗费时间(Time)，等待锁的时间(Lock)，发送给客户端的行总数(Rows)，扫描的行总数(Rows) （1）查询Mysqldumpslow的帮助信息，随便进入一个文件夹下，执行：mysqldumpslow –help 查看mysqldumpslow命令安装在哪个目录：whereis mysqldumpslow 说明： -s，是order的顺序，主要有c（按query次数排序）、t（按查询时间排序）、l（按lock的时间排序）、r （按返回的记录数排序）和 at、al、ar，前面加了a的代表平均数 -t，是top n的意思，即为返回前面多少条的数据 -g，后边可以写一个正则匹配模式，大小写不敏感的 -r：倒序 （2）案例：取出耗时最长的两条sql 格式：mysqldumpslow -s t -t 2 慢日志文件 mysqldumpslow -s t -t 2 /var/lib/mysql/localhost-slow.log 参数分析： 出现次数(Count), 执行最长时间(Time), 累计总耗费时间(Time), 等待锁的时间(Lock), 发送给客户端的行总数(Rows), 扫描的行总数(Rows), 用户以及sql语句本身(抽象了一下格式, 比如 limit 1, 20 用 limit N,N 表示). （3）案例：取出查询次数最多，且使用了in关键字的1条sql mysqldumpslow -s c -t 1 -g ‘in’ /var/lib/mysql/localhost-slow.log 这种方式更加方便，更加快捷！ 8.show profile 用途：用于分析当前会话中语句执行的资源消耗情况 （1）查看是否开启profile，mysql默认是不开启的，因为开启很耗性能 show variables like ‘profiling%’; （2）开启profile（会话级别的，关闭当前会话就会恢复原来的关闭状态） set profiling=1; 或者 set profiling=ON; （3）关闭profile set profiling=0; 或者 set profiling=OFF; （4）显示当前执行的语句和时间 show profiles; （5）显示当前查询语句执行的时间和系统资源消耗 show profile cpu,block io for query 4;（分析show profiles中query_id等于4的sql所占的CPU资源和IO操作） 或者直接 ： show profile for query 4;]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（六）-- in和exists]]></title>
    <url>%2F2019%2F05%2F23%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E5%85%AD%EF%BC%89-in%E5%92%8Cexists%2F</url>
    <content type="text"><![CDATA[in和exists哪个性能更优sql脚本： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/*建库*/create database testdb6;use testdb6;/* 用户表 */drop table if exists users;create table users( id int primary key auto_increment, name varchar(20));insert into users(name) values (&apos;A&apos;);insert into users(name) values (&apos;B&apos;);insert into users(name) values (&apos;C&apos;);insert into users(name) values (&apos;D&apos;);insert into users(name) values (&apos;E&apos;);insert into users(name) values (&apos;F&apos;);insert into users(name) values (&apos;G&apos;);insert into users(name) values (&apos;H&apos;);insert into users(name) values (&apos;I&apos;);insert into users(name) values (&apos;J&apos;);/* 订单表 */drop table if exists orders;create table orders( id int primary key auto_increment,/*订单id*/ order_no varchar(20) not null,/*订单编号*/ title varchar(20) not null,/*订单标题*/ goods_num int not null,/*订单数量*/ money decimal(7,4) not null,/*订单金额*/ user_id int not null /*订单所属用户id*/)engine=myisam default charset=utf8 ;delimiter $$drop procedure batch_orders $$/* 存储过程 */create procedure batch_orders(in max int)begindeclare start int default 0;declare i int default 0;set autocommit = 0; while i &lt; max do set i = i + 1; insert into orders(order_no,title,goods_num,money,user_id) values (concat(&apos;NCS-&apos;,floor(1 + rand()*1000000000000 )),concat(&apos;订单title-&apos;,i),i%50,(100.0000+(i%50)),i%10); end while;commit;end $$delimiter ;/*插入1000万条订单数据*/call batch_orders(10000000); /*插入数据的过程根据机器的性能 花费的时间不同，有的可能3分钟，有的可能10分钟*/ 上面的sql中 订单表中（orders） 存在user_id，而又有用户表（users），所以我们用orders表中user_id和user表中的id 来in 和 exists。 结果 1.where后面是小表 （1）select count(1) from orders o where o.user_id in(select u.id from users u); （2）select count(1) from orders o where exists (select 1 from users u where u.id = o.user_id); 2.where后面是大表 （1）select count(1) from users u where u.id in (select o.user_id from orders o); （2）select count(1) from users u where exists (select 1 from orders o where o.user_id = u.id); 分析 我们用下面的这两条语句分析： 12select count(1) from orders o where o.user_id in(select u.id from users u);select count(1) from orders o where exists (select 1 from users u where u.id = o.user_id); 1.in：先查询in后面的users表，然后再去orders中过滤，也就是先执行子查询，结果出来后，再遍历主查询，遍历主查询是根据user_id和id相等查询的。 即查询users表相当于外层循环，主查询就是外层循环 小结：in先执行子查询，也就是in()所包含的语句。子查询查询出数据以后，将前面的查询分为n次普通查询(n表示在子查询中返回的数据行数) 2.exists：主查询是内层循环，先查询出orders，查询orders就是外层循环，然后会判断是不是存在order_id和 users表中的id相等，相等才保留数据，查询users表就是内层循环 这里所说的外层循环和内层循环就是我们所说的嵌套循环，而嵌套循环应该遵循“外小内大”的原则，这就好比你复制很多个小文件和复制几个大文件的区别 小结：如果子查询查到数据，就返回布尔值true；如果没有，就返回布尔值false。返回布尔值true则将该条数据保存下来，否则就舍弃掉。也就是说exists查询，是查询出一条数据就执行一次子查询 结论 小表驱动大表。 in适合于外表大而内表小的情况，exists适合于外表小而内表大的情况。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（五）-- using filesort]]></title>
    <url>%2F2019%2F05%2F23%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%94%EF%BC%89-using-filesort%2F</url>
    <content type="text"><![CDATA[Using filesort表示在索引之外，需要额外进行外部的排序动作。导致该问题的原因一般和order by有者直接关系，一般可以通过合适的索引来减少或者避免。 一、order by产生using filesort详解1.首先建表和索引（以下使用的sql版本是5.5.54） 12345678910111213141516171819/*课程表*/create table course( id int primary key auto_increment,/* 主键自增*/ title varchar(50) not null,/* 标题*/ category_id int not null,/* 属于哪个类目*/ school_id int not null,/* 属于哪个学校*/ buy_times int not null,/* 购买次数*/ browse_times int not null/* 浏览次数*/);insert into course(title,category_id,school_id,buy_times,browse_times) values(&apos;java课程&apos;,1,1,800,8680);insert into course(title,category_id,school_id,buy_times,browse_times) values(&apos;android课程&apos;,2,1,400,8030);insert into course(title,category_id,school_id,buy_times,browse_times) values(&apos;mysql课程&apos;,3,2,200,2902);insert into course(title,category_id,school_id,buy_times,browse_times) values(&apos;oracle课程&apos;,2,2,100,6710);insert into course(title,category_id,school_id,buy_times,browse_times) values(&apos;C#课程&apos;,1,3,620,2890);insert into course(title,category_id,school_id,buy_times,browse_times) values(&apos;PS课程&apos;,4,4,210,4300);insert into course(title,category_id,school_id,buy_times,browse_times) values(&apos;CAD课程&apos;,5,1,403,6080);/*在category_id和buy_times上建立组合索引*/create index idx_cate_buy on course(category_id,buy_times); 2.order by 和 group by 会产生 using filesort的有哪些？ （1）explain select id from course where category_id&gt;1 order by category_id; 根据最左前缀原则，order by后面的的category_id会用到组合索引 （2）explain select id from course where category_id&gt;1 order by category_id,buy_times; 根据最左前缀原则，order by后面的的category_id buy_times会用到组合索引，因为索引就是这两个字段 （3）explain select id from course where category_id&gt;1 order by buy_times; 根据最左前缀原则，order by后面的字段是缺少了最左边的category_id，所以会产生 using filesort （4）explain select id from course where category_id&gt;1 order by buy_times,category_id; order by后面的字段顺序不符合组合索引中的顺序，所以order by后面的不会走索引，即会产生using filesort （5）explain select id from course order by category_id; 根据最左前缀原则，order by后面存在索引中的最左列，所以会用到索引 （6）explain select id from course order by buy_times; 根据最左前缀原则，order by后面的字段 没有索引中的最左列的字段，所以不会走索引，会产生using filesort （7）explain select id from course where buy_times &gt; 1 order by buy_times; 根据最左前缀原则，order by后面的字段 没有索引中的最左列的字段，所以不会走索引，会产生using fillesort （8）explain select id from course where buy_times &gt; 1 order by category_id; 根据最左前缀原则，order by后面的字段存在于索引中最左列，所以会走索引 （9）explain select id from course order by buy_times desc,category_id asc; 根据最最左前缀原则，order by后面的字段顺序和索引中的不符合，则会产生using filesort （10）explain select id from course order by category_id desc,buy_times asc; 这一条虽然order by后面的字段和索引中字段顺序相同，但是一个是降序，一个是升序，所以也会产生using filesort，同时升序和同时降序就不会产生using filesort了 总结：终上所述，（3）（4）（6）（7）（9）（10）都会产生using filesort.]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（四）-- MySQL explain详解]]></title>
    <url>%2F2019%2F05%2F21%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E5%9B%9B%EF%BC%89-MySQL-explain%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[MySQL中的explain命令显示了mysql如何使用索引来处理select语句以及连接表。explain显示的信息可以帮助选择更好的索引和写出更优化的查询语句。 一、格式explain + select 语句； 例如：explain select * from tb_student; 二、5.5和5.7版本explain的区别5.7之后的版本默认会有 partitions 和 filtered两列，但是5.5版本中是没有的，需要 使用explain partitions select ……来显示带有partitions 的列， 使用explain extended select ……来显示带有filtered的列。 本文是基于5.5.54版本的。 三、explain的作用1.描述MySQL如何执行查询操作、执行顺序，使用到的索引，以及MySQL成功返回结果集需要执行的行数。 2.可以帮助我们分析 select 语句,让我们知道查询效率低下的原因,从而改进我们的查询，让查询优化器能够更好的工作 查询优化器的作用： 1.优化select 语句，分析哪些是常量表达式（例如id=1），以及分析哪些表达式可以直接转换成常量的 2.对where条件进行简化和转换，如去掉无用条件，调整条件结构等 3.读取涉及的表的统计信息，并计算分析（例如返回的行数，索引信息等），最终得出执行计划 四、执行计划（QEP）包含的信息 id：标识符，表示执行顺序 select _type：查询类型 table：输出行所引用的表 partitions：使用的哪个分区，需要结合表分区才可以看到 type：表示按某种类型来查询，例如按照索引类型查找，按照范围查找。从最好到最差的连接类型为const、eq_reg、ref、range、indexhe和all possible_keys：可能用到的索引，保存的是索引名称，如果是多个索引的话，用逗号隔开 key：实际用到的索引，保存的是索引名称，如果是多个索引的话，用逗号隔开 key_len：表示本次查询中，所选择的索引长度有多少字节 ref：显示索引的哪一列被使用了，如果可能的话，是一个常数 rows：显示mysql认为执行查询时必须要返回的行数 filtered：通过过滤条件之后对比总数的百分比 extra：额外的信息，例如：using file sort ，using where， using join buffer，using index等 五、执行计划中各个参数的详解1.id 表示select标识符，同时表明执行顺序，也就是说id是一个查询的序列号，查询序号即为sql语句执行的顺序。 （1）当id值相同时，按从上到下的顺序执行 （2）当id全部不同时，按id从大到小执行 （3）当id部分不同时，先执行id大的，id相同的，按从上到下的顺序执行 2.select_type （1）simple：表示简单的select，没有union和子查询 （2）primary：最外面的查询 或者 主查询，在有子查询的语句中，最外面的select查询就是primary （3）subquery：子查询 （4）union：union语句的第二个或者说是后面那一个select （5）union result：union之后的结果 （6）dependent unoin：unoin 中的第二个或随后的 select 查询，依赖于外部查询的结果集 （7）dependent subquery：子查询中的第一个 select 查询，依赖于外部 查询的结果集 （8）derived：衍生表（5.7版本中不存在这一个） 3.table 通常是表名，或者表的别名，或者一个为查询产生临时表的标示符（如派生表、子查询、集合） 4.partitions 使用的哪些分区（对于非分区表值为null），在5.5版本中需要加上explain partitions select ….. 5.type （1）const：表中最多有一个匹配行，const用于比较primary key 或者unique索引。因为只匹配一行数据，所以很快 （2）eq_ref：唯一性索引扫描，对于每个来自于前面的表的记录，从该表中读取唯一一行 （3）ref：非唯一性索引扫描，对于每个来自于前面的表的记录，所有匹配的行从这张表取出 （4）ref_or_null：类似于ref，但是可以搜索包含null值的行，例如：select * from student where address=’xxx’ or address is null，需要在address建立索引。 （5）index_merge：查询语句用到了一张表的多个索引时，mysql会将多个索引合并到一起 （6）range：按指定范围（如in、&lt;、&gt;、between and等，但是前提是此字段要建立索引）来检索，很常见。如：select * from student where id &lt; 5，id上要有索引。 （7）index：全”表“扫描，但是是在索引树中扫描，通常比ALL快，因为索引文件通常比数据文件小，index扫描是通过二叉树的方式扫描，而all是扫描物理表。（也就是说虽然all和index都是读全表，但index是从索引中读取的，而all是从硬盘中读的）。例如：select name from student，但name字段上需要建立索引，也就是查询的字段属于索引中的字段。 （8）all：全表扫描，扫描完整的物理表，此时就需要优化了。 6.possible_keys 指出 MySQL 能在该表中可能使用的索引，显示的是索引的名称，多个索引用逗号隔开，如果没有，则为null。 7.key MySQL决定实际用到的索引，显示的是索引的名称，多个索引用逗号隔开，如果没有，则为null 8.key_len 当用到组合索引的时候判断索引是否完全用上。 实例：假设student表中有id int,name char(20) DEFAULT NULL,address varchar(20) DEFAULT NULL,remark varchar(20) NOT NULL 字段，建立的索引是 idx_address_remark（在address和remark上建立的组合索引） 查询的sql是：select from student where address=’深圳’ and remark=’java coder’，此时，执行计划中的key_len是 （203+1+2）+ （20*3+2）= 125，那么这个是怎么得来的呢？ 解析：20表示建表的时候 varchar(20) ，3表示utf8字符集占用3个字节，1表示MySQL需要1个字节表示null，2表示变长字段（varchar是变长的）。 假设drop掉刚刚建立的索引，新建索引 idx_name_address（在name和address上建立组合索引） 查询的sql是：select from student where name=’xbq’ and address=’深圳’，此时，执行计划中的key_len是 （203+1）+ （20*3+2）= 123，那么这个值是怎么得来的呢？ 解析：20表示建表的时候 char(20) ，3表示utf8字符集占用3个字节，1表示MySQL需要1个字节标识null，即 20*3+1，后面的同样的道理。 key_len只计算where条件用到的索引长度，而排序和分组就算用到了索引，也不会计算到key_len中。 计算key_len的公式： varchr(10)变长字段且允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1) + 1(NULL) + 2(变长字段) varchr(10)变长字段且不允许NULL = 10 *( character set：utf8=3,gbk=2,latin1=1) + 2(变长字段) char(10)固定字段且允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+1(NULL) char(10)固定字段且不允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1) 9.ref 显示索引的哪一列被使用了，如果可能的话，是一个常数 10.rows 显示mysql认为执行查询时必须要返回的行数，可结合type和key分析，没有用上索引的情况下，会全表扫描。rows的值越小越好，说明检索的数据少 11.filtered 给出了一个百分比的值，这个百分比值和rows列的值一起使用，可以估计出那些将要和执行计划中的前一个表（前一个表就是指id列的值比当前表的id小的表）进行连接的行的数目。 这一列在5.5版本中，需要加上 explain extended select ….。 12.extra 此字段显示一些额外的信息，但是此字段的部分值具有优化的参考意义。 （1）using where：表示查询使用了where 语句来处理结果 （2）using index：表示使用了覆盖索引。这个值重点强调了只需要使用索引就可以满足查询表的要求，不需要直接访问表数据。 （3）using join buffer：这个值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进性能 （4）using filesort：这是 order by 语句的结果。这可能是一个CPU密集型的过程。using filesort表示出现了文件内排序，表示很不好的现象，必须要优化，特别是大表，可以通过选择合适的索引来改进性能，用索引来为查询结果排序。 （5）using temporary：mysql需要创建一张临时表来保存中间结果。 也就是说，需要先把数据放到临时表中，然后从临时表中获取需要的数据。出现这种临时表，也是必须需要优化的地方，特别是数据量大的情况。两个常见的原因是在来自不同表的列上使用了distinct，或者使用了不同的 order by 和 group by 列。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（三）-- 索引]]></title>
    <url>%2F2019%2F05%2F21%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%89%EF%BC%89-%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[一、什么是索引及索引的特点索引是一种数据结构 索引的特点：查找速度快，排好序，数据结构 索引的数据结构类型有：BTREE索引和HASH索引，下面展示的是BTREE索引。 BTREE：balance tree （平衡树） BTREE的特点实例： 假设有一张表，表中的数据为下图中的左侧，则索引中数据为下图中的右侧： 如果查询id为9，name为ii的，在表中需要查询9次，但是在二叉树中需要查询3次。 二、索引的“类型”1.聚集索引：节点就是数据本身，即索引表中存的就是数据本身 2.非聚集索引：节点仍然是索引节点，只不过有指向对应数据块的指针，上面所说的BTREE索引就是非聚集索引 聚集索引的速度比非聚集索引快。 三、索引的类型1.单列索引（一个索引只包含一个列,一个表可以有多个单列索引） 1-1.主键索引：primary key 创建主键索引，有两种方式，其中t1和t2位表名，id为列： 1）建表的时候创建：create table t1 (id int primary key); 2）通过alter语句：alter table t2 add primary key(id); 1-2.普通索引：index，最基本的索引 创建普通索引： （1）create index 索引名 on 表(列)， 例如： create index idx_id on t3(id); （2）通过alter语句：alter table t4 add index(id); 1-3.唯一索引：unique 创建唯一索引： （1）建表的时候创建：create table t5 (id int unique); （2）create unique index 索引名 on 表(列)， 例如：create unique index idx_id on t5(id); 注意：unique的字段可以为null，也可以重复，“”不可以重复出现。 1-4.全文索引：fulltext 数据库自带的全文索引，对中文支持不友好，可以借助第三方的框架，如：sphinx（斯芬克斯）、coreseek 2.多列索引（组合索引，一个组合索引包含两个或两个以上的列） 创建：create index idx_列名1_列名2 on 表名（列名1，列名2） 实例： 建表：create table student(id int,name varchar(20),address varchar(20),remark varchar(20)); 建组合索引：create index idx_name_address_remark on student(name,address,remark); 查询sql是否使用到了索引，可以使用explain进行分析，后续会给出介绍。 在使用查询的时候遵循mysql组合索引的“最左前缀”，where时的条件要按建立索引的时候字段的排序方式，下面都是基于多列索引讲述的： 1、不按索引最左列开始查询 （1） where address=’深圳’ 不会走索引 （2）where address = ‘深圳’ and remark=’程序员’ 不会走索引 2、查询中某个列有范围查询，则其右边的所有列都无法使用查询（多列查询） where name=’xbq’ and address like ‘%深%’ and remark=’程序员’ ，该查询只会使用索引中的前两列，因为like是范围查询 3、查询中第一个索引字段出现like ‘%xxx%’或者’%xxx’，不会走索引 4.查询中多条件用or连接，此类型和1相似 那么对于索引怎么删除呢？删除索引：alter table 表名 drop index 索引名称 查询索引有两种方式： show index from t1; show keys from t1; 四、索引的优点和缺点优点： 1.可以通过建立唯一索引或者主键索引,保证数据库表中每一行数据的唯一性. 2.提高检索速度，降低磁盘读取IO 索引是排序好的，不需要进行全表扫描，降低了数据排序的运算成本，也就是降低了CPU的消耗 缺点： 1.索引也需要存储，所以也需要空间，实际上索引也是一张表，保存了索引字段的值和指向实体表的指针 2.降低更新（增删改）表的速度，更新不仅仅只是数据本身，如果数据正好是索引字段，同时需要更新索引信息 当索引字段对应的数据改变了，则索引表也会改变，例如，当图书馆中 的书柜和书，书的类型为科普类，书柜上的目录也是科普类， 当此书柜中的书改变了位置的时候，则对应的书柜的目录表也要改变。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（二）-- 数据类型，SQL，八种连接]]></title>
    <url>%2F2019%2F05%2F20%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%EF%BC%8CSQL%EF%BC%8C%E5%85%AB%E7%A7%8D%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[一、mysql数据类型优化原则 能够使用最小数据类型存储时，尽量使用最小数据类型存储（前提是要评估字段值的真实数据存储范围） eg：create table aaa(id1 int(10) zerofill,id2 int (10)); (1) int(n) 中的 n代表宽度，表示没有设定宽度的时候，用0填充，需要配合zerofill来使用。 eg: insert into aaa values(12,11); (2) varchar 存储变长的字符串，即根据存入的值的长度来动态变化 存储空间： 非空：65533字节 可空：65532字节 优点：节省空间，操作简单 缺点：在update的时候，如果数据的长度变化了，就会去申请空间，从而有额外的工作，对性能有影响。 应用场景：1.字符串最大长度比平均真实值大的比较多，这个时候就会用。 2.更新频率比较低的字符串字段 (3) char 存储定长的字符串 存储长度：create table t (char(10) not null) 非空：存储255字节 可空：存储254字节 应用场景：1.适合存储比较短的字符串 2.存储固定的长度或者存储长度比较接近的字符串 。例如：手机号、MD5加密的密码、邮编等 3.经常变更的数据（不需要释放空间重新申请，即不会产生内存碎片） 4.存储比较短的字符串值，例如：Y/N 有趣的例子： 123create table test_char(name char(10));insert into test_char values(&apos;abc&apos;),(&apos; abc&apos;),(&apos;abc &apos;);select concat(&quot;&apos;&quot;,name,&quot;&apos;&quot;) from test_char; 结果如下： 结论：如果插入的值末尾有空格，会自动截取掉。 (4) text：存储变长的非二进制的字符串 (5) blog：用于存储二进制数据，大二进制数据，也就是可以存储流，图片或者音频文件 (6) 日期时间类型 3. 尽量避免NULL 4. 尽量使用简单类型 建表的时候可以将为null的 赋其他默认值，如：字符串的设置’’，数据类型的设为0，不要将null设为默认值。 二、SQL性能下降原因 I/O吞吐量小，形成了瓶颈效应 CPU性能差 内存不足，磁盘空间不足 没有索引或者没有用到索引 查询语句太烂，没有优化（各种join，子查询） 检索返回的数据量过大 返回了不必要的行和列 锁或者死锁（程序设计的缺陷） 配置参数没有优化 插曲：查询Linux性能参数 top、htop、free -m、df -hl 三、SQL手写顺序12345678select distinct &lt;select_list&gt; from &lt;left_table&gt; &lt;join_type&gt; join &lt;right_table&gt; on &lt;join_condition&gt; group by &lt;group_by_list&gt; having &lt;having_condition&gt; order by &lt;order_bu_list&gt; limit &lt;limit_number&gt; 四、SQL读取顺序 from：左表和右表的笛卡尔积，产生虚拟表v1 on：对v1进行筛选，根据join-condition过滤，产生v2 join：如果是left join，就把左表在v2的结果通过on过滤，通过右表的外部行过滤，产生v3 where：过滤条件，产生v4 group by ：分组，产生v5 having：过滤条件，产生v6 select：load出指定的列，产生v7 distinct：排重，产生v8 order by：排序，产生v9 limit：取出指定的行，并返回结果 五、常用的JOIN SQL 左外连接 右外连接 内连接 全连接 交叉连接 全外连接 六、实例join 建表语句 12create database test2017;use test2017; 左表t1 123456create table t1(id int not null,name varchar(20));insert into t1 values(1,&apos;t1a&apos;);insert into t1 values(2,&apos;t2a&apos;);insert into t1 values(3,&apos;t3a&apos;);insert into t1 values(4,&apos;t4a&apos;);insert into t1 values(5,&apos;t5a&apos;); 右表t2 123456create table t2(id int not null,name varchar(20));insert into t2 values(2,&apos;t2b&apos;);insert into t2 values(3,&apos;t2c&apos;);insert into t2 values(4,&apos;t2d&apos;);insert into t2 values(5,&apos;t2f&apos;);insert into t2 values(6,&apos;t2a&apos;); 1.笛卡尔积（相当于两个for循环） 2.左连接 left join left outer join 左表全部保留，右表关联不上的用null表示。结果 t1 中存在id为1的，t2中没有，则对应t2中的记录用null表示。 左表应该都是小表。 3.右连接 right join 右表全部保留，左表关联不上的用null表示。 4.内连接 inner join 两个表的公共部分。 5.查询左表中独有部分 6.查询右表中独有数据 7.全连接 在MySQL中没有full join 8. 查询t1和t2的独有数据部分]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL性能优化（一）-- 存储引擎和三范式]]></title>
    <url>%2F2019%2F05%2F19%2FMySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89-%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E5%92%8C%E4%B8%89%E8%8C%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一、MySQL存储引擎存储引擎说白了就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法。因为在关系数据库中数据的存储是以表的形式存储的，所以存储引擎也可以称为表类型（即存储和操作此表的类型）。MySQL5.5以后默认使用InnoDB存储引擎。 下图是MySQL中各种存储引擎的对比。 1.MyISAM： 这种引擎是mysql最早提供的。它不支持事务，也不支持外键，尤其是访问速度快。这种引擎又可以分为静态MyISAM、动态MyISAM 和压缩MyISAM三种： 1) 静态MyISAM：如果数据表中的各数据列的长度都是预先固定好的，服务器将自动选择这种表类型。因为数据表中每一条记录所占用的空间都是一样的，所以这种表存取和更新的效率非常高。 当数据受损时，恢复工作也比较容易做。这种存储方式的优点是存储非常迅速，容易缓存，出现故障容易恢复；缺点是占用的空间通常比动态表多。 2) 动态MyISAM：如果数据表中出现varchar、xxxtext或xxxBLOB字段时，服务器将自动选择这种表类型。相对于静态MyISAM，这种表存储空间比较小，但由于每条记录的长度不一，所以 多次修改数据后，数据表中的数据就可能离散的存储在内存中，进而导致执行效率下降。同时，内存中也可能会出现很多碎片。因此，这种类型的表要经常用optimize table命令或者myisamchk -r命令 或 优化工具来整理碎片、改善性能，并且出现故障的时候恢复相对比较困难。 3) 压缩MyISAM：以上说到的两种类型的表都可以用myisamchk工具压缩。这种类型的表进一步减小了占用的存储，但是这种表压缩之后不能再被修改。另外，因为是压缩数据，所以这种表在读取的时候要先时行解压缩。但是，不管是何种MyISAM表，目前它都不支持事务，行级锁和外键约束的功能。 2.Merge： 这种类型是MyISAM类型的一种变种。合并表是将几个相同的MyISAM表合并为一个虚表。常应用于日志和数据仓库。 3.InnoDB： InnoDB表类型可以看作是对MyISAM的进一步更新产品，它提供了事务、行级锁机制和外键约束的功能。对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多 的磁盘空间以保留数据和索引。 4.memory： 这种类型的数据表只存在于内存中。它使用HASH索引，所以数据的存取速度非常快。因为是存在于内存中，所以这种类型常应用于临时表中，但是一旦服务器关闭，表中的数据就会丢失，但表还会继续存在。默认情况下，memory数据表使用散列索引，利用这种索引进行“相等比较”非常快，但是对“范围比较”的速度就慢多了。因此，散列索引值适合使用在”=”和”&lt;=&gt;”的操作符中，不适合使用在”&lt;”或”&gt;”操作符中，也同样不适合用在order by字句里。如果确实要使用”&lt;”或”&gt;”或betwen操作符，可以使用btree索引来加快速度。 存储在MEMORY数据表里的数据行使用的是长度不变的格式，因此加快处理速度，这意味着不能使用BLOB和TEXT这样的长度可变的数据类型。VARCHAR是一种长度可变的类型，但因为它在MySQL内部当作长度固定不变的CHAR类型，所以可以使用。 使用USING HASH/BTREE来指定特定到索引：create index mem_hash using hash on tab_memory(city_id); 5.archive： 这种类型只支持select 和 insert语句，而且不支持索引。常应用于日志记录和聚合分析方面。 二、存储引擎如何选择 是否支持事务 检索和添加速度 锁机制 缓存 是否支持全文索引 是否支持外键 三、MyISAM和InnoDB对比 四、什么时候使用MyISAM和InnoDB MyISAM：读事务要求不高，以查询和插入为主，可以使用这个引擎来创建表，例如各种统计表。 InnoDB：对事务要求高，保存的是重要的数据，例如交易数据，支付数据等，对用户重要的数据，建议使用InnoDB。 五、对存储引擎的操作1.查看数据库默认的存储引擎：show engines; 或者 show variables like ‘default_storage_engine’; 2.查看表的存储引擎： 1) 显示表的创建语句：show create table tablename; 2) 显示表的当前状态值：show table status like ‘tablename’ \G 3) 设置或修改表的存储引擎 创建数据库表时设置存储存储引擎的基本语法是： 1234create table tableName( columnName(列名1) type(数据类型) attri(属性设置), columnName(列名2) type(数据类型) attri(属性设置)， ....) engine = engineName 修改存储引擎，可以用命令 Alter table tableName engine = engineName 五、配置和数据文件1.配置文件默认位置 Linux: /etc/my.cnf Windows: my.ini 2.数据文件位置 1) 查看数据文件位置的命令： show variables like ‘%datadir%’ ; 2) 数据文件格式： InnoDB：frm（存储的表结构）、ibd（存储的数据和索引） MyISAM：frm（存储的表结构）、MYD（存储的数据）、MYI（存储的索引） 六、数据库表设计1.第一范式 1) 概念：列不可分。每一列都是不可分割的基本数据项。 2) 例子：假设我们有一个学生表，字段包括：id,name,age,contact，如下： 当我们需要根据QQ来查询学生的时候，就查询不出，所以以上的设计就不符合1NF。我们可以将contact字段拆分为phone和QQ，如下： 这样就满足1NF了。 2.第二范式 1) 概念：1NF的基础上面，非主属性完全依赖于主关键字。 2) 例子：学生表：(学号, 姓名, 年龄, 课程名称, 成绩, 学分) ，从字段可以看出，此表联合主键是（学号，课程名称）。 存在如下决定关系： (学号, 课程名称) → (姓名, 年龄, 成绩, 学分) (课程名称) → (学分) (学号) → (姓名, 年龄) 其中，姓名、年龄、学分是部分依赖于主键的，而成绩是完全依赖于主键的，存在部分依赖关系，所以不满足第二范式。 这会造成如下问题： (1) 数据冗余： 同一门课程由n个学生选修，”学分”就重复n-1次；同一个学生选修了m门课程，姓名和年龄就重复了m-1次。 (2) 更新异常： 若调整了某门课程的学分，数据表中所有行的”学分”值都要更新，否则会出现同一门课程学分不同的情况。 (3) 插入异常： 假设要开设一门新的课程，暂时还没有人选修。这样，由于还没有”学号”关键字，课程名称和学分也无法记录入数据 库。 ​ (4) 删除异常： 假设一批学生已经完成课程的选修，这些选修记录就应该从数据库表中删除。但是，与此同时，课程名称和学分信息也被删除了。很显然，这也会导致插入异常。 问题就在于存在非主属性对主键的部分依赖。 解决办法：把原表(学号, 姓名, 年龄, 课程名称, 成绩, 学分)分成三个表： 学生：Student(学号, 姓名, 年龄)； 课程：Course(课程名称, 学分)； 选课关系：SelectCourse(学号, 课程名称, 成绩)。 3.第三范式 1) 概念：2NF的基础上，属性不依赖于其它非主属性 , 消除传递依赖。第三范式又可描述为：表中不存在可以确定其他非关键字的非关键字段。 2) 例子：学生表：(学号, 姓名, 年龄, 所在学院, 学院地点, 学院电话)，主键必然是学号。 由于主键是单一属性，所以非主属性完全依赖于主键，所以必然满足第二范式。但是存在如下传递依赖： (学号) → (所在学院) → (学院地点, 学院电话)， 学院地点 和 学院电话传递依赖于学号，而学院地点和学院电话都是非关键字段，即表中出现了“某一非关键字段可以确定出其它非关键字段”的情况，于是违反了第三范式。 解决办法： 把原表分成两个表： 学生：(学号, 姓名, 年龄, 所在学院)； 学院：(学院, 地点, 电话)。 七、参考： http://www.cnblogs.com/gbyukg/archive/2011/11/09/2242271.html http://www.cnblogs.com/lina1006/archive/2011/04/29/2032894.html http://www.cnblogs.com/ybwang/archive/2010/06/04/1751279.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[【算法技巧】位运算装逼指南]]></title>
    <url>%2F2019%2F05%2F18%2F%E3%80%90%E7%AE%97%E6%B3%95%E6%8A%80%E5%B7%A7%E3%80%91%E4%BD%8D%E8%BF%90%E7%AE%97%E8%A3%85%E9%80%BC%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[位算法的效率有多快我就不说，不信你可以去用 10 亿个数据模拟一下，今天给大家讲一讲位运算的一些经典例子。不过，最重要的不是看懂了这些例子就好，而是要在以后多去运用位运算这些技巧，当然，采用位运算，也是可以装逼的，不信，你往下看。我会从最简单的讲起，一道比一道难度递增，不过居然是讲技巧，那么也不会太难，相信你分分钟看懂。 1、判断奇偶数判断一个数是基于还是偶数，相信很多人都做过，一般的做法的代码如下 1234if( n % 2) == 01 // n 是个奇数&#125;复制代码 如果把 n 以二进制的形式展示的话，其实我们只需要判断最后一个二进制位是 1 还是 0 就行了，如果是 1 的话，代表是奇数，如果是 0 则代表是偶数，所以采用位运算的方式的话，代码如下： 1234if(n &amp; 1 == 1)&#123; // n 是个奇数。&#125;复制代码 有人可能会说，我们写成 n % 2 的形式，编译器也会自动帮我们优化成位运算啊，这个确实，有些编译器确实会自动帮我们优化。但是，我们自己能够采用位运算的形式写出来，当然更好了。别人看到你的代码，我靠，牛逼啊。无形中还能装下逼，是不是。当然，时间效率也快很多，不信你去测试测试。 2、交换两个数交换两个数相信很多人天天写过，我也相信你每次都会使用一个额外来变量来辅助交换，例如，我们要交换 x 与 y 值，传统代码如下： 1234int tmp = x;x = y;y = tmp;复制代码 这样写有问题吗？没问题，通俗易懂，万一哪天有人要为难你，不允许你使用额外的辅助变量来完成交换呢？你还别说，有人面试确实被问过，这个时候，位运算大法就来了。代码如下： 1234x = x ^ y // （1）y = x ^ y // （2）x = x ^ y // （3）复制代码 我靠，牛逼！三个都是 x ^ y，就莫名交换成功了。在此我解释下吧，我们知道，两个相同的数异或之后结果会等于 0，即 n ^ n = 0。并且任何数与 0 异或等于它本身，即 n ^ 0 = n。所以，解释如下： 把（1）中的 x 带入 （2）中的 x，有 y = x^y = (x^y)^y = x^(y^y) = x^0 = x。 x 的值成功赋给了 y。 对于（3）,推导如下： x = x^y = (x^y)^x = (x^x)^y = 0^y = y。 这里解释一下，异或运算支持运算的交换律和结合律哦。 以后你要是别人看不懂你的代码，逼格装高点，就可以在代码里面采用这样的公式来交换两个变量的值了，被打了不要找我。 讲这个呢，是想告诉你位运算的强大，让你以后能够更多着去利用位运算去解决一些问题，一时之间学不会也没事，看多了就学会了，不信？继续往下看，下面的这几道题，也是非常常见的，可能你之前也都做过。 3、找出没有重复的数 给你一组整型数据，这些数据中，其中有一个数只出现了一次，其他的数都出现了两次，让你来找出一个数 。 这道题可能很多人会用一个哈希表来存储，每次存储的时候，记录 某个数出现的次数，最后再遍历哈希表，看看哪个数只出现了一次。这种方法的时间复杂度为 O(n)，空间复杂度也为 O(n)了。 然而我想告诉你的是，采用位运算来做，绝对高逼格！ 我们刚才说过，两个相同的数异或的结果是 0，一个数和 0 异或的结果是它本身，所以我们把这一组整型全部异或一下，例如这组数据是：1， 2， 3， 4， 5， 1， 2， 3， 4。其中 5 只出现了一次，其他都出现了两次，把他们全部异或一下，结果如下： 由于异或支持交换律和结合律，所以: 1^2^3^4^5^1^2^3^4 = （1^1)^(2^2)^(3^3)^(4^4)^5= 0^0^0^0^5 = 5。 也就是说，那些出现了两次的数异或之后会变成0，那个出现一次的数，和 0 异或之后就等于它本身。就问这个解法牛不牛逼？所以代码如下 12345678int find(int[] arr)&#123; int tmp = arr[0]; for(int i = 1;i &lt; arr.length; i++)&#123; tmp = tmp ^ arr[i]; &#125; return tmp;&#125;复制代码 时间复杂度为 O(n)，空间复杂度为 O(1)，而且看起来很牛逼。 4、3的n次方如果让你求解 3 的 n 次方，并且不能使用系统自带的 pow 函数，你会怎么做呢？这还不简单，连续让 n 个 3 相乘就行了，代码如下： 12345678int pow(int n)&#123; int tmp = 1; for(int i = 1; i &lt;= n; i++) &#123; tmp = tmp * 3; &#125; return tmp;&#125;复制代码 不过你要是这样做的话，我只能呵呵，时间复杂度为 O(n) 了，怕是小学生都会！如果让你用位运算来做，你会怎么做呢？ 我举个例子吧，例如 n = 13，则 n 的二进制表示为 1101, 那么 3 的 13 次方可以拆解为: 3^1101 = 3^0001 3^0100 3^1000。 我们可以通过 &amp; 1和 &gt;&gt;1 来逐位读取 1101，为1时将该位代表的乘数累乘到最终结果。直接看代码吧，反而容易理解： 12345678910111213int pow(int n)&#123; int sum = 1; int tmp = 3; while(n != 0)&#123; if(n &amp; 1 == 1)&#123; sum *= tmp; &#125; tmp *= tmp; n = n &gt;&gt; 1; &#125; return sum;&#125;复制代码 时间复杂度近为 O(logn)，而且看起来很牛逼。 这里说一下，位运算很多情况下都是很二进制扯上关系的，所以我们要判断是否是否位运算，很多情况下都会把他们拆分成二进制，然后观察特性，或者就是利用与，或，异或的特性来观察，总之，我觉得多看一些例子，加上自己多动手，就比较容易上手了。所以呢，继续往下看，注意，先别看答案，先看看自己会不会做。 5、找出不大于N的最大的2的幂指数传统的做法就是让 1 不断着乘以 2，代码如下： 12345678910int findN(int N)&#123; int sum = 1; while(true)&#123; if(sum * 2 &gt; N)&#123; return sum; &#125; sum = sum * 2; &#125;&#125;复制代码 这样做的话，时间复杂度是 O(logn)，那如果改成位运算，该怎么做呢？我刚才说了，如果要弄成位运算的方式，很多时候我们把某个数拆成二进制，然后看看有哪些发现。这里我举个例子吧。 例如 N = 19，那么转换成二进制就是 00010011（这里为了方便，我采用8位的二进制来表示）。那么我们要找的数就是，把二进制中最左边的 1 保留，后面的 1 全部变为 0。即我们的目标数是 00010000。那么如何获得这个数呢？相应解法如下： 1、找到最左边的 1，然后把它右边的所有 0 变成 1 2、把得到的数值加 1，可以得到 00100000即 00011111 + 1 = 00100000。 3、把 得到的 00100000 向右移动一位，即可得到 00010000，即 00100000 &gt;&gt; 1 = 00010000。 那么问题来了，第一步中把最左边 1 中后面的 0 转化为 1 该怎么弄呢？我先给出代码再解释吧。下面这段代码就可以把最左边 1 中后面的 0 全部转化为 1， 1234n |= n &gt;&gt; 1;n |= n &gt;&gt; 2;n |= n &gt;&gt; 4;复制代码 就是通过把 n 右移并且做或运算即可得到。我解释下吧，我们假设最左边的 1 处于二进制位中的第 k 位(从左往右数),那么把 n 右移一位之后，那么得到的结果中第 k+1 位也必定为 1,然后把 n 与右移后的结果做或运算，那么得到的结果中第 k 和 第 k + 1 位必定是 1;同样的道理，再次把 n 右移两位，那么得到的结果中第 k+2和第 k+3 位必定是 1,然后再次做或运算，那么就能得到第 k, k+1, k+2, k+3 都是 1，如此往复下去…. 最终的代码如下 12345678int findN(int n)&#123; n |= n &gt;&gt; 1; n |= n &gt;&gt; 2; n |= n &gt;&gt; 4; n |= n &gt;&gt; 8 // 整型一般是 32 位，上面我是假设 8 位。 return (n + 1) &gt;&gt; 1;&#125;复制代码 这种做法的时间复杂度近似 O(1)，重点是，高逼格。 总结上面讲了 5 道题，本来想写十道的，发现五道就已经写了好久了，，，，十道的话，怕你们也没耐心写完，而且一道比一道难的那种，，，，。 不过呢，我给出的这些例子中，并不是让你们学会了这些题就 Ok，而且让你们有一个意识：很多时候，位运算是个不错的选择，至少时间效率会快很多，而且高逼格，装逼必备。所以呢，以后可以多尝试去使用位运算哦，以后我会再给大家找些题来讲讲，遇到高逼格的，感觉很不错的，就会拿来供大家学习了。 如果你觉得该文章不错，不妨 1、点赞，让更多的人也能看到这篇内容（收藏不点赞，都是耍流氓 -_-） 2、关注我，让我们成为长期关系 3、关注公众号「苦逼的码农」，里面已有100多篇原创文章，我也分享了很多视频、书籍的资源，以及开发工具，欢迎各位的关注，第一时间阅读我的文章。 作者：帅地 链接：https://juejin.im/post/5cdce78d5188250d8b2df3a7 来源：掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
  </entry>
  <entry>
    <title><![CDATA[git上统计每个人增删行数]]></title>
    <url>%2F2019%2F04%2F05%2Fgit%E4%B8%8A%E7%BB%9F%E8%AE%A1%E6%AF%8F%E4%B8%AA%E4%BA%BA%E5%A2%9E%E5%88%A0%E8%A1%8C%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1git log --format=&apos;%aN&apos; | sort -u | while read name; do echo -en &quot;$name\t&quot;; git log --author=&quot;$name&quot; --pretty=tformat: --numstat | awk &apos;&#123; add += $1; subs += $2; loc += $1 - $2 &#125; END &#123; printf &quot;added lines: %s, removed lines: %s, total lines: %s\n&quot;, add, subs, loc &#125;&apos; -; done 结果示例 123456789101112Max-laptop added lines: 1192, removed lines: 748, total lines: 444chengshuai added lines: 120745, removed lines: 71738, total lines: 49007cisen added lines: 3248, removed lines: 1719, total lines: 1529max-h added lines: 1002, removed lines: 473, total lines: 529max-l added lines: 2440, removed lines: 617, total lines: 1823mw added lines: 148721, removed lines: 6709, total lines: 142012spider added lines: 2799, removed lines: 1053, total lines: 1746thy added lines: 34616, removed lines: 13368, total lines: 21248wmao added lines: 12, removed lines: 8, total lines: 4xrl added lines: 10292, removed lines: 6024, total lines: 4268yunfei.huang added lines: 427, removed lines: 10, total lines: 417³ö added lines: 5, removed lines: 3, total lines: 2]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18个高效使用Google搜索的技巧]]></title>
    <url>%2F2019%2F03%2F23%2F18%E4%B8%AA%E9%AB%98%E6%95%88%E4%BD%BF%E7%94%A8Google%E6%90%9C%E7%B4%A2%E7%9A%84%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[本文来源：https://mp.weixin.qq.com/s/0WqC1dQBAvehYK3hlVOueQ 前言如果把浩瀚的互联网资源比喻成是一个图书馆，那么google搜索引擎毫无疑问是这个图书馆的导航中心，通过google可以轻而易举得检索到绝大多数你需要的资料，然而大多数人可能并没有充分发挥谷歌搜索的潜力。 如何才能更加高效，快速的利用好谷歌这个搜索引擎呢？这里有18个技巧用来提升你的检索效率。 1.使用Tab面板使用谷歌使用结果完成后，在搜索栏的下面会出现多个Tab面板，默认分别是全部，新闻，图片，视频，地图，更多等，这里面如果我们已经知道我们要搜索的分类是某个类目的时候，可以直接点击Tab面板将搜索结果限定在大的类目中，这样更方便我们定位检索的资源。 2.使用双引号比如我想搜索宠物狗穿的毛衣，我在输入框里面输入了三个关键词pet dog sweaters，默认情况下谷歌返回的内容是包含这个三个词可任意调换顺序的命中结果，如果我们认为这三个词是一个整体，搜索的结果里面必须保持和搜索关键字一样的出现顺序，这个时候我们双引号来告诉谷歌，我们想要更精确的查询： “pet dog sweaters”。 3.使用连字符排除指定搜索内容有时候我们搜索的关键词本身可能有多种含义，这个时候通过连字符可以排除我们不需要出现的结果。比如我们搜 apple 这个词，在谷歌里面可能是水果的意思，也能是iPhone手机相关的含义，如果我们搜的是iPhone相关的内容，这个时候可以设置搜索关键词为： apple -fruit，这样以来就可以排除掉与水果有关的apple信息。 4.使用site关键词site关键字是google索引的一个内置字段，有时我们已经明确我们要搜索的内容就在某个网站，这个时候这个关键字就会很有用。比如说我在谷歌搜索hadoop，但是我只想看官网文档的内容，不想看其他乱七八糟的网站的东西，那么我们就可以这样搜索： hadoop site:apache.org ，这样就能够只检索我们想看的目标网站的内容。 5.使用link关键词使用link关键词限定是另外一个使用比较少的功能，这个功能可以让我们找到含有指定关键词的网页是否链接了我们指定的网站，例如，我们搜索的关键词如下： csdn link:stackoverflow ， 这个关键词的含义代表含有csdn关键词的网页里面，那些有引用可以链接到stackoverflow这个网站。 6.使用通配符检索通配符检索也就是所谓的模糊检索，比如我们可以这样在google中搜索世界最大的国家， “ is the largest country in the world”。或者我们想检索”sp“开头的单词，通过通配符我们使用占位的方式来检索特定内容的结果集。 7.使用related关键词related关键字可以搜索内容相关或者类似的网站，比如我们天天用淘宝购物，现在想知道做电商的其他的网站有哪些，我们的搜索关键词可以这么输入：related:taobao.com 8.使用google去做数学运算这个特性可以在让我们在谷歌搜索框直接输入数学计算表达式，然后谷歌会直接返回一个第一条是个计算器的页面，并且计算结果也显示在计算器里面。例如直接搜索：PI , 1+2+3 , 5*100+3 , 10/3(结构是浮点数)等表达式。 9.一次搜索多个关键词注意，这里有个强调关键词的概念，大部分情况下，如果我们在谷歌搜索框输入的关键词越多，那么命中的结果集就会越来越小，有可能直接导致搜索不到数据，所以在搜索中尽量找到一个或多个关键词，默认情况谷歌使用的AND逻辑，多个关键词都必须出现才能命中结果，但一些情况下我们想要只出现其中任意一个关键词命中就可以，我们就可以使用OR关键词，例如：dog OR cat wiki，一下检索两个关键词的维基百科，注意OR必须大写。 10.搜索使用数字范围搜索范围限制功能使用也非常简单，使用两个点号就可以了，举个例子，比如我要搜索java在2013-2014年有关的文章，输入的语法如下：java 2013..2014 就可以了。如果我们不写任何关键词，直接输入比如： 33..35，那么google就会广泛的搜索在33和35之间任何有关的东西。 11.关键字尽量简单谷歌检索其实是依据关键词来检索的，这就要求描述尽量精简和准确而并不是描述详细和冗长，比如你想搜索附近的肯德基餐厅有哪些？ 如果直接输入： 我想知道附近的肯德基餐厅有哪些？ 其实是没必要的。 可以直接替换为： 肯德基 附近 就行了，这样以来既精简又准确。 12.逐步增加搜索关键词有时候，简单的关键词可能描述的确实不够完善，这个时候我们应该逐步的增加关键字来获取更好的搜索效果。比如你要进行一次演讲而不知道如何准备，那么你可以在谷歌里面搜索的阶段如下： a. 演讲 b. 准备 演讲 c. 如何 准备 演讲 注意这里面没有从第一步直接过度到第三步的原因是如果缺少了第二步，可能会漏掉某些我们想要的结果。因为很多网站描述同一件事使用不同的方式，我们通过这种方式可以尽可能全的，准的找到我们想要的信息。 13.描述替换这是一个非常重要的原则，有时候我们说的话表达同一个意思，但可能使用不同的描述。 不同的描述，虽然意思或者目的可能相同，但谷歌搜索的结果却是不一样。举个例子，某一天你开的车的轮胎坏了，如果你直接在谷歌搜索： 我的车的轮胎坏了，可能解决不了你的问题，而你真实的意思表达的是想修理轮胎，所以这个时候你应该这样描述： 修理 轮胎。 另外一个例子，如果你的头受伤了，如果你直接搜： 我的头受伤了这可能不是你真实的目的，其实你可能想要找如何减轻头痛或者缓解的方式，这个时候你应该检索： 头痛 缓解。通过这样的转换，可以帮助我们找到更准确的结果。 14.只使用最重要的关键词这个原则其实很前面说的几条有点类似，默认情况下输入谷歌搜索的关键词越多，返回的结果就会越少，如果找不到最重要的关键词，那么反而会浪费时间在切换尝试上。举个例子如果你搜索： 我在哪里可以找到一个海底捞餐厅。这样反而可能搜不到结果。相反替换成： 海底捞 餐厅 附近 可能效果会好的多。总之使用谷歌搜索的时候，尽量保持关键词简单和重要。 15.快捷搜索命令在谷歌上有一些常用的快速检索出结果的关键词，比如你搜下面的几个关键词： 中国邮政编码 中国高校 中国首都 中国历史古都 圆周率 miles to km 人民币汇率 等等都可以直接出对应的结果 16.拼写自动纠正这个功能我们在很多软件里面都有，比如word，ppt里面，当然谷歌里面也一样，如果某个单词的某个字母写错了，不管是缺少字母，还是多字母或者顺序不对，都基本不影响我们的使用。 17.使用描述性词语这个技巧和前面的几条其实是有关系的，简单的说，如果搜索某个关键词没有命中的时候，我们可以使用其同义词，或者意思相近的描述来增加搜索范围，这样就会有更多可能找到我们想要的。 举个例子，比如搜： 如何给windows系统装驱动 ？ 可以替换描述为： 解决windows驱动问题。 等等类似的。 18.使用filetype关键词filetype也是一个非常使用的功能，在寻找或者下载某一类文件时候能够快速检索我们需要的文件后缀的资源。比如搜索： 深入理解计算机系统 filetype:pdf 就可以找到某些电子书的pdf版本，同样的我们可以用来搜索ppt，word，mp3等各种格式的资源。 总结谷歌搜索是世界强大的搜索引擎没有之一，使用上面的这些搜索技巧可以让我们能够快速找到分布在互联网里面任何你需要的东西。善用谷歌搜索引擎，你可以无国界的遨游在互联网上，你可以学习到任何免费的资源，无论是哪个领域，只要你愿意，Google都可以成为你的人生导师。]]></content>
      <categories>
        <category>Google</category>
      </categories>
      <tags>
        <tag>Google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM调优实践]]></title>
    <url>%2F2019%2F02%2F25%2FJVM%E8%B0%83%E4%BC%98%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[原文地址https://blog.wangqi.love/articles/Java/JVM%E8%B0%83%E4%BC%98%E5%AE%9E%E8%B7%B5.html JVM调优是一个非常依赖实践的工作，本文就是在某些场景下对JVM调优方法的整理。 CPU占用高CPU占用高是我们在线上会遇到的场景。出现这种情况，我们首先需要定位消耗CPU资源的代码。 我们以下面的代码为例，介绍怎么定位问题： 1234567891011121314public class InfiniteLoop &#123; public static void main(String[] args) &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; long i = 0; while (true) &#123; i++; &#125; &#125; &#125;); thread.start(); &#125;&#125; 这段代码就是一个简单的死循环。 执行程序后，执行top命令： 通过top命令，我们发现PID为10995的Java进程占用CPU高达99.9%。 下一步如何定位到具体线程？ 执行以下命令显示线程列表： 1ps -mp pid -o THREAD,tid,time 找到了占用CPU最高的线程11005，占用CPU时间为02:23 然后通过以下命令将找到的线程ID转换为16进制格式： printf &quot;%x\n&quot; tid 最后通过以下命令打印线程的堆栈信息： jstack pid | grep tid -A 30 通过线程堆栈信息，我们可以定位到是InfiniteLoop中的run方法。 Full GC频繁在线上环境，频繁的执行Full GC会导致程序经常发生停顿，从而导致接口的响应时间变长，这时就需要对JVM的状态进行监控，确定Full GC发生的原因。 首先我们在启动程序的时候可以加上GC日志相关的参数，主要有以下几个： -XX:+PrintGC：输出GC日志 -XX:+PrintGCDetails：输出GC的详细日志 -XX:+PrintGCTimeStamps：输出GC的时间戳（以基准时间的形式） -XX:+PrintGCDateStamps：输出GC的时间戳（以日期的形式，如2018-08-29T19:22:48.741-0800） -XX:+PrintHeapAtGC：在进行GC的前后打印出堆的信息 -Xloggc:gc.log：日志文件的输出路径 现在通过程序来模拟Full GC频繁发生的情形： 123456789101112131415161718192021222324252627282930class Object1 &#123; int size = (10 * 1024 * 1024) / 4; int[] nums = new int[size]; public Object1() &#123; for (int i = 0; i &lt; size; i++) &#123; nums[i] = i; &#125; &#125;&#125;class Object2 &#123; int size = (1 * 1024 * 1024) / 4; int[] nums = new int[size]; public Object2() &#123; for (int i = 0; i &lt; size; i++) &#123; nums[i] = i; &#125; &#125;&#125;public class HeapOOM &#123; public static void main(String[] args) throws InterruptedException &#123; Object1 object1 = new Object1(); while (true) &#123; Object2 object2 = new Object2(); Thread.sleep(100); &#125; &#125;&#125; 我们知道Java堆被划分为新生代和老年代。默认比例为1:2（可以通过-XX:NewRatio设定）。 新生代又分为Eden、From Survivor、To Survivor。这样划分的目的是为了使JVM能够更好地管理堆内存中的对象，包括内存的分派以及回收。默认比例为Eden:From:To = 8:1:1（可以通过参数-XX:SurvivorRatio来设定，-XX:SurvivorRatio=8表示Eden与一个Survivor空间比例为8:1） 一般新建的对象会分配到Eden区。这些对象经过第一次Minor GC后，如果仍然存活，将会被移到Survivor区。在Survivor每熬过一轮Minor GC年龄就增加1 当年龄达到一定程度是（年龄阈值，默认为15，可以通过-XX:MaxTenuringThreshold来设置），就会被移动到老年代。 from和to之间会经常互换角色，from变成to，to变成from。每次GC时，把Eden存活的对象和From Survivor中存活且没超过年龄阈值的对象复制到To Survivor中，From Survivor清空，变成To Survivor。 GC分为两种： Minor GC是发生在新生代中的垃圾收集动作，所采用的是复制算法，所采用的是复制算法，因为Minor GC比较频繁，因此一般回收速度较快。 Full GC是发生在老年代的垃圾收集动作，所采用的是标记-清除算法，速度比Minor GC慢10倍以上 大对象直接进入老年代。比如很长的字符串以及数组。通过设置-XX:PretenureSizeThreshold，令大于这个值的对象直接在老年代分配。这样做是为了避免在Eden和两个Survivor之间发生大量的内存复制。 什么时候发生Minor GC？什么时候发生Full GC？ 当新生代Eden区没有足够的空间进行分配时，虚拟机将发起一次Minor GC 老年代空间不足时发起一次Full GC 我们以下面的命令来执行程序： 1java -Xms30m -Xmx30m -Xmn2m -XX:SurvivorRatio=8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=dump/dump.hprof dump.HeapOOM 以下是对上面JVM参数的说明： -Xms：堆初始大小 -Xmx：堆最大值 -Xmn：新生代大小（老年代大小=堆大小-新生代大小） -XX:+HeapDumpOnOutOfMemoryError：发生内存溢出时生成heapdump文件 -XX:HeapDumpPath：指定heapdump文件 我们之所以将新生代的大小设为2m，是因为这样新建的Object2对象就无法在新生代上分配，从而直接进入老年代，当老年代空间占满后就会触发Full GC。 程序执行之后，我们从GC日志中看到频繁发生Full GC，于是我们开始定位Full GC发生的原因。 以下面的两段GC日志，来看一下GC日志的含义： 1231.840: [GC (Allocation Failure) [PSYoungGen: 573K-&gt;432K(1536K)] 28221K-&gt;28088K(30208K), 0.0014619 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]1.842: [GC (Allocation Failure) [PSYoungGen: 432K-&gt;400K(1536K)] 28088K-&gt;28056K(30208K), 0.0005985 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]1.843: [Full GC (Allocation Failure) [PSYoungGen: 400K-&gt;0K(1536K)] [ParOldGen: 27656K-&gt;10558K(28672K)] 28056K-&gt;10558K(30208K), [Metaspace: 2657K-&gt;2657K(1056768K)], 0.0038527 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 最前面的数字1.840:、1.842:和1.843:代表了GC发生的时间，这个数字的含义是从Java虚拟机启动以来经过的秒数 GC日志开头的[GC和[Full GC说明了这次垃圾收集的停顿类型，而不是用来区分新生代GC还是老年代GC的。如果有”Full GC”，说明这次GC是发生了Stop-The-World的。 接下来的[PSYoungGen、[ParOldGen、[Metaspace表示GC发生的区域。这里显示的区域名称与使用的GC收集器是密切相关的，例如上面的PSYoungGen表示采用Parallel Scavenge收集器，ParOldGen表示采用Parallel Old收集器。如果使用Serial收集器显示[DefNew，如果使用ParNew收集器显示[ParNew。 后面方括号内部的400K-&gt;0K(1536K)含义是”GC前该内存区域已经使用容量-&gt;GC后该内存区域已使用容量（该内存区域总容量）”。而在方括号之外的28056K-&gt;10558K(30208K)表示”GC前Java堆已使用容量-&gt;GC后Java堆已使用容量（Java堆总容量）”。 再往后的0.0038527 secs表示该内存区域GC所占用的时间，单位是秒。有的收集器会给出更具体的时间数据，如[Times: user=0.01 sys=0.00, real=0.01 secs]，这里面的user、sys、real与Linux的time命令所输出的时间含义一致，分别代表用户态消耗的CPU时间、内核态消耗的CPU时间和操作从开始到结束所经过的墙钟时间（Wall Clock Time）。CPU时间与墙钟时间的区别是，墙钟时间包括各种非运算的等待耗时，例如等待磁盘IO、等待线程阻塞，而CPU时间不包括这些耗时，但当系统有多CPU或者多核的话，多线程操作会叠加这些CPU时间，所以读者看到user或sys时间超过real时间是完全正常的。 下面开始定位问题。 首先执行jps命令定位程序的进程号。 然后执行jstat命令监视Java堆的状况. 1jstat -gc 11172 1000 其中11172是进程号，1000表示每隔1000毫秒打印一次日志 S0C和S1C（Survivor0、Survivor1）：两个Survivor区的大小 S0U和S1U（Survivor0、Survivor1）：两个Survivor区的使用大小 EC（Eden）：Eden区的大小 EU（Eden）：Eden区的使用大小 OC（Old）：老年代大小 OU（Old）：老年代使用大小 MC：元数据区大小 MU：元数据区使用大小 CCSC：压缩类空间大小 CCSU：压缩类空间使用大小 YGC（Young GC）：年轻代垃圾回收次数 YGCT（Young GC Time）：年轻代垃圾回收总耗时（秒） FGC（Full GC）：老年代垃圾回收次数 FGCT（Full GC Time）：老年代垃圾回收总耗时（秒） GCT（GC Time）：所有GC总耗时（秒） 可以看到，程序在不断发生Full GC。 执行jmap把当前的堆dump下来： 1jmap -dump:live,format=b,file=dump.hprof 11172 其中11172是进程ID 然后将dump.hprof文件使用VisualVM来打开 我们可以看到，int[]对象占用的空间最大，其中int[]#1的GC Root指向了dump.Object1对象，无法被回收，这样一个大对象占用了老年代空间，因此导致了频繁发生Full GC。 解决这个问题有两种思路： 一般情况下原因都是代码问题，导致某个大对象没有及时释放，在多次GC之后进入老年代空间。我们要做的首先是定位到占用大量空间的对象，优化其中的代码，及时释放大对象，腾空老年代空间 增加新生代的大小，让对象都在新生代分配与释放，从而不进入老年代空间。这样就会大大减少Full GC的发生 https://zhangguodong.me/2017/11/25/%E7%90%86%E8%A7%A3GC%E6%97%A5%E5%BF%97/https://segmentfault.com/a/1190000002677695https://www.jianshu.com/p/45415ebe0721https://blog.csdn.net/u010862794/article/details/78020231http://www.blogjava.net/hankchen/archive/2012/05/09/377735.htmlhttp://swcdxd.iteye.com/blog/1859858http://huachao1001.github.io/article.html?C2xJwZZnhttps://www.zybuluo.com/zero1036/note/872396]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用google map实现周边搜索功能]]></title>
    <url>%2F2019%2F02%2F15%2F%E7%94%A8google-map%E5%AE%9E%E7%8E%B0%E5%91%A8%E8%BE%B9%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[项目要实现根据经纬度获取附近的建筑，由于项目在海外运营，谷歌地图首当其冲。 首先说明的是，该功能需要在服务端实现，也就是安卓的SDK不适用。 api文档地址： https://developers.google.com/places/web-service/search#PlaceSearchResults 获取秘钥key的方法： https://developers.google.com/places/web-service/get-api-key api文档地址打不开怎么办，我将文档中的东西复制下来了，如下： 附近的搜索请求默认情况下，当用户选择某个地点时，“附近搜索”会返回所选地点的所有可用数据字段，您将收到相应的结算费用。没有办法将附近搜索请求限制为仅返回特定字段。要避免请求（并支付）您不需要的数据，请改用 查找位置请求。 通过“附近搜索”，您可以搜索指定区域内的位置。您可以通过提供关键字或指定要搜索的地点类型来优化搜索请求。 附近搜索请求是以下格式的HTTP URL： 1https://maps.googleapis.com/maps/api/place/nearbysearch/output?parameters 其中output可能是以下任一值： json （推荐）表示JavaScript Object Notation（JSON）中的输出 xml 表示输出为XML 启动“附近搜索”请求需要某些参数。作为URL中的标准，所有参数都使用ampersand（&amp;）字符分隔。 必需参数 key- 您的应用程序的 API密钥。此密钥标识您的应用程序。有关 更多信息，请参阅 获取密钥。 location - 检索地点信息的纬度/经度。必须将其指定为 纬度，经度。 radius - 定义返回位置结果的距离（以米为单位）。允许的最大半径为50 000米。请注意，radius如果指定rankby=distance（在下面的可选参数下描述），则不得包括 。 如果rankby=distance（在所描述的可选参数下面）被指定，那么一个或多个 keyword，name或type是必需的。 可选参数 keyword - 与Google为此地点编入索引的所有内容匹配的字词，包括但不限于姓名，类型和地址，以及客户评论和其他第三方内容。 language - 语言代码，如果可能，指示应返回结果的语言。请参阅支持的语言 及其代码列表。请注意，我们经常更新支持的语言，因此此列表可能并非详尽无遗。 minprice和maxprice （可选） - 仅将结果限制在指定范围内的那些位置。有效值的范围介于0（最实惠）到4（最昂贵）之间。具体值表示的确切数量因地区而异。 name - 与Google为此地点编入索引的所有内容匹配的字词。相当于keyword。该 name字段不再局限于地名。此字段中的值与keyword字段中的值组合，并作为同一搜索字符串的一部分传递。我们建议仅对keyword所有搜索词使用 参数。 opennow - 仅返回在发送查询时为业务开放的那些位置。如果在查询中包含此参数，则不会返回未在Google地方信息数据库中指定营业时间的地点。 rankby - 指定列出结果的顺序。请注意，rankby如果指定了radius （在上面的必需参数中描述），则不得包括。可能的值是： prominence（默认）。此选项根据结果的重要性对结果进行排序。排名将有利于指定区域内的显着位置。地方在Google索引中的排名，全球受欢迎程度以及其他因素都会影响到突出程度。 distance。此选项按照与指定距离的距离按升序对搜索结果进行偏差location。当 distance被指定时，一个或多个keyword， name或type是必需的。 type - 将结果限制为与指定类型匹配的位置。只能指定一种类型（如果提供了多种类型，则忽略第一个条目后面的所有类型）。请参阅 支持的类型列表。 pagetoken - 返回先前运行的搜索的后20个结果。设置pagetoken参数将使用先前使用的相同参数执行搜索 - pagetoken将忽略除以外的所有参数。 Google Maps API Premium Plan客户注意事项：您必须在请求中包含API密钥。你应该不包括client或 signature参数您的要求。 附近的搜索示例以下示例是澳大利亚悉尼一个1500米半径范围内“餐馆”类型的地点的搜索请求，其中包含“游轮”一词： 1https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=-33.8670522,151.1957362&amp;radius=1500&amp;type=restaurant&amp;keyword=cruise&amp;key=YOUR_API_KEY 注意：在此示例中，您需要key 使用自己的API密钥替换，以使请求在您的应用程序中起作用。 JSON响应最多包含四个根元素： &quot;status&quot;包含请求的元数据。请参阅 下面的状态代码 &quot;results&quot;包含一系列地点，包含每个地方的信息。 有关这些结果的信息，请参见搜索结果 Places API establishment 每个查询最多返回20个结果。另外，political可以返回结果，其用于识别请求的区域。 html_attributions 可能包含一组关于此列表的归属，必须向用户显示（某些列表可能没有归属）。 next_page_token包含一个令牌，可用于返回最多20个附加结果。next_page_token如果没有要显示的其他结果，则不会返回A. 可以返回的最大结果数为60.在next_page_token发布a 和有效之间会有短暂的延迟。 状态代码该&quot;status&quot;搜索响应对象中字段包含请求的状态，并且可能会包含调试信息，以帮助您跟踪请求失败的原因。该&quot;status&quot;字段可能包含以下值： OK表示没有发生错误; 成功检测到该地点，并返回至少一个结果。 ZERO_RESULTS表示搜索成功但未返回任何结果。如果搜索是latlng在远程位置传递的，则可能会发生这种情况 。 OVER_QUERY_LIMIT 表示您已超过配额。 REQUEST_DENIED表示您的请求被拒绝，通常是因为缺少无效key参数。 INVALID_REQUEST通常表示缺少必需的查询参数（location或radius）。 UNKNOWN_ERROR表示服务器端错误; 再试一次可能会成功。 错误消息当Google商家信息服务返回其他状态代码时 OK，error_message搜索响应对象中可能还有一个附加字段。该字段包含有关给定状态代码背后原因的更多详细信息。 访问其他结果默认情况下，每个附近搜索或文本搜索establishment每个查询最多返回20个结果; 但是，每个搜索可以返回多达60个结果，分为三个页面。如果您的搜索返回超过20，那么搜索响应将包含一个额外的值 - next_page_token。将值的值传递给新搜索next_page_token的pagetoken参数以查看下一组结果。如果 next_page_token为null，或者未返回，则没有进一步的结果。在next_page_token发布a 和何时生效之间会有短暂的延迟 。在可用之前请求下一页将返回INVALID_REQUEST响应。使用相同的方法重试请求 next_page_token将返回下一页结果。 例如，在下面的查询中，我们搜索澳大利亚悉尼达令港附近的餐馆，并按距离对结果进行排名。您可以看到响应包含next_page_token属性。 1https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=-33.8670522,151.1957362&amp;rankby=distance&amp;type=food&amp;key=YOUR_API_KEY 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&#123; &quot;html_attributions&quot; : [], &quot;next_page_token&quot; : &quot;CpQCAgEAAFxg8o-eU7_uKn7Yqjana-HQIx1hr5BrT4zBaEko29ANsXtp9mrqN0yrKWhf-y2PUpHRLQb1GT-mtxNcXou8TwkXhi1Jbk-ReY7oulyuvKSQrw1lgJElggGlo0d6indiH1U-tDwquw4tU_UXoQ_sj8OBo8XBUuWjuuFShqmLMP-0W59Vr6CaXdLrF8M3wFR4dUUhSf5UC4QCLaOMVP92lyh0OdtF_m_9Dt7lz-Wniod9zDrHeDsz_by570K3jL1VuDKTl_U1cJ0mzz_zDHGfOUf7VU1kVIs1WnM9SGvnm8YZURLTtMLMWx8-doGUE56Af_VfKjGDYW361OOIj9GmkyCFtaoCmTMIr5kgyeUSnB-IEhDlzujVrV6O9Mt7N4DagR6RGhT3g1viYLS4kO5YindU6dm3GIof1Q&quot;, &quot;results&quot; : [ &#123; &quot;geometry&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : -33.867217, &quot;lng&quot; : 151.195939 &#125; &#125;, &quot;icon&quot; : &quot;http://maps.gstatic.com/mapfiles/place_api/icons/cafe-71.png&quot;, &quot;id&quot; : &quot;7eaf747a3f6dc078868cd65efc8d3bc62fff77d7&quot;, &quot;name&quot; : &quot;Biaggio Cafe - Pyrmont&quot;, &quot;opening_hours&quot; : &#123; &quot;open_now&quot; : true &#125;, &quot;photos&quot; : [ &#123; &quot;height&quot; : 600, &quot;html_attributions&quot; : [], &quot;photo_reference&quot; : &quot;CnRnAAAAmWmj0BqA0Jorm1_vjAvx1n6c7ZNBxyY-U9x99-oNyOxvMjDlo2npJzyIq7c3EK1YyoNXdMFDcRPzwLJtBzXAwCUFDGo_RtLRGBPJTA2CoerPdC5yvT2SjfDwH4bFf5MrznB0_YWa4Y2Qo7ABtAxgeBIQv46sGBwVNJQDI36Wd3PFYBoUTlVXa0wn-zRITjGp0zLEBh8oIBE&quot;, &quot;width&quot; : 900 &#125; ], &quot;place_id&quot; : &quot;ChIJIfBAsjeuEmsRdgu9Pl1Ps48&quot;, &quot;scope&quot; : &quot;GOOGLE&quot;, &quot;price_level&quot; : 1, &quot;rating&quot; : 3.4, &quot;reference&quot; : &quot;CoQBeAAAAGu0wNJjuZ40DMrRe3mpn7fhlfIK1mf_ce5hgkhfM79u-lqy0G2mnmcueTq2JGWu9wsgS1ctZDHTY_pcqFFJyQNV2P-kdhoRIeYRHeDfbWtIwr3RgFf2zzFBXHgNjSq-PSzX_OU6OT2_3dzdhhpV-bPezomtrarW4DsGl9uh773yEhDJT6R3V8Fyvl_xeE761DTCGhT1jJ3floFI5_c-bHgGLVwH1g-cbQ&quot;, &quot;types&quot; : [ &quot;cafe&quot;, &quot;bar&quot;, &quot;restaurant&quot;, &quot;food&quot;, &quot;establishment&quot; ], &quot;vicinity&quot; : &quot;48 Pirrama Rd, Pyrmont&quot; &#125;, &#123; &quot;geometry&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : -33.866786, &quot;lng&quot; : 151.195633 &#125; &#125;, &quot;icon&quot; : &quot;http://maps.gstatic.com/mapfiles/place_api/icons/generic_business-71.png&quot;, &quot;id&quot; : &quot;3ef986cd56bb3408bc1cf394f3dad9657c1d30f6&quot;, &quot;name&quot; : &quot;Doltone House&quot;, &quot;photos&quot; : [ &#123; &quot;height&quot; : 1260, &quot;html_attributions&quot; : [ &quot;From a Google User&quot; ], &quot;photo_reference&quot; : &quot;CnRwAAAAeM-aLqAm573T44qnNe8bGMkr_BOh1MOVQaA9CCggqtTwuGD1rjsviMyueX_G4-mabgH41Vpr8L27sh-VfZZ8TNCI4FyBiGk0P4fPxjb5Z1LrBZScYzM1glRxR-YjeHd2PWVEqB9cKZB349QqQveJLRIQYKq2PNlOM0toJocR5b_oYRoUYIipdBjMfdUyJN4MZUmhCsTMQwg&quot;, &quot;width&quot; : 1890 &#125; ], &quot;place_id&quot; : &quot;ChIJ5xQ7szeuEmsRs6Kj7YFZE9k&quot;, &quot;scope&quot; : &quot;GOOGLE&quot;, &quot;reference&quot; : &quot;CnRvAAAA22k1PAGyDxAgHZk6ErHh_h_mLUK_8XNFLvixPJHXRbCzg-gw1ZxdqUwA_8EseDuEZKolBs82orIQH4m6-afDZV9VcpggokHD9x7HdMi9TnJDmGb9Bdh8f-Od4DK0fASNBL7Me3CsAWkUMWhlNQNYExIQ05W7VbxDTQe2Kh9TiL840hoUZfiO0q2HgDHSUyRdvTQx5Rs2SBU&quot;, &quot;types&quot; : [ &quot;food&quot;, &quot;establishment&quot; ], &quot;vicinity&quot; : &quot;48 Pirrama Rd, Pyrmont&quot; &#125;, &#123; &quot;aspects&quot; : [ &#123; &quot;rating&quot; : 23, &quot;type&quot; : &quot;overall&quot; &#125; ], ... ], &quot;status&quot; : &quot;OK&quot;&#125; 要查看下一组结果，您可以提交新查询，并将结果传递next_page_token给pagetoken 参数。例如： 1https://maps.googleapis.com/maps/api/place/nearbysearch/json?pagetoken=CpQCAgEAAFxg8o-eU7_uKn7Yqjana-HQIx1hr5BrT4zBaEko29ANsXtp9mrqN0yrKWhf-y2PUpHRLQb1GT-mtxNcXou8TwkXhi1Jbk-ReY7oulyuvKSQrw1lgJElggGlo0d6indiH1U-tDwquw4tU_UXoQ_sj8OBo8XBUuWjuuFShqmLMP-0W59Vr6CaXdLrF8M3wFR4dUUhSf5UC4QCLaOMVP92lyh0OdtF_m_9Dt7lz-Wniod9zDrHeDsz_by570K3jL1VuDKTl_U1cJ0mzz_zDHGfOUf7VU1kVIs1WnM9SGvnm8YZURLTtMLMWx8-doGUE56Af_VfKjGDYW361OOIj9GmkyCFtaoCmTMIr5kgyeUSnB-IEhDlzujVrV6O9Mt7N4DagR6RGhT3g1viYLS4kO5YindU6dm3GIof1Q&amp;key=YOUR_API_KEY 设置pagetoken将导致忽略任何其他参数。查询将执行与之前相同的搜索，但将返回一组新结果。您可以在原始查询后最多两次请求新页面。必须依次显示每页结果。搜索结果的两页或多页不应作为单个查询的结果显示。请注意，每次搜索都会计入针对您的使用限制的单个请求。 但是，比较坑的一点是同一个key一天调用的次数最多150000次，如果用户量较大时，要专门交费调整限制次数 。 代码示例pom依赖 12345&lt;dependency&gt; &lt;groupId&gt;de.taimos&lt;/groupId&gt; &lt;artifactId&gt;httputils&lt;/artifactId&gt; &lt;version&gt;1.11&lt;/version&gt;&lt;/dependency&gt; 测试方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import de.taimos.httputils.WS;import org.apache.http.HttpResponse;/** * @author: xbq * @date: 2019/2/13 10:47 * @description: */public class GoogleMap &#123; public static final String NEARBY_SEARCH_URL = "https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=%s&amp;radius=%s&amp;key=%s"; public static final String NEXT_PAGE_URL = "https://maps.googleapis.com/maps/api/place/nearbysearch/json?pagetoken=%s&amp;key=%s"; public static final String PHOTO_URL = "https://maps.googleapis.com/maps/api/place/photo?maxwidth=480&amp;photoreference=%s&amp;key=%s"; // 秘钥 public static final String KEY = "你申请的KEY"; public static void main(String[] args) &#123; HttpResponse response = null; String resStr = null; // 调用接口进行请求地址数据 String location = "-33.8670522,151.1957362"; String radius = "1500"; response = WS.url(String.format(NEARBY_SEARCH_URL, location, radius, KEY)).get(); resStr = WS.getResponseAsString(response); System.out.println("resStr==" + resStr); /** * 请求返回的数据中有nextPageToken字段不为空时，就有多页数据，请求下一页数据时需要将该字段携带去请求 * 查询下一页的内容 */ String nextPageToken = "CoQE_QEAAK74QaEIfb5nG5Zfjuk0Dk2zRojKXhXr1-XhgzEY8xJrwPDvWTs82dGJuQ9JNIERxYWjadPeP-XwPqiKMWbuUpRw0vYfp7xwoj7YOhoYyF9yXwREjhKiRT_F-gaBJnvG_6FqqPbql6f4vBlzclrgu5pjSh4rUFgyU_lpHSRKSHmDaoSvVFynQe7G29-xRT54QXq35_dzIYRVEiHGhv-8qX2b8R_G237_dHIaZr5LpXbLWA7Y6j_78USKHy1t0Mpa2kKLK-bjmYlPniyX-CMocX_KwfQJplnrpLet-4vZiXo9HaPP_jaVOm6HSj-O3vdsra0Dn1fFBIt10kBpt0j1LuQlShjT2ivDgS1UjhiwGqtXRvj_iAN1SKWYuV2CXMqAFg4lkHCtfcPF0H_YUiHYiVup-xQI0cnBtbVmVR7VlvJs3S98H0hhuVyfNfp0b7KoFqwbDaw6Cfc3ohxRD-pnn5ZAfqcKFbuEYyqsHbiUAdtaFIgF07hQTNk-cswO0zaw8jQofrAkS_GjR4QCL1HY2mvWnl1g6fpi4yR28n5O6jRbtcs6MSxILh2QJhZBttmHkKYGDr218971kvmPWL9gcS981xfSNPxEjzd6IcCUrgh4ObV19OLr1JUgigqH2mD2g1JcEmgvX5SQuxIhDOQKnFPuF4AEYFO7Y58ZjTls4GTTEhBTOpSDTtIN1OYtspW9OjpXGhSVH5BjnBGwuG5HYPf-SERJMC1Pkg"; response = WS.url(String.format(NEXT_PAGE_URL, nextPageToken, KEY)).get(); resStr = WS.getResponseAsString(response); System.out.println("resStr2==" + resStr); /** * 获取某个地点对应的照片。然后将得到的 url 复制到 浏览器中查看 * photoreference 这个值 是在上面的请求中得到的 */ String photoreference = "CmRaAAAAdxvDbnaBSQO4MDseo-3SB_TZ4pd2c1EC765iu_Vu3-2XOA-LFgaZ6iiTY5sYCKxh_ZiQP0ds6qVDP4RAQn4Lxw6OEonSpgzzBBU0BrwrYMP91shZA1HIkaQVZaxKtYeqEhCKrYqwlzcV8dVs4Xv7AY2KGhRdpmMGa3n0S6d2n5cN6SckW49D6g"; resStr = String.format(PHOTO_URL, photoreference, KEY); System.out.println("resStr3==" + resStr); &#125;&#125; 得到的返回结果为（有点长，省略了一部分json。。需要什么值 直接解析下面的JSON即可）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899resStr==&#123; &quot;html_attributions&quot; : [], &quot;next_page_token&quot; : &quot;CoQE_QEAAFOBxJ8gWNrKkExgSbivQYAd46cKlMl8LNTu-ZIx8y3JmpiYhh5dqi3m9sUUwxcQ-MH7eKUgfljdH4RSlU1ExLyzw0vhfvYwfOfdmnbnIHz9xPM1hh5vBgYW2xn85NDRHNYjvB7uGTKoT3w7J5ZIy0pGCesQMjb3ritCHMT-y24DazvRsMEYgsyHk5H7TZWi6169xPyPgKi5uH5wSBpAB0zZdbuGGUY9979sVzRor32JEx6K-hkH6nfo8b1-gIrXBg_YASS81wkkHlqhMj04RDROXBpe_5Y9s4DMqGZNJuAem1Y3O0aEspDI-tT-swpUUYpe3Nvve9seDO7tqkwPWaAGMKGynt5ZlF0GSUXcEao-6dKuvRE0RaiOpdwNTNvGN7kYu6ACfAmDPnXmmIGoo-kdLhzxB4YVXgFYzHiaRIGE6Agj5kCoZ2HNaq-WHQWxx3CquyRV6bb77O_L-rds48ukOVYZ2QkhmfaJAfHGeGF0AyuPcUSip9PkjztzVzRw0oT7m3cASeTyWhci6e97h4chEKhddAc2xAqYZQ-hC8G4uyjp6L0uO_9ENEKYRRFRYWTtYtlpC_mn_xwyr76WN_IjYIyQDci2fFa9y12sPa3fow0W0FOlACfUc-RmU7f_ml1gRAYzyAQIHbDBYxYSS3dhuGDSIBnZfC3Upgh_O51jEhCkXggwwuSE1-t2CtLV_CotGhRCej6bWUC0D4ObssTvgnosi2LDHA&quot;, &quot;results&quot; : [ &#123; &quot;geometry&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : -33.8688197, &quot;lng&quot; : 151.2092955 &#125;, &quot;viewport&quot; : &#123; &quot;northeast&quot; : &#123; &quot;lat&quot; : -33.5781409, &quot;lng&quot; : 151.3430209 &#125;, &quot;southwest&quot; : &#123; &quot;lat&quot; : -34.118347, &quot;lng&quot; : 150.5209286 &#125; &#125; &#125;, &quot;icon&quot; : &quot;https://maps.gstatic.com/mapfiles/place_api/icons/geocode-71.png&quot;, &quot;id&quot; : &quot;044785c67d3ee62545861361f8173af6c02f4fae&quot;, &quot;name&quot; : &quot;Sydney&quot;, &quot;photos&quot; : [ &#123; &quot;height&quot; : 1536, &quot;html_attributions&quot; : [ &quot;\u003ca href=\&quot;https://maps.google.com/maps/contrib/115027288387975928704/photos\&quot;\u003eAlan Chen\u003c/a\u003e&quot; ], &quot;photo_reference&quot; : &quot;CmRaAAAA2CmGfo6miJRR93a5XHlI8CUC8ms65rFBpvc5S6UZaKDffZkd3ACtDfnmemKl-AiCeYtev2l3-e8TSVK79B12jINbfk7pzmydQv2auPDTzCqpuGaFqSnwmn6wbzolzZcfEhBcKU3f6uUEPh6gtA3qlq_ZGhS3kXjaoHny1qtBO7YcDcIUmryV-g&quot;, &quot;width&quot; : 2300 &#125; ], &quot;place_id&quot; : &quot;ChIJP3Sa8ziYEmsRUKgyFmh9AQM&quot;, &quot;reference&quot; : &quot;ChIJP3Sa8ziYEmsRUKgyFmh9AQM&quot;, &quot;scope&quot; : &quot;GOOGLE&quot;, &quot;types&quot; : [ &quot;colloquial_area&quot;, &quot;locality&quot;, &quot;political&quot; ], &quot;vicinity&quot; : &quot;Sydney&quot; &#125; ], &quot;status&quot; : &quot;OK&quot;&#125;resStr2==&#123; &quot;html_attributions&quot; : [], &quot;next_page_token&quot; : &quot;CqQFnQIAACwVVBaTf9R5PKhw3tFgSYiobH1a7xJR4sAj8tMXbjdPG8iyUafylxU9Ve1LsQncixVCFUprcYeSpjPzpk1JMCHRzPcEQ0UkJsSSt4Gnmwbqn2sJ6EnUywGul-rVN9tg2No3KGx3ezIBc8ITnogFZAxXCkfGsP6ty4y4wC1Zqc4d4JjYD-P_JIwlSQmO5kjCKLfrnMzbAIaQrYWcUVnqcbtYdxiv41u7UL3zaEly7wDEK4d0kGnOriD1GCFD5Drl9KnGeNJ5kdCT3tiUduNQMioiU5XkKrb8DYLpEfBmCpgqCxL2_AKssad-WBGGu_OU5yIz1NOqi1g78q_-R7JF3rfKo2ZQ2KF8XV3vOrhoK_Y6699-kZ7XQI0ztxoRTUTssiBnpjBRhkDWI-IACrj9FwPyogE0qCB0BEDyuXUk05tR9GqSoLy56JLMSimFVCiUSHz2dXnStHl7Sg-wia562jQacFgjq_w1_wJmHvYr-QRwKt-YPCZuS5x7Fo7dzz5qJniulQ_FB5UwVVw-DHuXF5KJJ0x8uFLbiCm-9B4q7XjF3Y8rTX94tnpfR9ow92GTXL1GHnQZRC67cQqEZG7w5OSHnsyv0rbagM-DYfBfB4dxP1CBrXvZxVs713Eh3kwNqfSJaBZDkOJKzF-ObMuVfTLL3giugfx1knM4j_--0e_117MzeC4skxQRV6Q67kQqpf3lDQmMPUZX87BULX6Lx394G6DBQYf2XUwPGrRF9c_CckUo2_OOg5KbdAUQqlfTE0-Wk6l8b4njzJ8_BluMHMZLwGqa_SllxEsY_4F8vMRw-ml_6gsc_lapkrh9MylaPD7FeHioTEDKPB_UcJ5O67pNDRfyFee1jW2MHyfAev-3RpyT18kgk_QVNQhkjsO9NBIQh41tRo68GtQhXHfxlx1MJxoUcP_2zUJbBnv0cKwt44vWPiTWo4A&quot;, &quot;results&quot; : [ &#123; &quot;geometry&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : -33.86536760000001, &quot;lng&quot; : 151.2090887 &#125;, &quot;viewport&quot; : &#123; &quot;northeast&quot; : &#123; &quot;lat&quot; : -33.86401861970851, &quot;lng&quot; : 151.2104376802915 &#125;, &quot;southwest&quot; : &#123; &quot;lat&quot; : -33.8667165802915, &quot;lng&quot; : 151.2077397197085 &#125; &#125; &#125;, &quot;icon&quot; : &quot;https://maps.gstatic.com/mapfiles/place_api/icons/lodging-71.png&quot;, &quot;id&quot; : &quot;7966e287de7b33958cd5fac4bdcab4c3c8a7cf75&quot;, &quot;name&quot; : &quot;Radisson Blu Plaza Hotel Sydney&quot;, &quot;opening_hours&quot; : &#123; &quot;open_now&quot; : true &#125;, &quot;photos&quot; : [ &#123; &quot;height&quot; : 1243, &quot;html_attributions&quot; : [ &quot;\u003ca href=\&quot;https://maps.google.com/maps/contrib/113323939240497973930/photos\&quot;\u003eRadisson Blu Plaza Hotel Sydney\u003c/a\u003e&quot; ], &quot;photo_reference&quot; : &quot;CmRaAAAAGqGsjVf5XLDhg0WDZJHoo6K2GD6145vEVOOPL6oRSYJOTnQ7IB7xg_HR7VAx5Txkv_xdKRg9X5qJ9dHluAXV-eMifF4oqizMSnzjSw62OzCBLySWJmVw-SpievEfp-GrEhAfho_Cnggh1UjsWhaW1l1SGhQccEY-CZKWsXSuZFifHGWttUyKpA&quot;, &quot;width&quot; : 1244 &#125; ], &quot;place_id&quot; : &quot;ChIJI6ovxEGuEmsRAdcebtTwTrU&quot;, &quot;plus_code&quot; : &#123; &quot;compound_code&quot; : &quot;46M5+VH Sydney, New South Wales, Australia&quot;, &quot;global_code&quot; : &quot;4RRH46M5+VH&quot; &#125;, &quot;rating&quot; : 4.5, &quot;reference&quot; : &quot;ChIJI6ovxEGuEmsRAdcebtTwTrU&quot;, &quot;scope&quot; : &quot;GOOGLE&quot;, &quot;types&quot; : [ &quot;lodging&quot;, &quot;point_of_interest&quot;, &quot;establishment&quot; ], &quot;user_ratings_total&quot; : 1080, &quot;vicinity&quot; : &quot;27 O&apos;Connell Street, Sydney&quot; &#125; ], &quot;status&quot; : &quot;OK&quot;&#125;resStr3==https://maps.googleapis.com/maps/api/place/photo?maxwidth=480&amp;photoreference=CmRaAAAAdxvDbnaBSQO4MDseo-3SB_TZ4pd2c1EC765iu_Vu3-2XOA-LFgaZ6iiTY5sYCKxh_ZiQP0ds6qVDP4RAQn4Lxw6OEonSpgzzBBU0BrwrYMP91shZA1HIkaQVZaxKtYeqEhCKrYqwlzcV8dVs4Xv7AY2KGhRdpmMGa3n0S6d2n5cN6SckW49D6g&amp;key=AIzaSyD-M2PDsTZaEP28taVQD9wysAJLyZxkUDM]]></content>
      <categories>
        <category>Social</category>
      </categories>
      <tags>
        <tag>google</tag>
        <tag>map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的会高效的在GitHub搜索开源项目吗?]]></title>
    <url>%2F2019%2F02%2F13%2F%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BC%9A%E9%AB%98%E6%95%88%E7%9A%84%E5%9C%A8GitHub%E6%90%9C%E7%B4%A2%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%90%97%2F</url>
    <content type="text"><![CDATA[文章来源于【纯洁的微笑】公众号 GitHub的流行， GitHub在开源世界的受欢迎程度自不必多言。再加上今天，GitHub官方又搞了个大新闻：私有仓库也改为免费使用，这在原来可是需要真金白银的买的。可见微软收购后，依然没有改变 GitHub 的定位，甚至还更进一步。 花开两朵，各表一枝。我们今天想要聊的并不是 GitHub 多么重要，而是要说一下 GitHub 的搜索功能。 你在 GitHub上搜索代码时，是怎么样操作的呢？是不是也是像我这样，直接在搜索框里输入要检索的内容，然后不断在列表里翻页找自己需要的内容？ 或者是简单筛选下，在左侧加个语言的过滤项。 再或者改变一下列表的排序方式 这就是「全部」了吗？ 一般的系统检索功能，都会有一个「高级搜索」的功能。需要在另外的界面里展开，进行二次搜索之类的。 GitHub 有没有类似的呢？ 答案是「肯定的」。做为一个为万千工程师提供服务的网站，不仅要有，而且还要技术范儿。 如果我们自己开发一个类似的应用，会怎样实现呢？ 带着思路，咱们一起来看看，GitHub 是怎样做的。 这里我们假设正要学习 Spring Cloud，要找一个 Spring Cloud 的 Demo 参考练手。 1. 明确搜索仓库标题、仓库描述、README GitHub 提供了便捷的搜索方式，可以限定只搜索仓库的标题、或者描述、README等。 以Spring Cloud 为例，一般一个仓库，大概是这样的 其中，红色箭头指的两个地方，分别是仓库的名称和描述。咱们可以直接限定关键字只查特定的地方。比如咱们只想查找仓库名称包含 spring cloud 的仓库，可以使用语法 in:name关键词 如果想查找描述的内容，可以使用这样的方式： in:descripton 关键词 这里就是搜索上面项目描述的内容。 一般项目，都会有个README文件，如果要查该文件包含特定关键词的话，我想你猜到了 in:readme 关键词 2. 明确搜索 star、fork 数大于多少的 一个项目 star 数的多少，一般代表该项目有受欢迎程度。虽然现在也有垃圾项目刷 star ，但毕竟是少数， star 依然是个不错的衡量标准。 stars: &gt;数字 关键字。 比如咱们要找 star 数大于 3000 的Spring Cloud 仓库，就可以这样 stars:&gt;3000 spring cloud 如果不加 &gt;= 的话，是要精确找 star 数等于具体数字的，这个一般有点困难。 如果要找在指定数字区间的话，使用 stars: 10..20 关键词 fork 数同理，将上面的 stars 换成 fork，其它语法相同 3. 明确搜索仓库大小的 比如你只想看个简单的 Demo，不想找特别复杂的且占用磁盘空间较多的，可以在搜索的时候直接限定仓库的 size 。 使用方式： size:&gt;=5000 关键词 这里注意下，这个数字代表K, 5000代表着5M。 4. 明确仓库是否还在更新维护 我们在确认是否要使用一些开源产品，框架的时候，是否继续维护是很重要的一点。如果已经过时没人维护的东西，踩了坑就不好办了。而在 GitHub 上找项目的时候，不再需要每个都点到项目里看看最近 push 的时间，直接在搜索框即可完成。 元旦刚过，比如咱们要找临近年底依然在勤快更新的项目，就可以直接指定更新时间在哪个时间前或后的 通过这样一条搜索pushed:&gt;2019-01-03 spring cloud 咱们就找到了1月3号之后，还在更新的项目。 你是想找指定时间之前或之后创建的仓库也是可以的，把 pushed 改成 created就行。 5. 明确搜索仓库的 LICENSE 咱们经常使用开源软件，一定都知道，开源软件也是分不同的「门派」不同的LICENSE。开源不等于一切免费，不同的许可证要求也大不相同。 2018年就出现了 Facebook 修改 React 的许可协议导致各个公司纷纷修改自己的代码，寻找替换的框架。 例如咱们要找协议是最为宽松的 Apache License 2 的代码，可以这样 license:apache-2.0 spring cloud 其它协议就把apache-2.0替换一下即可，比如换成 mit 之类的。 6. 明确搜索仓库的语言 比如咱们就找 Java 的库， 除了像上面在左侧点击选择之外，还可以在搜索中过滤。像这样： language:java 关键词 7.明确搜索某个人或组织的仓库 比如咱们想在 GitHub 上找一下某个大神是不是提交了新的功能，就可以指定其名称后搜索，例如咱们看下 Josh Long 有没有提交新的 Spring Cloud 的代码，可以这样使用 user:joshlong 组合使用一下，把 Java 项目过滤出来，多个查询之间「空格」分隔即可。 user:joshlong language:java 找某个组织的代码话，可以这样： org:spring-cloud 就可以列出具体org 的仓库。 这个搜索使用起来是不是相当的便捷? 比起直接搜一个关键词不停的翻页点开找效率高多了吧。 推荐阅读： 孤独求败张小龙 在流感爆发的季节里 官方攻略： 高级查询：https://github.com/search/advanced 查询帮助：https://help.github.com/articles/about-searching-on-github]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(7) - 运算符]]></title>
    <url>%2F2019%2F01%2F05%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-7-%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[运算符用于在程序运行时执行数学或逻辑运算。 Go语言内置的运算符有： 算术运算符 关系运算符 逻辑运算符 位运算符 赋值运算符 其他运算符 算术运算符 算术运算符主要有：+、-、*、/、%（求余）、++（自增）、–（自减） 关系运算符 关系运算符主要有：==、!=、&gt;、&lt;、&gt;=、&lt;= 逻辑运算符 逻辑运算符主要有：&amp;&amp;（逻辑AND）、|| （逻辑OR）、! （逻辑NOT） 位运算符位运算符对整数在内存中的二进制进行操作。 位运算符比一般的算术运算符速度要快，而且可以实现一些算术运算符不能实现的功能。如果要开发高效率程序，位运算符是必不可少的。位运算符用来对二进制进行操作，包括：按位与（&amp;）、按位或（|）、按位异或（^）、按位左移（&lt;&lt;）、按位右移（&gt;&gt;） 按位与按位与（&amp;）：对两个数进行操作，然后返回一个新的数，这个数的每个位都需要两个输入数的同一位都为1时 才为1。简单的说：同一位同时为1 则为1 按位或按位或（|）：比较两个数，然后返回一个新的数，这个数的每一位置设置1的条件是任意一个数的同一位为1 则为1。简单的说：同一位其中一个为1 则为1 按位异或按位异或（^）：比较两个数，然后返回一个数，这个数的每一位设为1的条件是两个输入数的同一位不同 则为1，如果相同就设为 0 。简单的说：同一位不相同 则为1 左移运算符（&lt;&lt;）按二进制形式把所有的数字向左移动对应的位数，高位移出（舍弃），低位的空位补零 语法格式： 需要移位的数字 &lt;&lt; 移位的次数 例如：3 &lt;&lt; 4，则是将 数字3 左移了4位 计算过程： 3 &lt;&lt; 4 首先把 3 转换为 二进制数字 0000 0000 0000 0000 0000 0000 0000 0011，然后把该数字高位（左侧）的两个零移出。其他的数字都朝左平移4位，最后在 低位（右侧）的两个空位补零，则得到的最终结果为 0000 0000 0000 0000 0000 0000 0011 0000，则转换为十进制为 48 用 3 * 2 ^4 计算 更加方便，3 乘以 2 的4次方 数字意义 在数字没有溢出的前提下，对于正数 和 负数，左移一位 都相当于 乘以 2的一次方，左移 n 位就相当于 乘以2的 n 次方 右移运算符（&gt;&gt;）按二进制形式把所有的数字都向右移动对应 位移位数，低位移出（舍弃），高位的空位补符号位，即正数补零，负数 补1 语法格式 需要移位的数字 &gt;&gt; 移位的次数 例如：11 &gt;&gt; 2，则是将数字 11 右移2位 计算过程 11的二进制形式为：0000 0000 0000 0000 0000 0000 0000 1011，然后把低位的最后两个数字移出，因为该数字是正数，所有在高位补零，则得到的最终结果为 0000 0000 0000 0000 0000 0000 0000 0010，转化为十进制为2 用 11 / (2 ^ 2) 计算 更加方便，11 除以 2 的2次方 数字意义 右移一位相当于 除2，右移 n 位，则相当于 除以2的 n 次方 赋值运算符赋值运算符有：=、+=、-=、*=、/=、%=、&lt;&lt;=、&gt;&gt;=、&amp;=、^=、|=。 DEMO示例以下是上面知识点的代码演示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137package mainimport "fmt"var a = 21.0var b = 5.0var c float64func main() &#123; fmt.Println("算术运算符 --- start---") Arithmetic() fmt.Println() fmt.Println("关系运算符 --- start---") Relational() fmt.Println() fmt.Println("逻辑运算符 --- start---") Logical() fmt.Println() fmt.Println("位运算符 --- start---") Bitwise() fmt.Println() fmt.Println("赋值运算符 --- start---") Assignment() fmt.Println()&#125;// 算术运算符func Arithmetic() &#123; c = a + b fmt.Printf("第一行 - c 的值为 %.2f\n", c) c = a - b fmt.Printf("第二行 - c 的值为 %.2f\n", c) c = a * b fmt.Printf("第三行 - c 的值为 %.2f\n", c) c = a / b fmt.Printf("第四行 - c 的值为 %.2f\n", c) //c = a % b fmt.Printf("第五行 - c 的值为 %d\n", int(a)%int(b)) a++ fmt.Printf("第六行 - a 的值为 %f\n", a) a = 21 a-- fmt.Printf("第七行 - a 的值为 %f\n", a)&#125;// 关系运算符func Relational() &#123; if (a == b) &#123; fmt.Printf("第一行 - a 等于 b \n") &#125; else &#123; fmt.Printf("第一行 - a 不等于 b \n") &#125; if (a &lt; b) &#123; fmt.Printf("第二行 - a 小于 b \n") &#125; else &#123; fmt.Printf("第二行 - a 不小于 b \n") &#125; if (a &gt; b) &#123; fmt.Printf("第三行 - a 大于 b \n") &#125; else &#123; fmt.Printf("第三行 - a 不大于 b \n") &#125;&#125;// 逻辑运算符func Logical() &#123; a := true b := false if (a &amp;&amp; b) &#123; fmt.Printf("第一行 - 条件为 true \n") &#125; if (a || b) &#123; fmt.Printf("第二行 - 条件为 true \n") &#125; if (!(a &amp;&amp; b)) &#123; fmt.Printf("第三行 - 条件为 true \n") &#125;&#125;// 位运算符func Bitwise() &#123; fmt.Println(252 &amp; 63) fmt.Println(178 | 94) fmt.Println(20 ^ 5) fmt.Println(3 &lt;&lt; 4) // 3 * 2 ^ 4 fmt.Println(11 &gt;&gt; 2) // 11 / (2 ^ 2)&#125;// 赋值运算符func Assignment() &#123; c = a fmt.Printf("第1行 - =运算符实例，c的值为 %f \n", c) c += a fmt.Printf("第2行 - +=运算符实例，c的值为 %f \n", c) c -= a fmt.Printf("第3行 - -=运算符实例，c的值为 %f \n", c) c *= a fmt.Printf("第4行 - *=运算符实例，c的值为 %f \n", c) c /= a fmt.Printf("第5行 - /=运算符实例，c的值为 %f \n", c) c = a fmt.Printf("第6行 - =运算符实例，c的值为 %f \n", c) d := 200 d &lt;&lt;= 2 fmt.Printf("第7行 - &lt;&lt;=运算符实例，d的值为 %d \n", d) // d = d * 2^2 = 200 * 2^2 = 800 d &gt;&gt;= 2 fmt.Printf("第8行 - &gt;&gt;=运算符实例，d的值为 %d \n", d) // d = d / (2^2) = 800 / (2^2) = 200 d &amp;= 2 fmt.Printf("第9行 - &amp;=运算符实例，d的值为 %d \n", d) // d = d &amp; 2 = 0 d |= 2 fmt.Printf("第10行 - |=运算符实例，d的值为 %d \n", d) // d = d | 2 = 2 d ^= 2 fmt.Printf("第11行 - ^=运算符实例，d的值为 %d \n", d) // d = d ^ 2 = 0&#125; 运行结果为： 12345678910111213141516171819202122232425262728293031323334353637算术运算符 --- start---第一行 - c 的值为 26.00第二行 - c 的值为 16.00第三行 - c 的值为 105.00第四行 - c 的值为 4.20第五行 - c 的值为 1第六行 - a 的值为 22.000000第七行 - a 的值为 20.000000关系运算符 --- start---第一行 - a 不等于 b 第二行 - a 不小于 b 第三行 - a 大于 b 逻辑运算符 --- start---第二行 - 条件为 true 第三行 - 条件为 true 位运算符 --- start---6025417482赋值运算符 --- start---第1行 - =运算符实例，c的值为 20.000000 第2行 - +=运算符实例，c的值为 40.000000 第3行 - -=运算符实例，c的值为 20.000000 第4行 - *=运算符实例，c的值为 400.000000 第5行 - /=运算符实例，c的值为 20.000000 第6行 - =运算符实例，c的值为 20.000000 第7行 - &lt;&lt;=运算符实例，d的值为 800 第8行 - &gt;&gt;=运算符实例，d的值为 200 第9行 - &amp;=运算符实例，d的值为 0 第10行 - |=运算符实例，d的值为 2 第11行 - ^=运算符实例，d的值为 0]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>运算符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(6) - 数据类型转换和常量以及iota]]></title>
    <url>%2F2018%2F12%2F31%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-6-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E5%92%8C%E5%B8%B8%E9%87%8F%E4%BB%A5%E5%8F%8Aiota%2F</url>
    <content type="text"><![CDATA[数据类型转换的格式1.T（表达式） 采用数据类型前置加括号的方式进行类型转换。T表示要转换的类型；表达式包括变量、数值、函数返回值等 类型转换时，需要考虑采用两种类型之间的关系和范围，是否会发生数值截断 布尔型无法与其他类型转换 2.float与int之间的转换 float转int会导致精度损失 3.int转string 相当于是 byte 或 rune 转string 该int 值是ASCII吗的编号或者Unicode字符集的编号，转成 string 就是讲根据字符集，将对应编号的字符查找出来 当该值超过Unicode编号的返回，则转成的字符串显示为乱码 例如，当19968转string，就是“一” 【备注】 ASCII字符集中数字的10进制范围是 [30 - 39] ASCII字符集中大写字母的10进制范围是 [65 - 90] ASCII字符集中小写字母的10进制范围是 [97 - 122] Unicode字符集中汉字的范围是 [4e00 - 9fa5]，10进制的范围是[19968 - 40869] 1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport "fmt"func main() &#123; /* 语文成绩是 90，英语成绩是 80.5，计算平均值 */ chinese := 90 english := 80.5 // float转int avg := (chinese + int(english)) / 2 fmt.Println("平均值：" , avg) // 平均值： 85 // int转float avg2 := (float64(chinese) + english) / 2 fmt.Println("平均值2：" , avg2) // 平均值2： 85.25 // 布尔型不能转换为其他 // flag := true // int(flag) // 字符串不能转为int // str := "Joe" // int(str) result := string(chinese) fmt.Println("语文成绩转string，结果为：", result) // 语文成绩转string，结果为： Z x := 'Z' fmt.Println("字符Z转string，结果为：" , string(x)) // 字符Z转string，结果为： Z X := '一' fmt.Println("字符 一 转string，结果为：" , string(X)) // 字符 一 转string，结果为： 一 XX := 19968 fmt.Println("数值19968 转string，结果为：" , string(XX)) // 数值19968 转string，结果为： 一&#125; 常量声明方式1.相对于变量，常量是恒定不变的值，例如圆周率 常量是一个简单值的标识符，在程序运行时，不会被修改 2.常量中的数据类型只可以是布尔型、数字型（整数型、浮点型和复数）和字符串型 3.常量的定义格式： const 标识符 [类型] = 值 可以省略类型说明符 [type]，因为编译器可以根据变量的值来自动推断其类型 显示类型定义： const B string = “Joe” 隐式类型定义： const C = “Joe” 4.多个相同类型的声明可以简写为： const WIDTH , HEIGHT = value1, value2 5.常量定义未被使用，不会再编译时出错 常量用于枚举（常量组）例如如下格式： 12345const ( Unkonwn = 0 Female = 1 Male = 2) 数字0、1、2分别表示未知性别、女性、男性 常量组中如果不指定类型和初始值，则与上一行非空常量的值相同 12345const ( a = 10 b c) 打印 a b c ，输出 ：10 10 10 iota1.iota ，特殊常量值，是一个系统定义的可以被编译器修改的常量值。iota只能出现在常量中。 2.在每一个const 关键字出现时，被重置为0，然后每出现一个常量，iota所代表的数值会自动增加1。iota可以理解成常量组中的常量的计数器，不论该常量的值是什么，只要有一个常量，那么iota就加1 3.iota可以被用作枚举值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport &quot;fmt&quot;func main() &#123; const ( L = iota M = iota N = iota ) fmt.Println(L, M, N) // 输出：0 1 2 const ( L1 = iota M1 N1 ) fmt.Println(L1, M1, N1) // 输出：0 1 2 const ( L2 = &quot;Joe&quot; M2 = iota N2 ) fmt.Println(L2, M2, N2) // 输出：Joe 1 2 const ( i = 1 &lt;&lt; iota // 1 * 2^iota j = 3 &lt;&lt; iota // 3 * 2^iota k l ) fmt.Println(i, j, k ,l) // 输出：1 6 12 24 const ( a1 = &apos;一&apos; b1 c1 = iota d1 ) fmt.Println(a1, b1, c1 ,d1) // 输出：19968 19968 2 3 const name = iota fmt.Println(&quot;name=&quot;, name) // 输出：name= 0&#125;]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>数据类型转换</tag>
        <tag>常量</tag>
        <tag>iota</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次从删库到恢复的经历]]></title>
    <url>%2F2018%2F12%2F31%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BB%8E%E5%88%A0%E5%BA%93%E5%88%B0%E6%81%A2%E5%A4%8D%E7%9A%84%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[公司正在做社交项目，涉及到的各个端人员比较多，开发、运维、产品、测试20多个，当然，对大公司来说，20其实不算多，进入正题，我们数据库使用的是MySQL，运维搭建的主从，而正是由于这个所谓的主从，害苦了开发的兄弟们；缓存使用的是Redis，MySQL数据同步到Redis使用的是阿里开源的Canal，了解Canal的都知道，Canal是读取MySQL的二进制文件，进而得到数据的变更，然后同步到Redis。 某天傍晚，后台开发兄弟们日常更新，由于产品要求用户的昵称支持输入表情，我们不得不修改MySQL的配置文件，使其支持表情，修改了Master配置文件，自然就需要修改Slave 的配置文件，将Master重启后，然后重启Slave，都重启后，发现master上的数据不能同步到slave了，我擦，不过由于项目刚刚起步，访问量不大，加上天色已晚，后台开发兄弟决定，不能同步就不能同步吧，明天找运维同事搞定。 第二天，运维同事了解情况后，连上服务器，一顿操作。下午三点左右， 产品正在演示APP，说APP首页没有数据了，我们说，呵呵，怎么可能，然后登上数据库后，我我我擦，master 用户表的数据被清空了，瞬间有点发毛，难道被攻击了，赶紧看下 slave，我*，比master上的数据都干净，而此时的master 里面的用户表 还有几条刚刚注册进来的几条数据，冷汗直流。缓存，对，看下缓存中，赶紧连上缓存服务器，疑问了，缓存里面 还有数据，master 用户表的数据没有了，而缓存又是读取的master里面的二进制文件，先不考虑这个问题，目前的紧急问题是赶紧恢复数据。由于开启了binlog，那就从这里恢复，一同事赶紧下载binlog，玛德，文件太大，拉下来 将近40分钟，这个方法暂缓。诶，缓存不是有数据嘛，那就把缓存里面的数据重新写到MySQL，赶紧写一个for循环，从 丢失的数据的 ID 往前读，然后一个个insert到数据库，当然，这种方法会导致数据有点旧，不过，和数据完全丢失比起来，那就毛毛雨了。终于，历经将近一小时，数据恢复。 数据恢复后，我们首要的目标就是查原因，原因无非就是两个，一是被攻击，二是内部人员所为。查看MySQL的二进制日志，数据丢失大约是在3点左右，而服务器是在曼谷，时间比北京时间晚一个小时，也就是服务器上的时间是2点左右，拉二进制文件找原因，呵，太慢了，几行命令搞定， 12345-- 查看某个时间段的二进制日志，并且输出到指定的文件mysqlbinlog --no-defaults --start-datetime=&quot;2018-12-12 13:00:00&quot; --stop-datetime=&quot;2018-12-12 14:40:00&quot; mysql-bin.000085 -vv --base64-output=decode-rows | more &gt;&gt; target.txt-- 将@1、@2等一系列看不懂的符号转换为SQL语句cat target.txt | sed -n &apos;/###/p&apos; | sed &apos;s/### //g;s/\/\*.*/,/g;s/DELETE FROM/INSERT INTO/g;&apos; | sed -r &apos;s/(@4.*),/\1;/g&apos; | sed &apos;s/@[0-9]*\=//g&apos; &gt; test.sql 然后将 test.sql 文件拉下来，查看，发现两处可疑点（即两条SQL），如下： 1234-- 创建一个像user一样的表，user_bakcreate table user_bak like user;-- 删除 user_bakdrop table user_bak; 但是，没有清空或者删除user 表的sql，奇了怪了。突然，技术总监过来问，有没有找到原因，我说还没有，只是找到了两条可疑的SQL，但是还没发现有清空user的SQL，因为一直在看master上的日志，因为是主从，看master 上的日志和看slave上的一样，然而，总监却说，slave上的日志也要查下，好吧，虽然觉得看了没什么用，谁让他是老大呢。登上slave服务器，却发现，为什么 slave 上也有二进制日志呢，先不管，看下日志，发现有如下SQL： 1234567891011-- 删除 userdrop table user;-- 创建usercreate table user( ...省略字段)COMMENT=&apos;用户表&apos;COLLATE=&apos;utf8_general_ci&apos;ENGINE=InnoDBAUTO_INCREMENT=4xxxxxx; 虽然打脸了，但是为什么slave上也会有二进制日志呢，赶紧找运维问下到底是不是主从，运维说是，算了，还是看下配置文件，呵呵，竟然是主主，不是说好的主从呢，怪不得在 slave上执行了 删除用户表，然后创建用户表，创建的下一个自增点还是丢失前的自增点，master上用户表的数据就不存在了，这样Redis中数据存在的原因也破解了，Redis 读取的是master中的二进制文件，正是由于从所谓的“从”删除然后重建user表后（记录到“从”的二进制日志），“主”上就有了user 表（记录到“主”的中继日志，并没有在这个“主”的二进制日志，所以 Canal根本没有读取到在“从”上的操作），原因是找到了，但是 是谁执行的呢？？当我们正在讨论的时候，运维说他在做测试，测试昨天的主从有没有正常了，他在修复昨天不能主从复制的问题。 崩溃，修复就修复，执行删除干嘛，而说好的主从，咋还变成主主了，问题 原因找到了，意味着不用加班到天亮了。 这种问题一定要杜绝： 非特殊情况，禁止使用ROOT账户； 相关人员分配MySQL账户，制定权限； 禁止执行不带条件的DELETE、UPDATE，TRUNCATE，DROP]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>删库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[根据IP定位地理位置]]></title>
    <url>%2F2018%2F12%2F26%2F%E6%A0%B9%E6%8D%AEIP%E5%AE%9A%E4%BD%8D%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[背景： 项目在海外运行，需要根据IP获取国家，城市，经纬度等信息，但是，百度地图、高德地图、淘宝等API的使用不了，而谷歌地图的又有频率限制，于是网上各种搜索，找到 GeoLiteCity.dat，GeoLiteCity.dat就好比一个本地的数据库文件，方法如下： 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.maxmind.geoip&lt;/groupId&gt; &lt;artifactId&gt;geoip-api&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 测试类如下： 12345678910111213141516171819public class IPTest &#123; public static void main(String[] args) &#123; try &#123; LookupService cl = new LookupService("C:\\GeoLiteCity.dat", LookupService.GEOIP_MEMORY_CACHE); Location l2 = cl.getLocation("128.1.35.120"); System.out.println( "countryCode: " + l2.countryCode +"\n"+ "countryName: " + l2.countryName +"\n"+ "region: " + l2.region +"\n"+ "city: " + l2.city +"\n"+ "latitude: " + l2.latitude +"\n"+ "longitude: " + l2.longitude); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果如下： 123456countryCode: THcountryName: Thailandregion: 40city: Bangkoklatitude: 13.753998longitude: 100.5014 项目中使用如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import com.maxmind.geoip.Location;import com.maxmind.geoip.LookupService;/** * @author: xbq * @Date: 2018/8/1 09:35 */@Servicepublic class LoadIp &#123; private LookupService cl; @PostConstruct public void init() &#123; try &#123; cl = new LookupService(systemConstants.getIpDb(), LookupService.GEOIP_MEMORY_CACHE); &#125; catch (IOException e) &#123; CusLogger.error("加载ip纯真库异常：" + e.getMessage(), e); &#125; &#125; /** * 使用 * @param ip * @return */ public void fun(String ip) &#123; // 根据ip来判定国家地区 if(cl != null) &#123; Location l2 = null; l2 = cl.getLocation(ip); if(l2 != null) &#123; // 获取国家编码 String countryCode = l2.countryCode; // 获取国家名称 String countryName = l2.countryName; // 获取城市 String city = l2.city; // 业务处理 ... &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Social</category>
      </categories>
      <tags>
        <tag>Social</tag>
        <tag>定位</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(5) - 打印格式化]]></title>
    <url>%2F2018%2F12%2F07%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-5-%E6%89%93%E5%8D%B0%E6%A0%BC%E5%BC%8F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[打印格式化中我们常常 需要用到的格式化的标记，每个标记实际来源于我们的单词，本文介绍Go语言中的打印格式化。 通用 %v：值的默认格式，对应英文为：value %T：值的类型，对应英文为：Type 布尔值%t：单词 true或者 false，对应英文：true 整型 %b：表示为二进制，对应英文：binary %c：该值对应的inicode码值，对应英文：char %d：表示为十进制，对应英文：digital %8d：表示该整型长度为8，不足8位，则在数值前补空格，超过8，则以实际为准 %08d：数字长度为8，不足8位，则在数值前补0，超过8，则以实际为准 %o：表示为八进制，对应英文：octal %q：该值对应的单引号 括起来的的Go语法字符字面值，必要时 会采用安全的转义表示 %x：表示为 十六进制，使用 a-f，对应英文：hex %X：表示为 十六进制，使用 A-F，对应英文：hex %U 表示为 unicode格式：U+1234，等价于“U+%04X” 浮点与复数 %f (=%.6f) 有6位小数部分，如果想保留两位小数，则 %.2f 即可（使用的是四舍五入） %e (=%.6e) 有6位小数部分的科学计数法，如果想保留两位小数的科学计数，则 %.2e 即可 字符串和byte[] %s 直接输出字符串 或者 byte[] %q 该值的对应的双引号括起来的Go语法字符串字面值，必要时采用安全的转义表示 说了这么多文字，不如来点实际的，上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package mainimport "fmt"type Student struct &#123; x , y int&#125;func main() &#123; // 通用 a := 12 fmt.Printf("%T , a = %v \n", a, a) // 相当于实例化 p := Student&#123;1,2&#125; fmt.Printf("%T , a = %v \n", p, p) var s rune = '一' fmt.Printf("%T , a = %v \n", s, s) // 布尔 b := true fmt.Printf("%T , b = %t \n", b, b) // 整数 c := 123 // %b 表示为 二进制 fmt.Printf("%T , c = %b \n", c, c) // %d 表示为 十进制 fmt.Printf("%T , c = %d \n", c, c) // %8d 表示该整型长度为8，不足8则在数值前补空格，如果超出8，以实际为准 fmt.Printf("%T , c = %8d \n", c, c) // %08d 数字长度为8，不足8在前面补零，超出8，以实际为准 fmt.Printf("%T , c = %08d \n", c, c) // %o 表示为 八进制 fmt.Printf("%T , c = %o \n", c, c) // %x 表示为 十六进制，使用 a-f fmt.Printf("%T , c = %x \n", c, c) // %x 表示为 十六进制，使用 A-F fmt.Printf("%T , c = %X \n", c, c) // %U 表示为 unicode格式：U+1234，等价于“U+%04X” fmt.Printf("%T , c = %U \n", c, c) cc := '一' fmt.Printf("%T , cc = %U \n", cc, cc) d := 97 // %c 对应的是 unicode码值 fmt.Printf("%T , d = %c \n", d, d) // %q 该值对应的单引号 括起来的的Go语法字符字面值，必要时 会采用安全的转义表示 fmt.Printf("%T , d = %q \n", d, d) // 浮点数 e := 123.23456 // %f (=%.6f) 有6位小数部分，如果想保留两位小数，则 %.2f 即可（使用的是四舍五入） fmt.Printf("%T , e = %f \n", e, e) fmt.Printf("%T , e = %.2f \n", e, e) // %e (=%.6e) 有6位小数部分的科学计数法，如果想保留两位小数的科学计数，则 %.2e 即可 fmt.Printf("%T , e = %e \n", e, e) fmt.Printf("%T , e = %.2e \n", e, e) // %E 科学计数法 fmt.Printf("%T , %E \n", e, e) // 字符串和byte[] f := "测试" // %s 直接输出字符串 或者 byte[] fmt.Printf("%T , f = %s \n", f, f) // %q 该值的对应的双引号括起来的Go语法字符串字面值，必要时采用安全的转义表示 fmt.Printf("%T , f = %q \n", f, f) arr := [3]byte&#123;97, 98, 99&#125; fmt.Printf("%T , arr = %s \n", arr, arr) arr2 := [3]byte&#123;'a', 'b', 'c'&#125; fmt.Printf("%T , arr2 = %s \n", arr2, arr2) fmt.Printf("%T , arr2 = %x \n", arr2, arr2) fmt.Printf("%T , arr2 = %X \n", arr2, arr2) // 变量赋值 g := fmt.Sprintf(f) fmt.Println("g==" + g)&#125; 运行结果如下： 123456789101112131415161718192021222324252627int , a = 12 main.Student , a = &#123;1 2&#125; int32 , a = 19968 bool , b = true int , c = 1111011 int , c = 123 int , c = 123 int , c = 00000123 int , c = 173 int , c = 7b int , c = 7B int , c = U+007B int32 , cc = U+4E00 int , d = a int , d = &apos;a&apos; float64 , e = 123.234560 float64 , e = 123.23 float64 , e = 1.232346e+02 float64 , e = 1.23e+02 float64 , 1.232346E+02 string , f = 测试 string , f = &quot;测试&quot; [3]uint8 , arr = abc [3]uint8 , arr2 = abc [3]uint8 , arr2 = 616263 [3]uint8 , arr2 = 616263 g==测试]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>打印格式化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(2) - HelloWorld]]></title>
    <url>%2F2018%2F11%2F26%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-2-HelloWorld%2F</url>
    <content type="text"><![CDATA[下载地址 https://golang.org/dl ，这个地址需要翻墙下载 https://studygolang.com/dl ，可直接在此网站下载，windows下载如下： 安装配置环境变量1.假设将安装包安装在D盘，新建 如下环境变量： GPROOT：Go的安装路径 GOPATH：Go的工程路径（如果有多个，就以分号分隔添加） 在PATH中增加：D:\Go\bin;%GOPATH%\bin; 注：需要把GOPATH中的可执行目录页配置到环境变量中，否则下载的第三方go工具就无法使用了。 2.查看是否安装成功 go env：查看得到的go的配置信息 go version：查看go的版本号 编译工具安装这里使用的编译编译工具是goland，比较方便好用，用惯了IDEA的，用这个很顺手。 1.下载goland，地址见百度网盘： 链接：https://pan.baidu.com/s/1xEUsFpnfjOAb9ceZ4IzcHA ，提取码：t5mt 。 2.安装 安装属于傻瓜式安装，一路next即可，破解方法也在百度网盘中。 Hello World12345678package mainimport &quot;fmt&quot;func main() &#123; /* 这是一个简单的程序 */ fmt.Println(&quot;Hello World&quot;)&#125; 1.第一行 package main 定义了包名。必须在源文件非注释的第一行指明这个文件属于哪个包，如：package main，表示一个科独立执行的程序，每个Go应用程序都包含一个名为main的包。 2.下一行 import “fmt”，告诉Go编译器这个程序需要使用fmt包，fmt包中实现了格式化IO的函数。 3.下一行的 func main() 是程序的入口。main函数式每一个可执行程序必须包含的，一般来说都是在启动后的第一个可执行函数，如果有 init() 函数，则先执行 init() 函数。 4.下一行/ … / 是注释，在程序执行时被忽略。 5.下一行 fmt.Println( .. ) 可以将字符串输出到控制台上，并在最后增加换行符 \n 。 编码规范注释 单行注释是最常见的注释形式，可以再任何地方使用以 // 开头的单行注释 多行注释也叫块注释，均已 / 开头，并以 / 结尾，且不可以嵌套使用，多行注释一般用于文档描述或注释成块的代码片段。 标识符 标识符是用来命名变量、类型等程序实体。一个标识符实际上就是一个或者多个字母数字、下划线组成的序列，但是第一个字符必须是以字母或者下划线，而不能是数字。 Go不允许在标识符中使用@、$和&amp;等标识符。 Go是一种区分大小写的语言。 空格 Go语言中变量的声明必须使用空格隔开，如：var age int 语句中适当使用空格可以让程序更简易阅读 在变量与运算符间增加空格，程序会更加美观。 语句的结尾 在Go程序中，一行代表一个语句结束，不用使用分号结尾 如果需要将多个语句写在一行，必须使用分号区分 可见性规则 Go语言中，使用大小来决定标识符（常量、变量。接口、类型、结构或者函数）是否可以被外部包所调用 以大写字母开头，表示可以被外部包的代码所调用，此时就类似于Java中加了public 以小写字母开头，则对包外是不可见的，就类型与Java中加了private Go程序结构组成Go一般程序 12345678910111213141516171819202122232425// 当前的包名package main// 导入包名import . &quot;fmt&quot;// 常量定义const PI = 3.14// 全局类型的声明和赋值var name = &quot;Joe&quot;// 一般类型声明type newType int// 结构的声明type Joe struct &#123;&#125;// 接口的声明type xbq interface &#123;&#125;// 由main函数作为程序入口点启动func main() &#123; Println(&quot;hello World&quot;)&#125; Go文件的基本组成 包声明 引入包 函数 变量 语句 &amp; 表达式 注释 Go文件结构组成 Go程序是通过 package 来组织的 只有 package 名称为 main 的包可以包含 main 函数 一个可执行程序有且仅有一个 main 包 通过 import 关键字来导入其他非main包 可以通过 import 关键字单个导入，也可以多个导入 程序一般由关键字、常量、变量、运算符、类型和函数组成 程序中可能会使用这些分隔符，括号、中括号 和 大括号 程序中可能会使用这些标点符号，点（.）、逗号（,）、分号（;）、冒号（:）、省略号（…） 通过在函数体外部使用 var 关键字来进行全局变量的声明和赋值 通过 type 关键字来进行结构（struct）和接口（interface）的声明 通过 func 关键字来进行函数的声明]]></content>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(4) - 基本数据类型]]></title>
    <url>%2F2018%2F11%2F26%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-4-%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Go语言中的数据类型包含两种： 基本数据类型（原生数据类型）：整型、浮点型、布尔型、字符串、字符（byte、rune） 复合数据类型（派生数据类型）：指针（pointer）、数组（array）、切片（slice）、映射（map）、函数（function）、结构体（struct）、通道（channel） 整型整型分为两大类 按长度分：int8、int16、int32、int64、int 无符号整型：uint8、uint16、uint32、uint64、uint 其中，uint8就是byte型、int16对应C语言中的short型、int64对应C语言中的long型 序号 类型和描述 1 uint8：无符号8位整型（0到255）【2的8次方】 2 uint16：无符号16位整型（0到65535）【2的16次方】 3 uint32：无符号32位整型（0到4292967295）【2的32次方】 4 uint64：无符号位64位整型（0到 18446744073709551615）【2的64次方】 5 int8：有符号8位整型（-128到127） 6 int16：有符号16位整型（-32768到32767） 7 uint32：有符号32位整型（-2147483648到2147483647） 8 uint64：有符号位64位整型（-9223372036854775808到 9223372036854775807） 还有其他数字类型 序号 类型和描述 1 byte 类似 uint8 2 rune 类似int32 3 uint 32或者64位，根据电脑机器的位数决定 4 int 与uint一样大小 5 uintptr 无符号整型，用于存放一个指针 当超过数据类型所要求的长度时，就会报错：constant xxx overflows byte。 字符下面看一下 byte 和 rune 到底是不是 我们上面描述的，分别 对应 uint8 和 int32，代码如下，我们还是打印出变量的类型 和 值： 12345678910111213package mainimport "fmt"func main() &#123; a := 100 var b byte = 100 var c rune = 200 fmt.Printf("%T %v \n", a , a) fmt.Printf("%T %v \n", b , b) fmt.Printf("%T %v \n", c , c)&#125; 输出结果如下： 123int 100 uint8 100 int32 200 事实证明，我们上面描述的是没有问题的。实际上，我们通常不会将 byte 和 rune 直接赋值为 数字类型，我们通常 赋值为 字符，类似于 Java 中的 char，如下： 1234var e byte = 'a'var f rune = 'A'fmt.Printf("%T %v \n", e , e)fmt.Printf("%T %v \n", f , f) 输出结果如下： 12uint8 97 int32 65 那么，byte 和 rune 有什么区别呢？我们先来看一个例子，假设 将 上面的 f 值 赋值为 汉字 大写的 一，那么 是怎么样的结果呢，然后将 e 的 值也赋值为 “一”，结果是什么呢？ 我们会发现，将 f 赋值为 “一”，输出 为：int32 19968；将 e 赋值为 “一”，会报一个异常：constant 19968 overflows byte，这是因为 byte 即 uint8 的最大值 是 255，而 “一” 对应的值 为 19968，远远大于 uint8 的最大值。 byte 和 rune 的区别如下： byte型：其实就是 uint8 的别名，代表了一个ASCLL 码的一个字符 rune型：其实就是 int32 ，代表了一个UTF-8字符，当需要处理中文等 unicode 字符集时就要用到 rune 类型 字符串字符串在Go语言中是以基本数据类型出现的。 在Go语言中，字符串 即可以单行定义，又可以多行定义，单行定义 不需要阐述，下面我们介绍下定义多行字符串 双引号书写字符串被称为字符串字面量，这种字面量不能跨行； 多行字符串需要使用 “·” 反引号（位于Tab键上面的一个），多用于内嵌源码 和 内嵌数据； 在反引号中的所有代码不会被编译器识别，而只是作为字符串的一部分 当我们想 将 一段代码作为 字符串输出的时候，我们 发现 要输出 就只能 调为一行，但Go语言给我们提供了 多行字符串，如下： 12345678910temp := ` a := 100 var b byte = 256 var c rune = 200 fmt.Printf("%T %v \n", a , a) fmt.Printf("%T %v \n", b , b) fmt.Printf("%T %v \n", c , c)`fmt.Println(temp) 输出结果如下： 1234567a := 100var b byte = 256var c rune = 200fmt.Printf(&quot;%T %v \n&quot;, a , a)fmt.Printf(&quot;%T %v \n&quot;, b , b)fmt.Printf(&quot;%T %v \n&quot;, c , c) 这样就可以讲 我们拷贝的一段代码 原样输出。 以上主要讲解了 基本数据类型，后续 会继续讲解 派生数据类型。]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(3) - 变量与初始化]]></title>
    <url>%2F2018%2F11%2F11%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-3-%E5%8F%98%E9%87%8F%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[变量的概念变量是计算机语言中存储数据的抽象概念，变量通过变量名访问 变量的本质是计算机分配的一小块内存，专门用于存放指定数据，在程序运行过程中该数值可以改变 变量的存储往往具有瞬时性、或者说是临时存储，当程序运行结束，存放该数据的内存就会释放，该变量就会消息 Go语言的变量名由数字、字母、下划线组成，首个字符不能是数字 声明变量声明变量有多种形式： 1.未初始化话的标准格式 1var 变量名 变量类型 2.未初始化的批量格式 不用每行都用 var 声明 12345678910var( a int b string c []float32 d func() bool e struct&#123; x int y string &#125;) 未初始化变量的默认值 整型和浮点型变量默认值为 0 字符串默认值为空字符串 布尔型默认值为false 函数、指针变量默认值为 nil 123456789101112131415161718192021222324package mainimport "fmt"func main() &#123; var( a int b string c []float32 d bool e []int f [3]int h int32 = 100 g func() string ) fmt.Printf("%T , % v \n", a , a) fmt.Printf("%T , % v \n", b , b) fmt.Printf("%T , % v \n", c , c) fmt.Printf("%T , % v \n", d , d) fmt.Printf("%T , % v \n", e , e) fmt.Printf("%T , % v \n", f , f) fmt.Printf("%T , % v \n", h , h) fmt.Printf("%T , % v \n", g , g)&#125; 输出如下： 12345678int , 0 string , []float32 , [] bool , false []int , [] [3]int , [ 0 0 0] int32 , 100 func() string , &lt;nil&gt; 3.初始化变量的标准格式 var 变量名 类型 = 表格式 4.初始化变量的编译器自动推断类型格式 var 变量名 = 表达式 5.初始化变量的简短声明格式（短变量声明格式） 变量名 := 表达式 使用 := 赋值操作符， := 可以高效的创建一个新的变量，称之为初始化声明 声明语句省略了 var 关键字 声明类型将由编译器自动推断 这是声明变量的首选方式，但是它只能被用在函数体内，而不可以用于全局变量的声明与赋值 该变量名必须是没有定义过的变量，若定义过，将发生编译错误 在多个短变量声明和赋值中，至少有一个新声明的变量出现在左侧中，那么即便有其他变量名可能是重复声明的，编译器也不会报错 变量多重赋值 Go语法中，变量初始化和变量赋值是两个不同的概念，Go语言的变量赋值与其他语言一样，但是Go提供了其他程序员期待已久的多重赋值功能，可以实现变量替换，多重赋值让Go语言比其他语言减少了代码量 如想要对 q 和 w 变量的值进行互换： 123456q := 10w := 20fmt.Println(q , " " , w)q, w = w, qfmt.Println(q , " " , w) 得到的结果为： 1210 2020 10 有了变量的多重赋值，就不用像Java中引入第三个变量，来进行两个变量值的互换。 匿名变量 Go语言的函数可以返回多个值，而事实上我们并不是对所有的返回值都用得上，那么就可以使用匿名变量，用“_”下划线替换即可。 匿名变量不占用命名空间，不会分配内存]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(1) - 简介]]></title>
    <url>%2F2018%2F11%2F04%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-1-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Go语言的三个作者是：Robert Giresemer，Rob Pike 和 Ken Thompson Robert 在开发Go之前是Google V8、Chubby和HotSpot JVM的主要贡献者； Rob主要是Unix、UTF-8、plan 9的作者； Ken主要是B语言、C语言的作者、Unix之父。 Go语言的主要发展过程 2007年9月，Rob Pike正式命名为Go 2008年5月，Google全力支持该项目； 2009年11月，Go将代码全部开源，它获得了当年的年度语言； 2012年3月28日，Go发布第一个正式的稳定版 Go语言的特点1.设计Go语言是为了解决当时Google开发者遇到的问题 大量的 C++代码，同时又引入了Java和Python 成千上万行的代码 分布式的编译系统 数百万的服务器 2.Google开发中的痛点： 编译慢 失控的依赖 每个工程师只是用了一个语言里面的一部分 程序难以维护 交叉编译困难 3.如何解决当时的问题和痛点 Go希望成为互联网时代的C语言，因此，Go语言也是足够简单 设计Go语言的目标是为了消除各种缓慢和笨重、改进各种低效和扩展性 4.Go语言的特点 没有继承多态的面向对象 强一致性类型 interface不需要显示声明 没有异常处理 基于首字母的可访问特性 不用的import或者变量引起编译错误 完美而卓越的标准库包 Go语言的优势1.学习曲线容易 Go语言的语法简单，包含了类C语法。所以Go语言容易学习 2.快速的编译时间、开发效率和运行效率高 Go语言拥有接近C的运行效率和接近PHP的开发效率 3.出身名门、血统纯正 Go语言出自Google公司，Google对这个新的宠儿还是很看重的 4.自由高效：组合的思想、无侵入式的接口 Go语言可以说是开发效率和运行效率的完美结合。天生的并发编程支持，Go语言支持所有的编程范式，包含过程式编程、面向对象编程、面向接口编程、函数式编程 5.强大的标准库 包括互联网应用、系统编程和网络编程，它里面的标准库基本上非常稳定了 6.部署方便：二进制文件、拷贝部署 7.简单的并发 Go是一种非常高效的语言，高度支持并发性。Go是为大数据、微服务、并发而生的一种编程语言。 Go作为一门语言致力于使事情简单化，他并未引入很多新概念，而是聚焦于打造一门简单的语言，他使用起来异常快速和简单，其唯一的创新之处是goroutine和通道。Goroutines是Go面向线程的轻量级方法，而通道是goroutines之间通信的优先方式。 创建Goroutines的成本很低，只需要几千个字节的额外内存，正因为如此，才使得同时运行数百个甚至数千个goroutines成为可能。可以借助通道实现Gotoutines之间的通信。Gotoutines以及基于通道的并发性方法使其非常容易使用所有的CPU内核，并处理并发的IO。 8.稳定性 Go拥有强大的编译检查、严格的编码规范和完整的软件生命周期工具，具有很强的稳定性。Go提供了软件生命周期（开发、测试、部署、维护等等）的各个环节的工具，如：go tool、gofmt、go test。 Go语言的核心特性和优势Go主要有静态语言、天生并发、内置GC、安全性高、语法简单、编译快速这几个方面的特性，这些特性决定了Go的三个高富帅特性：运行快、开发快、部署快。 Go语言能开发什么 服务器编程，以前用C或者C++做的事情，用Go来做很合适，例如处理日志、数据打包、虚拟机处理、文件系统等。 分布式系统，数据库代理器等，例如：Etcd。 网络编程，包括Web应用，API应用，下载应用等 数据库操作 开发云平台 欢迎关注我的公众号，第一时间接收文章推送~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>简介</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[object '/usr/local/lib/libdns.so' from /etc/ld.so.preload cannot be preloaded: ignored.]]></title>
    <url>%2F2018%2F10%2F19%2Fobject-usr-local-lib-libdns-so-from-etc-ld-so-preload-cannot-be-preloaded-ignored%2F</url>
    <content type="text"><![CDATA[做了如下操作后： 12rm -rf xxx.jar kill -9 xx 重启Jar包，出现如下错误： 1ld.so: object &apos;/usr/local/lib/libdns.so&apos; from /etc/ld.so.preload cannot be preloaded: ignored. 不晓得什么原因，咨询了下运维，运维给出的方法是清空 /etc/ld.so.preload 文件的内容： 1echo "" &gt; /etc/ld.so.preload 果然，重启成功。 记录一下linux中快速清空文件内容的几种方法： 12345: &gt; filename &gt; filename echo "" &gt; filename echo &gt; filename cat /dev/null &gt; filename]]></content>
      <categories>
        <category>Exception</category>
      </categories>
      <tags>
        <tag>Exception</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysqldump导出完整sql脚本]]></title>
    <url>%2F2018%2F10%2F18%2Fmysqldump%E5%AF%BC%E5%87%BA%E5%AE%8C%E6%95%B4sql%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[#导出某个数据库－－结构+数据 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt db_name |gzip -9 &gt; /db_bakup/db_name.gz #导出某个数据库的表－－结构+数据+函数+存储过程 shell&gt;mysqldump –no-defaults -h192.168.161.124 -uroot -pxxxxxx –opt -R db_name |gzip -9 &gt; /db_backup/db_name.gz #导出多个数据库 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –databases db_name1 db_name2 db_name3 |gzip -9 &gt; /db_backup/mul_db.gz #导出所有的数据库 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –all-databases |gzip -9 &gt; /db_bak/all_db.gz #导出某个数据库的结构 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –no-data db_name|gzip -9 &gt; /db_bak/db_name.strcut.gz #导出某个数据库的数据 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –no-create-info db_name|gzip -9 &gt; /db_bak/db_naem.data.gz #导出某个数据库的某张表 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt db_name tbl_name |gzip -9 &gt; /db_bak/db_name.tal_name.gz # 导出某个数据库的某张表的结构 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –no-data db_name tal_name | gzip -9 &gt; /db_bak/db_name.tal_name.struct.gz #导出某个数据库的某张表的数据 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –no-create-info db_name tbl_name | gzip -9 &gt; /db_bak/db_name.tbl_name.data.gz ##–opt==–add-drop-table + –add-locks + –create-options + –disables-keys + –extended-insert + –lock-tables + –quick + –set+charset ##默认使用–opt，–skip-opt禁用–opt参数]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>mysqldump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot中使用AOP统一处理Web请求日志]]></title>
    <url>%2F2018%2F10%2F12%2FSpring-Boot%E4%B8%AD%E4%BD%BF%E7%94%A8AOP%E7%BB%9F%E4%B8%80%E5%A4%84%E7%90%86Web%E8%AF%B7%E6%B1%82%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[本文由 程序猿DD-翟永超 创作，采用 CC BY 3.0 CN协议 进行许可。 可自由转载、引用，但需署名作者且注明文章出处 AOP为Aspect Oriented Programming的缩写，意为：面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。AOP是Spring框架中的一个重要内容，它通过对既有程序定义一个切入点，然后在其前后切入不同的执行内容，比如常见的有：打开数据库连接/关闭数据库连接、打开事务/关闭事务、记录日志等。基于AOP不会破坏原来程序逻辑，因此它可以很好的对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。 下面主要讲两个内容，一个是如何在Spring Boot中引入Aop功能，二是如何使用Aop做切面去统一处理Web请求的日志。 以下所有操作基于chapter4-2-2工程进行。 准备工作因为需要对web请求做切面来记录日志，所以先引入web模块，并创建一个简单的hello请求的处理。 pom.xml中引入web模块 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 实现一个简单请求处理：通过传入name参数，返回“hello xxx”的功能。 12345678910@RestControllerpublic class HelloController &#123; @RequestMapping(value = "/hello", method = RequestMethod.GET) @ResponseBody public String hello(@RequestParam String name) &#123; return "Hello " + name; &#125;&#125; 下面，我们可以对上面的/hello请求，进行切面日志记录。 引入AOP依赖在Spring Boot中引入AOP就跟引入其他模块一样，非常简单，只需要在pom.xml中加入如下依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 在完成了引入AOP依赖包后，一般来说并不需要去做其他配置。也许在Spring中使用过注解配置方式的人会问是否需要在程序主类中增加@EnableAspectJAutoProxy来启用，实际并不需要。 可以看下面关于AOP的默认配置属性，其中spring.aop.auto属性默认是开启的，也就是说只要引入了AOP依赖后，默认已经增加了@EnableAspectJAutoProxy。 1234# AOPspring.aop.auto=true # Add @EnableAspectJAutoProxy.spring.aop.proxy-target-class=false # Whether subclass-based (CGLIB) proxies are to be created (true) as opposed to standard Java interface-based proxies (false). 而当我们需要使用CGLIB来实现AOP的时候，需要配置spring.aop.proxy-target-class=true，不然默认使用的是标准Java的实现。 实现Web层的日志切面实现AOP的切面主要有以下几个要素： 使用@Aspect注解将一个java类定义为切面类 使用@Pointcut定义一个切入点，可以是一个规则表达式，比如下例中某个package下的所有函数，也可以是一个注解等。 根据需要在切入点不同位置的切入内容 使用@Before在切入点开始处切入内容 使用@After在切入点结尾处切入内容 使用@AfterReturning在切入点return内容之后切入内容（可以用来对处理返回值做一些加工处理） 使用@Around在切入点前后切入内容，并自己控制何时执行切入点自身的内容 使用@AfterThrowing用来处理当切入内容部分抛出异常之后的处理逻辑 12345678910111213141516171819202122232425262728293031@Aspect@Componentpublic class WebLogAspect &#123; private Logger logger = Logger.getLogger(getClass()); @Pointcut("execution(public * com.didispace.web..*.*(..))") public void webLog()&#123;&#125; @Before("webLog()") public void doBefore(JoinPoint joinPoint) throws Throwable &#123; // 接收到请求，记录请求内容 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); // 记录下请求内容 logger.info("URL : " + request.getRequestURL().toString()); logger.info("HTTP_METHOD : " + request.getMethod()); logger.info("IP : " + request.getRemoteAddr()); logger.info("CLASS_METHOD : " + joinPoint.getSignature().getDeclaringTypeName() + "." + joinPoint.getSignature().getName()); logger.info("ARGS : " + Arrays.toString(joinPoint.getArgs())); &#125; @AfterReturning(returning = "ret", pointcut = "webLog()") public void doAfterReturning(Object ret) throws Throwable &#123; // 处理完请求，返回内容 logger.info("RESPONSE : " + ret); &#125;&#125; 可以看上面的例子，通过@Pointcut定义的切入点为com.didispace.web包下的所有函数（对web层所有请求处理做切入点），然后通过@Before实现，对请求内容的日志记录（本文只是说明过程，可以根据需要调整内容），最后通过@AfterReturning记录请求返回的对象。 通过运行程序并访问：http://localhost:8080/hello?name=didi，可以获得下面的日志输出 1234562016-05-19 13:42:13,156 INFO WebLogAspect:41 - URL : http://localhost:8080/hello2016-05-19 13:42:13,156 INFO WebLogAspect:42 - HTTP_METHOD : http://localhost:8080/hello2016-05-19 13:42:13,157 INFO WebLogAspect:43 - IP : 0:0:0:0:0:0:0:12016-05-19 13:42:13,160 INFO WebLogAspect:44 - CLASS_METHOD : com.didispace.web.HelloController.hello2016-05-19 13:42:13,160 INFO WebLogAspect:45 - ARGS : [didi]2016-05-19 13:42:13,170 INFO WebLogAspect:52 - RESPONSE:Hello didi 优化：AOP切面中的同步问题在WebLogAspect切面中，分别通过doBefore和doAfterReturning两个独立函数实现了切点头部和切点返回后执行的内容，若我们想统计请求的处理时间，就需要在doBefore处记录时间，并在doAfterReturning处通过当前时间与开始处记录的时间计算得到请求处理的消耗时间。 那么我们是否可以在WebLogAspect切面中定义一个成员变量来给doBefore和doAfterReturning一起访问呢？是否会有同步问题呢？ 的确，直接在这里定义基本类型会有同步问题，所以我们可以引入ThreadLocal对象，像下面这样进行记录： 1234567891011121314151617181920212223242526@Aspect@Componentpublic class WebLogAspect &#123; private Logger logger = Logger.getLogger(getClass()); ThreadLocal&lt;Long&gt; startTime = new ThreadLocal&lt;&gt;(); @Pointcut("execution(public * com.didispace.web..*.*(..))") public void webLog()&#123;&#125; @Before("webLog()") public void doBefore(JoinPoint joinPoint) throws Throwable &#123; startTime.set(System.currentTimeMillis()); // 省略日志记录内容 &#125; @AfterReturning(returning = "ret", pointcut = "webLog()") public void doAfterReturning(Object ret) throws Throwable &#123; // 处理完请求，返回内容 logger.info("RESPONSE : " + ret); logger.info("SPEND TIME : " + (System.currentTimeMillis() - startTime.get())); &#125;&#125; 优化：AOP切面的优先级由于通过AOP实现，程序得到了很好的解耦，但是也会带来一些问题，比如：我们可能会对Web层做多个切面，校验用户，校验头信息等等，这个时候经常会碰到切面的处理顺序问题。 所以，我们需要定义每个切面的优先级，我们需要@Order(i)注解来标识切面的优先级。i的值越小，优先级越高。假设我们还有一个切面是CheckNameAspect用来校验name必须为didi，我们为其设置@Order(10)，而上文中WebLogAspect设置为@Order(5)，所以WebLogAspect有更高的优先级，这个时候执行顺序是这样的： 在@Before中优先执行@Order(5)的内容，再执行@Order(10)的内容 在@After和@AfterReturning中优先执行@Order(10)的内容，再执行@Order(5)的内容 所以我们可以这样子总结： 在切入点前的操作，按order的值由小到大执行 在切入点后的操作，按order的值由大到小执行 完整代码如下：以下是修改上文后项目中使用的完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import com.alibaba.fastjson.JSON;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;import org.springframework.web.context.request.RequestContextHolder;import org.springframework.web.context.request.ServletRequestAttributes;import javax.servlet.http.HttpServletRequest;import java.util.Enumeration;import java.util.HashMap;import java.util.Map;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * @date: 2018/10/9 19:39 * @description: 统一处理Web请求日志 */@Aspect@Order(5)@Componentpublic class WebLogAspect &#123; private static Logger logger = LoggerFactory.getLogger(WebLogAspect.class); ThreadLocal&lt;Long&gt; startTime = new ThreadLocal&lt;Long&gt;(); private static final String POST = "POST"; private static final String GET = "GET"; @Pointcut("execution(public * com.app.api..*.*(..))") public void webLog()&#123;&#125; @Before("webLog()") public void doBefore(JoinPoint joinPoint) throws Throwable &#123; // 接收到请求，记录请求内容 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); if(attributes == null) &#123; return; &#125; startTime.set(System.currentTimeMillis()); HttpServletRequest request = attributes.getRequest(); String method = request.getMethod(); Object[] args = joinPoint.getArgs(); String queryString = request.getQueryString(); String params = null; //获取请求参数集合并进行遍历拼接 if(args.length&gt;0)&#123; if(POST.equals(method))&#123; Object object = args[0]; //获取所有的请求参数 Map&lt;String, Object&gt; paramMap = new HashMap&lt;String, Object&gt;(); Enumeration&lt;String&gt; paraNames = request.getParameterNames(); for(Enumeration&lt;String&gt; e = paraNames;e.hasMoreElements();)&#123; String thisName = e.nextElement().toString(); String thisValue = request.getParameter(thisName); paramMap.put(thisName, thisValue); &#125; params = JSON.toJSONString(paramMap); &#125;else if(GET.equals(method))&#123; params = queryString; &#125; &#125; CusLogger.info("请求路径:" + request.getServletPath() + ",IP:" + request.getRemoteAddr() +",请求参数:" + params); &#125; @AfterReturning(returning = "ret", pointcut = "webLog()") public void doAfterReturning(Object ret) throws Throwable &#123; if(startTime.get() == null) &#123; return; &#125; // 处理完请求，返回内容 CusLogger.info("返回结果: " + ret + ",花费时间：" + (System.currentTimeMillis() - startTime.get()) + "ms"); &#125;&#125; 本文完整示例Chapter4-2-4]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子更新字段类]]></title>
    <url>%2F2018%2F10%2F07%2F%E5%8E%9F%E5%AD%90%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[原子更新某个类里的某个字段，Atomic包提供了以下3个类进行原子字段更新： AtomicIntegerFieldUpdater： AtomicLongFieldUpdater： AtomicStampedReference：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子更新引用类型]]></title>
    <url>%2F2018%2F10%2F07%2F%E5%8E%9F%E5%AD%90%E6%9B%B4%E6%96%B0%E5%BC%95%E7%94%A8%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Java原子操作的理解 原子更新引用类型包含3个类： AtomicReference：原子更新引用类型 AtomicReferenceFieldUpdater：原子更新引用类型里的字段 AtomicMarkableReference：原子更新带有标记位的引用类型。可以原子更新一个布尔类型的标记位和引用类型。构造方法是 AtomicMarkableReference（V inittialRef，boolean initialMark）。 AtomicReference示例： 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.concurrent.atomic.AtomicReference;/** * @author: xbq * @date: 2018/10/7 16:56 * @description: */public class AtomicReferenceTest &#123; public static AtomicReference&lt;User&gt; atomicUserRef = new AtomicReference&lt;User&gt;(); public static void main(String[] args) &#123; User initUser = new User(); initUser.setName("张山"); initUser.setOld(20); atomicUserRef.set(initUser); atomicUserRef.compareAndSet(initUser, new User("王五" ,21)); System.out.println(atomicUserRef.get().getName()); System.out.println(atomicUserRef.get().getOld()); &#125; /** * 用户实体 */ static class User &#123; private String name; private int old; public User() &#123;&#125; public User(String name, int old) &#123; this.name = name; this.old = old; &#125; // 省略 get set &#125;&#125; 运行结果如下： 12name：王五old：21 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[判断线程池中的线程是否全部执行完毕]]></title>
    <url>%2F2018%2F10%2F07%2F%E5%88%A4%E6%96%AD%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%98%AF%E5%90%A6%E5%85%A8%E9%83%A8%E6%89%A7%E8%A1%8C%E5%AE%8C%E6%AF%95%2F</url>
    <content type="text"><![CDATA[在使用多线程的时候有时候我们会使用 java.util.concurrent.Executors的线程池，当多个线程异步执行的时候，我们往往不好判断是否线程池中所有的子线程都已经执行完毕，但有时候这种判断却很有用，例如我有个方法的功能是往一个文件异步地写入内容，我需要在所有的子线程写入完毕后在文件末尾写“—END—”及关闭文件流等，这个时候我就需要某个标志位可以告诉我是否线程池中所有的子线程都已经执行完毕，我使用这种方式来判断。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MySemaphore &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; final File stream = new File("c:\\temp\\stonefeng\\stream.txt"); final OutputStream os = new FileOutputStream(stream); final OutputStreamWriter writer = new OutputStreamWriter(os); final Semaphore semaphore = new Semaphore(10); ExecutorService exec = Executors.newCachedThreadPool(); final long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) &#123; final int num = i; Runnable task = new Runnable() &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); writer.write(String.valueOf(num)+"\n"); semaphore.release(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; exec.submit(task); &#125; // 这里 exec.shutdown(); while(true)&#123; if(exec.isTerminated())&#123; writer.write("---END---\n"); writer.close(); System.out.println("所有的子线程都结束了！"); break; &#125; Thread.sleep(1000); &#125; // -------- final long end = System.currentTimeMillis(); System.out.println((end-start)/1000); &#125;&#125; 当调用ExecutorService.shutdown方法的时候，线程池不再接收任何新任务，但此时线程池并不会立刻退出，直到添加到线程池中的任务都已经处理完成，才会退出。在调用shutdown方法后我们可以在一个死循环里面用isTerminated方法判断是否线程池中的所有线程已经执行完毕，如果子线程都结束了，我们就可以做关闭流等后续操作了。 判断线程池中的线程是否全部执行完毕的另外一种解决方案则是使用闭锁(CountDownLatch)来实现，CountDownLatch是一种灵活的闭锁实现，它可以使一个或多个线程等待一组事件发生。闭锁状态包括一个计数器，该计数器被初始化为一个正数，表示需要等待的事件数量。countDown方法递减计数器，表示有一个事件已经发生了，而await方法等待计数器达到零，即表示需要等待的事情都已经发生。可以使用闭锁来这样设计程序达到目的： 1234567891011121314151617181920212223242526272829303132333435public class CountDownLatchApproach &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; final int nThreads = 10; // 这里 final CountDownLatch endGate = new CountDownLatch(nThreads); // -------- final File stream = new File("c:\\temp\\stonefeng\\stream.txt"); final OutputStream os = new FileOutputStream(stream); final OutputStreamWriter writer = new OutputStreamWriter(os); ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i &lt; nThreads; i++) &#123; final int num = i; Runnable task = new Runnable() &#123; @Override public void run() &#123; try &#123; writer.write(String.valueOf(num)+"\n"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 这里 endGate.countDown(); // -------- &#125; &#125; &#125;; exec.submit(task); &#125; // 这里 endGate.await(); // -------- writer.write("---END---\n"); writer.close(); &#125;&#125; 这种解决方案虽然可以达到目的但是性能差到没朋友，我更倾向于使用第一种方案。 现在我们有了更优雅的第三种方案，它的执行性能也不错。 1234567891011121314151617181920212223242526272829303132333435363738public class MySemaphore &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; final File stream = new File("c:\\temp\\stonefeng\\stream.txt"); final OutputStream os = new FileOutputStream(stream); final OutputStreamWriter writer = new OutputStreamWriter(os); final Semaphore semaphore = new Semaphore(10); ExecutorService exec = Executors.newCachedThreadPool(); final long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) &#123; final int num = i; Runnable task = new Runnable() &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); writer.write(String.valueOf(num)+"\n"); semaphore.release(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; exec.submit(task); &#125; // 这里 exec.shutdown(); exec.awaitTermination(1, TimeUnit.HOURS); // -------- writer.write("---END---\n"); writer.close(); final long end = System.currentTimeMillis(); System.out.println((end-start)/1000); &#125;&#125; 原文地址： 判断线程池中的线程是否全部执行完毕]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子更新数组]]></title>
    <url>%2F2018%2F10%2F07%2F%E5%8E%9F%E5%AD%90%E6%9B%B4%E6%96%B0%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Java原子操作的理解 通过原子的方式更新数组中的某个元素，Atomic包提供了以下3个类。 AtomicIntegerArray：原子更新整型数组里的元素。 AtomicLongArray ：原子更新长整型数组里的元素。 AtomicReferenceArray：原子更新引用类型数组里的元素。 AtomicIntegerArray类常用方法如下： int addAndGet（int i，int delta）：以原子方式将输入值与数组中索引 i 的元素相加 boolean compareAndSet（int i，int expect，int update）：如果当前值等于预期值，则以原子方式将数组位置i的元素设置成update值. AtomicIntegerArray示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.util.concurrent.atomic.AtomicIntegerArray;/** * @author: xbq * @date: 2018/10/7 10:58 * @description: */public class AtomicIntegerArrayTest &#123; static int[] value = new int[] &#123;1, 2&#125;; static AtomicIntegerArray array = new AtomicIntegerArray(value); public static void main(String[] args) &#123; System.out.println("---------------getAndSet---------------"); // 获取索引为0的值，并且赋值为3 System.out.println(array.getAndSet(0, 3)); System.out.println(array.get(0)); System.out.println("---------------addAndGet---------------"); // 获取索引为1的值 并且 加10 System.out.println(array.addAndGet(1, 10)); System.out.println(array.get(1)); System.out.println("-------------compareAndSet-----------------"); // 获取索引为1的值，和期望的值比较，若相等，就更新为 新值，否则，返回 false System.out.println(array.compareAndSet(1,12,20)); System.out.println(array.get(1)); System.out.println("-------------getAndIncrement-----------------"); // 获取索引为1的值，并且加1，返回 原值 System.out.println(array.getAndIncrement(1)); System.out.println(array.get(1)); System.out.println("-------------incrementAndGet-----------------"); // 获取索引为1的值，并且加1，返回 加1后的值 System.out.println(array.incrementAndGet(1)); System.out.println(array.get(1)); System.out.println("--------------decrementAndGet----------------"); // 获取索引为1的值，并且减1，返回减1 后的值 System.out.println(array.decrementAndGet(1)); System.out.println(array.get(1)); &#125;&#125; 结果如下： 123456789101112131415161718---------------getAndSet---------------13---------------addAndGet---------------1212-------------compareAndSet-----------------true20-------------getAndIncrement-----------------2021-------------incrementAndGet-----------------2222--------------decrementAndGet----------------2121 值得注意的是,数组value通过构造方法传递进去,然后AtomicIntegerArray会将当前数组复制一份,所以当AtomicIntegerArray对内部的数组元素进行修改时,不会影响传入的数组。 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子更新基本类型类]]></title>
    <url>%2F2018%2F10%2F06%2F%E5%8E%9F%E5%AD%90%E6%9B%B4%E6%96%B0%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Java原子操作的理解 当程序更新一个变量时，如果多线程同时更新这个变量。可能得到期望之外的值，比如变量 i=1，A线程更新 i+1，B线程也更新 i+1，最后得到的可能不是3，而是2。这是因为线程A和B在更新变量 i 的时候 拿到的 i 都是 1，这就是线程不安全的操作，通常我们会使用synchronized 来解决这个问题，synchronized 会保证多线程不会同时更新变量 i 。 而Java从JDK 1.5开始提供了java.util.concurrent.atomic包，这个包中的原子操作类提供了一种用法简单、性能高效、线程安全地一个变量的方式。 因为变量的类型有很多种，所以Atomic包里一共提供了13个类，属于4中类型的原子更新方式，分别是原子更新基本类型、原子更新数组、原子更新引用和原子更新属性（字段）。Atomic包里面的类基本都是使用Unsafe实现的包装类。 使用原子的方式更新基本类型，Atomic包提供了以下3个类。 AtomicBoolean：原子更新布尔类型 AtomicInteger：原子更新整型 AtomicLong：原子更新长整型 以上3个类提供的方法几乎一样。此处仅以AtomicInteger为例进行说明，其常用方法如下： int addAndGet（int dalta）：以原子方式见刚输入的数值与实例中的值（AtomicIntger里的value）相加，并返回结果。 boolean compareAndSet（ int expect， int update）：如果输入的数值等于期望值，则以原子方式将该值设置为输入的值。 int getAndIncrement（）：以原子方式将当前值加1，注意，这里返回的是自增前的值。 void lazySet（int newValue）：最终会设置成newValue，使用lazySet设置值后，可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 int getAndSet（ int newValue）：以原子方式设置为newValue的值，并返回旧值。 12345678910111213141516171819202122232425262728293031323334import java.util.concurrent.atomic.AtomicInteger;/** * @author: xbq * @date: 2018/10/6 18:31 * @description: */public class AtomicIntegerTest &#123; static AtomicInteger atomicInteger = new AtomicInteger(2); public static void main(String[] args) &#123; // addAndGet 以原子方式将输入的数值与实例中的数值相加，并返回结果 System.out.println("&gt;&gt;0&gt;&gt;&gt;" + atomicInteger.addAndGet(1)); // 如果输入的数值等于预期值，则以原子方式将该值设置为输入的值 System.out.println("&gt;&gt;1&gt;&gt;&gt;" + atomicInteger.compareAndSet(3, 8080)); // 获取值 System.out.println("&gt;&gt;2&gt;&gt;&gt;" + atomicInteger.get()); // 以原子方式将当前值加1，返回的值 为自增前的值 System.out.println("&gt;&gt;3&gt;&gt;&gt;" + atomicInteger.getAndIncrement()); // 获取值 System.out.println("&gt;&gt;4&gt;&gt;&gt;" + atomicInteger.get()); // 以原子方式将当前值加1，返回的值 为自增后的值 System.out.println("&gt;&gt;5&gt;&gt;&gt;" + atomicInteger.incrementAndGet()); // 获取值 System.out.println("&gt;&gt;6&gt;&gt;&gt;" + atomicInteger.get()); // 以原子方式设置为newValue的值，并返回旧值 System.out.println("&gt;&gt;7&gt;&gt;&gt;" + atomicInteger.getAndSet(86)); // 获取值 System.out.println("&gt;&gt;8&gt;&gt;&gt;" + atomicInteger.get()); // 最终会设置成newValue,使用lazySet设置值后,可能导致其他线程在之后的一小段时间内还是可以读到旧的值 atomicInteger.lazySet(10); System.out.println("&gt;&gt;9&gt;&gt;&gt;" + atomicInteger.get()); &#125;&#125; 输出结果如下： 12345678910&gt;&gt;0&gt;&gt;&gt;3&gt;&gt;1&gt;&gt;&gt;true&gt;&gt;2&gt;&gt;&gt;8080&gt;&gt;3&gt;&gt;&gt;8080&gt;&gt;4&gt;&gt;&gt;8081&gt;&gt;5&gt;&gt;&gt;8082&gt;&gt;6&gt;&gt;&gt;8082&gt;&gt;7&gt;&gt;&gt;8082&gt;&gt;8&gt;&gt;&gt;86&gt;&gt;9&gt;&gt;&gt;10 在JDK1.7中，AtomicInteger的getAndIncrement是这样的 ： 123456789101112public final int getAndIncrement() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return current; &#125;&#125;public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 而在JDK1.8中，是这样的： 123public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125; 可以看出，在JDK1.7中， 依靠的是我们熟悉的CAS算法，首先先循环获取当前值，然后对当前值加1 得到新值，对 当前值和新值进行比较并替换，成功的话返回当前值。而在JDK1.8中，直接使用了Unsafe的getAndAddInt 方法，在JDK1.7的Unsafe中，没有此方法。 JDK1.8中是对CAS算法的增强。 在Java的基本类型中除了Atomic包中提供原子更新的基本类型外，还有char、float和double。那么这些在Atomic包中没有提供原子更新的基本类型怎么保证其原子更新呢? 从AtomicBoolean源码中我们可以得到答案：首先将Boolean转换为整型，然后使用comareAndSwapInt进行CAS，所以原子更新char、float、double同样可以以此实现。 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的13个原子操作类]]></title>
    <url>%2F2018%2F10%2F06%2FJava%E4%B8%AD%E7%9A%8413%E4%B8%AA%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%E7%B1%BB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[使用融云发送消息]]></title>
    <url>%2F2018%2F10%2F01%2F%E4%BD%BF%E7%94%A8%E8%9E%8D%E4%BA%91%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[社交项目中难免会遇到发送消息，客户端发送消息暂时不作介绍，这里讲述的是Java服务端发送消息，其中，消息类型包括：单聊消息、系统消息和自定义消息。 当然，这些内容在融云官网上也有，这里只做记录以及遇到的坑。其中，这里涉及的API主要有：获取融云tokem、注册用户、更新用户、发送单聊消息、给多人发送消息、给所有用户发送消息、检查用户在线状态。 pom依赖1234567891011&lt;dependency&gt; &lt;groupId&gt;cn.rongcloud.im&lt;/groupId&gt; &lt;artifactId&gt;server-sdk-java&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.taimos&lt;/groupId&gt; &lt;artifactId&gt;httputils&lt;/artifactId&gt; &lt;version&gt;1.11&lt;/version&gt;&lt;/dependency&gt; IM接口定义1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import com.app.exception.CusException;import com.app.im.model.MediaMessage;import io.rong.messages.BaseMessage;/** * IM相关操作 */public interface IMService &#123; /** * 注册IM用户 * @param id * @param name * @param portrait * @return * @throws CusException */ boolean addUser(String id,String name,String portrait) throws CusException; /** * 修改IM用户信息 * @param id * @param name * @param portrait * @return * @throws CusException */ boolean updateUser(String id,String name,String portrait)throws CusException; /** * 单聊模块 发送文本、图片、图文消息 * @param fromId 发送人 Id * @param targetIds 接收人 Id * @param msg 消息体 * @param pushContent push 内容, 分为两类 内置消息 Push 、自定义消息 Push * @param pushData iOS 平台为 Push 通知时附加到 payload 中，Android 客户端收到推送消息时对应字段名为 pushData * @return * @throws CusException */ boolean sendPrivateMsg(String fromId, String[] targetIds,BaseMessage msg, String pushContent, String pushData) throws CusException; /** * 系统消息，发送给多人 * @param fromId 发送人 Id * @param targetIds 接收方 Id * @param msg 消息 * @param msg 消息内容 * @param pushContent push 内容, 分为两类 内置消息 Push 、自定义消息 Push * @param pushData iOS 平台为 Push 通知时附加到 payload 中，Android 客户端收到推送消息时对应字段名为 pushData * @return * @throws CusException */ boolean sendSystemMax100Msg(String fromId,String[] targetIds,BaseMessage msg,String pushContent,String pushData)throws CusException; /** * 发送消息给系统所有人 * @param fromId * @param msg * @param pushContent * @param pushData * @return * @throws CusException */ boolean sendSystemBroadcastMsg(String fromId, BaseMessage msg, String pushContent, String pushData)throws CusException; /** * 获取融云token * @param userId * @param name * @param portraitUri * @return * @throws CusException */ String getToken(String userId, String name, String portraitUri) throws CusException; /** * 单聊模块 发送自定义消息 * @param fromId 发送人 Id * @param targetIds 接收人 Id * @param msg 自定义 消息体 * @param pushContent 定义显示的 Push 内容，如果 objectName 为融云内置消息类型时，则发送后用户一定会收到 Push 信息 * @param pushData 针对 iOS 平台为 Push 通知时附加到 payload 中 * @return * @throws CusException */ boolean sendUserDefinedMsg(String fromId, String[] targetIds, MediaMessage msg, String pushContent, String pushData) throws CusException; /** * 检查用户在线状态方法 * 调用频率：每秒钟限 100 次 * @param userId * @return * @throws CusException */ Integer checkOnline(String userId) throws CusException;&#125; IM接口实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import com.app.exception.CusException;import com.app.im.model.MediaMessage;import com.app.im.service.IMService;import com.app.util.CusLogger;import de.taimos.httputils.HTTPRequest;import de.taimos.httputils.WS;import io.rong.RongCloud;import io.rong.messages.BaseMessage;import io.rong.methods.message.system.MsgSystem;import io.rong.models.Result;import io.rong.models.message.BroadcastMessage;import io.rong.models.message.PrivateMessage;import io.rong.models.message.SystemMessage;import io.rong.models.response.ResponseResult;import io.rong.models.response.TokenResult;import io.rong.models.user.UserModel;import org.apache.http.HttpResponse;import org.springframework.stereotype.Component;import javax.annotation.PostConstruct;import java.io.Reader;import java.security.MessageDigest;import java.util.HashMap;import java.util.Map;/** * IM相关操作 实现类 */@Componentpublic class IMServiceImpl implements IMService &#123; // 融云AppKey String appKey = "XXXXXXXXX"; // 融云AppSecret String appSecret = "XXXXXXXX"; RongCloud imClient; @PostConstruct public void init() &#123; imClient = RongCloud.getInstance(appKey, appSecret); &#125; @Override public boolean addUser(String id, String name, String portrait) throws CusException &#123; try &#123; UserModel user = new UserModel(id,name,portrait); TokenResult result = imClient.user.register(user); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("901","同步注册im用户出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public boolean updateUser(String id, String name, String portrait) throws CusException &#123; try &#123; UserModel user = new UserModel(id,name,portrait); Result result = imClient.user.update(user); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("902","同步更新im用户出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public boolean sendPrivateMsg(String fromId, String[] targetIds,BaseMessage msg, String pushContent, String pushData) throws CusException &#123; Reader reader = null ; PrivateMessage privateMessage = new PrivateMessage() .setSenderId(fromId) .setTargetId(targetIds) .setObjectName(msg.getType()) .setContent(msg) .setPushContent(pushContent) .setPushData(pushData) .setVerifyBlacklist(0) .setIsPersisted(0) .setIsCounted(0) .setIsIncludeSender(0); ResponseResult result = null; try &#123; result = imClient.message.msgPrivate.send(privateMessage); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("903","发送系统消息出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public boolean sendSystemMax100Msg(String fromId, String[] targetIds,BaseMessage msg, String pushContent, String pushData) throws CusException &#123; try &#123; MsgSystem system = imClient.message.system; SystemMessage systemMessage = new SystemMessage() .setSenderId(fromId) .setTargetId(targetIds) .setObjectName(msg.getType()) .setContent(msg) .setPushContent(pushData) .setPushData(pushData) .setIsPersisted(0) .setIsCounted(0) .setContentAvailable(0); ResponseResult result = system.send(systemMessage); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("903","发送系统消息出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public boolean sendSystemBroadcastMsg(String fromId,BaseMessage msg, String pushContent, String pushData) throws CusException &#123; try &#123; BroadcastMessage message = new BroadcastMessage() .setSenderId(fromId) .setObjectName(msg.getType()) .setContent(msg) .setPushContent(pushContent) .setPushData(pushData); ResponseResult result = imClient.message.system.broadcast(message); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("903","发送系统消息出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public String getToken(String userId, String name, String portraitUri) throws CusException &#123; try &#123; HTTPRequest req = WS.url("http://api.cn.ronghub.com/user/getToken.json"); Map&lt;String,String&gt; params = new HashMap&lt;String,String&gt;(); params.put("userId",userId); params.put("name",name); params.put("portraitUri",portraitUri); java.util.Random r= new java.util.Random(); String nonce = (r.nextInt(100000)+1)+""; String timestamp = System.currentTimeMillis()+""; String signature =string2Sha1(appSecret+nonce+timestamp); HttpResponse res = req.form(params).header("App-Key",appKey).header("Nonce",nonce).header("Timestamp",timestamp).header("Signature",signature).post(); String body = WS.getResponseAsString(res); JSONObject jo = JSONObject.parseObject(body); if(null!=jo &amp;&amp; jo.getInteger("code")==200)&#123; return jo.getString("token"); &#125;else&#123; new CusException("904","获取IM token 出现问题"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; return null; &#125; @Override public boolean sendUserDefinedMsg(String fromId, String[] targetIds, MediaMessage msg, String pushContent, String pushData) throws CusException &#123; Reader reader = null ; PrivateMessage privateMessage = new PrivateMessage() .setSenderId(fromId) .setTargetId(targetIds) .setObjectName(msg.getType()) .setContent(msg) .setPushContent(pushContent) .setPushData(pushData) .setCount("1") .setVerifyBlacklist(0) .setIsPersisted(0) .setIsCounted(0) .setIsIncludeSender(0); ResponseResult result = null; try &#123; // 发送单聊方法 result = imClient.message.msgPrivate.send(privateMessage); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("903","发送自定义单聊消息出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public Integer checkOnline(String userId) throws CusException &#123; HTTPRequest req = WS.url("http://api.cn.ronghub.com/user/checkOnline.json"); Map&lt;String,String&gt; params = new HashMap&lt;String,String&gt;(); params.put("userId",userId); java.util.Random r = new java.util.Random(); String nonce = (r.nextInt(100000)+1)+""; String timestamp = System.currentTimeMillis()+""; String signature =string2Sha1(appSecret + nonce + timestamp); HttpResponse res = req.timeout(3000).form(params).header("App-Key",appKey).header("Nonce",nonce).header("Timestamp",timestamp).header("Signature",signature).post(); String result = WS.getResponseAsString(res); Map&lt;String,Object&gt; resMap = JSON.parseObject(result, Map.class); Integer code = (Integer) resMap.get("code"); if(code != 200) &#123; CusLogger.error(userId + "调用是否在线接口结果为：" + result); return 2; &#125; String status = (String)resMap.get("status"); Integer resStatus = 0; if("0".equals(status)) &#123; resStatus = 0; &#125; else if("1".equals(status)) &#123; resStatus = 1; &#125; else &#123; resStatus = 2; &#125; return resStatus; &#125; private static String string2Sha1(String str)&#123; if(str==null||str.length()==0)&#123; return null; &#125; char hexDigits[] = &#123;'0','1','2','3','4','5','6','7','8','9', 'a','b','c','d','e','f'&#125;; try &#123; MessageDigest mdTemp = MessageDigest.getInstance("SHA1"); mdTemp.update(str.getBytes("UTF-8")); byte[] md = mdTemp.digest(); int j = md.length; char buf[] = new char[j*2]; int k = 0; for (int i = 0; i &lt; j; i++) &#123; byte byte0 = md[i]; buf[k++] = hexDigits[byte0 &gt;&gt;&gt; 4 &amp; 0xf]; buf[k++] = hexDigits[byte0 &amp; 0xf]; &#125; return new String(buf); &#125; catch (Exception e) &#123; // TODO: handle exception return null; &#125; &#125;&#125; 自定义消息实体1234567891011121314151617181920212223242526272829303132333435363738import io.rong.messages.BaseMessage;import io.rong.util.GsonUtil;public class MediaMessage extends BaseMessage &#123; // 自定义消息标志 private static final transient String TYPE = "go:media"; private String content = ""; // 以下是自定义参数 private String targetId ; private String sendId; private Long sendTime; private Long receptTime; private String userAge; private String userCount; public MediaMessage() &#123; &#125; public MediaMessage(String content) &#123; this.content = content; &#125; // 省略get set …………………… @Override public String getType() &#123; return "rx:media"; &#125; @Override public String toString() &#123; return GsonUtil.toJson(this, MediaMessage.class); &#125;&#125; 上面提到的坑就是发送自定义消息。和客户端定义的是发送JSON格式，那好，我就把定义好的JSON赋值到content中，然而，客户端获取到的值都为空，后面，一同事提示，试一下把定义的消息字段放到自定义实体中，我擦，真的可以了。虽然字段的值可以获取到了，但是 有些值获取到的不对，其中，这些字段的类型都是int 或者 Integer，定义的字段为 age和count，怀疑是 字段类型 或者 字段名 定义的不支持，于是，将 这两种都改掉，类型 改为String，字段 改为userAge和userCount，完美解决。 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Social</category>
      </categories>
      <tags>
        <tag>Social</tag>
        <tag>融云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cannal基本使用]]></title>
    <url>%2F2018%2F09%2F27%2Fcannal%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前提安装完MySQL（我安装的是5.7），安装JDK（canal依赖） 开启MySQL的binlog开启binlog，并且将binlog的格式改为Row，这样就可以获取到CURD的二进制内容。配置/etc/my.cnf，在[mysqld]增加 123log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=1 # 唯一，不能和其他集群MySQL的server_id一样 验证binlog是否开启登录MySQL，使用命令： 1show variables like &apos;log_%&apos;； 若 log_bin显示为 on ，则说明开启。 给canal分配MySQL的账号给canal分配一个MySQL的账号，方便canal偷取MySQL的binlog。 123CREATE USER canal IDENTIFIED BY &apos;canal&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;%&apos;;FLUSH PRIVILEGES; 查看是否给canal账号分配权限 1show grants for &apos;canal&apos; 下载解压canal地址：https://github.com/alibaba/canal/releases ，目前稳定版是 v1.1.0，下载 canal.deployer-1.1.0.tar.gz。解压到 canal目录下（没有该目录 就新建） 注：canal 是纯Java写的，所有需要依赖JDK环境，我这边使用的是：1.8.0_65-b17 123456# 下载wget https://github.com/alibaba/canal/releases/download/canal-1.1.0/canal.deployer-1.1.0.tar.gz# 创建canal目录mkdir canal# 解压tar -zxvf https://github.com/alibaba/canal/releases/download/canal-1.1.0/canal.deployer-1.1.0.tar.gz canal和instance配置文件一个canal里面可能会有多个instance，也就说一个instance可以监控一个mysql实例，多个instance也就可以对应多台服务器的mysql实例。也就是一个canal就可以监控分库分表下的多机器MySQL。 （1）canal.propertiescanal/config 中的canal.properties文件，是全局性的canal服务器配置 ，修改内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109########################################################## common argument ############# ################################################## id唯一，不可与mysql的server_id重复canal.id= 2 canal.ip=canal.port=11111canal.metrics.pull.port=11112canal.zkServers=# flush data to zkcanal.zookeeper.flush.period = 1000canal.withoutNetty = false# flush meta cursor/parse position to filecanal.file.data.dir = $&#123;canal.conf.dir&#125;canal.file.flush.period = 1000## memory store RingBuffer size, should be Math.pow(2,n)canal.instance.memory.buffer.size = 16384## memory store RingBuffer used memory unit size , default 1kbcanal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZEcanal.instance.memory.batch.mode = MEMSIZE## detecing configcanal.instance.detecting.enable = false#canal.instance.detecting.sql = insert into retl.xdual values(1,now()) on duplicate key update x=now()canal.instance.detecting.sql = select 1canal.instance.detecting.interval.time = 3canal.instance.detecting.retry.threshold = 3canal.instance.detecting.heartbeatHaEnable = false# support maximum transaction size, more than the size of the transaction will be cut into multiple transactions deliverycanal.instance.transaction.size = 1024# mysql fallback connected to new master should fallback timescanal.instance.fallbackIntervalInSeconds = 60# network configcanal.instance.network.receiveBufferSize = 16384canal.instance.network.sendBufferSize = 16384canal.instance.network.soTimeout = 30# binlog filter configcanal.instance.filter.druid.ddl = truecanal.instance.filter.query.dcl = falsecanal.instance.filter.query.dml = falsecanal.instance.filter.query.ddl = falsecanal.instance.filter.table.error = falsecanal.instance.filter.rows = falsecanal.instance.filter.transaction.entry = false# binlog format/image checkcanal.instance.binlog.format = ROW,STATEMENT,MIXED canal.instance.binlog.image = FULL,MINIMAL,NOBLOB# binlog ddl isolationcanal.instance.get.ddl.isolation = false# parallel parser configcanal.instance.parser.parallel = true## concurrent thread number, default 60% available processors, suggest not to exceed Runtime.getRuntime().availableProcessors()# parallelThreadSize默认是注释掉的，原值为16，因为canal装在本地VM上，分配了1个CPU，导致报错，改为1canal.instance.parser.parallelThreadSize = 1## disruptor ringbuffer size, must be power of 2canal.instance.parser.parallelBufferSize = 256# table meta tsdb infocanal.instance.tsdb.enable=truecanal.instance.tsdb.dir=$&#123;canal.file.data.dir:../conf&#125;/$&#123;canal.instance.destination:&#125;canal.instance.tsdb.url=jdbc:h2:$&#123;canal.instance.tsdb.dir&#125;/h2;CACHE_SIZE=1000;MODE=MYSQL;canal.instance.tsdb.dbUsername=canalcanal.instance.tsdb.dbPassword=canal# rds oss binlog accountcanal.instance.rds.accesskey =canal.instance.rds.secretkey =########################################################## destinations ############# #################################################canal.destinations= example# conf root dircanal.conf.dir = ../conf# auto scan instance dir add/remove and start/stop instancecanal.auto.scan = truecanal.auto.scan.interval = 5canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml#canal.instance.tsdb.spring.xml=classpath:spring/tsdb/mysql-tsdb.xmlcanal.instance.global.mode = spring canal.instance.global.lazy = false#canal.instance.global.manager.address = 127.0.0.1:1099#canal.instance.global.spring.xml = classpath:spring/memory-instance.xmlcanal.instance.global.spring.xml = classpath:spring/file-instance.xml#canal.instance.global.spring.xml = classpath:spring/default-instance.xml# position info，需要改成自己的数据库信息 canal.instance.master.address = 127.0.0.1:3306 canal.instance.master.journal.name =canal.instance.master.position =canal.instance.master.timestamp =# username/password，需要改成自己的数据库信息 canal.instance.dbUsername = canalcanal.instance.dbPassword = canalcanal.instance.defaultDatabaseName = testcanal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* （2）instance.properties位于 canal/example/instance.properties，是具体的某个instances实例的配置，未涉及到的配置都会从canal.properties上继承，内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243################################################### mysql serverId , v1.0.26+ will autoGen # canal.instance.mysql.slaveId=0# enable gtid use true/falsecanal.instance.gtidon=false# position info address修改为自己的mysql地址canal.instance.master.address=192.168.204.128:3306canal.instance.master.journal.name=canal.instance.master.position=canal.instance.master.timestamp=canal.instance.master.gtid=# rds oss binlogcanal.instance.rds.accesskey=canal.instance.rds.secretkey=canal.instance.rds.instanceId=# table meta tsdb infocanal.instance.tsdb.enable=true#canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb#canal.instance.tsdb.dbUsername=canal#canal.instance.tsdb.dbPassword=canal#canal.instance.standby.address =#canal.instance.standby.journal.name =#canal.instance.standby.position = #canal.instance.standby.timestamp =#canal.instance.standby.gtid=# username/password 修改为在mysql中给canal同步数据的账号 密码canal.instance.dbUsername=canalcanal.instance.dbPassword=canal# 监听的数据库canal.instance.defaultDatabaseName=testcanal.instance.connectionCharset=UTF-8# table regexcanal.instance.filter.regex=.*\\..*# table black regexcanal.instance.filter.black.regex=################################################# 创建test数据库查看MySQL上是否有test数据库，没有则创建 开启canal进入canal/bin，执行：./startup.sh。 使用 ps -ef|grep canal 验证是否开启。 Java client代码创建SpringBoot工程，引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; 创建TestCanal类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package com.xbq.canal.test;import java.awt.Event;import java.net.InetSocketAddress;import java.util.List;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.protocol.CanalEntry.Column;import com.alibaba.otter.canal.protocol.CanalEntry.Entry;import com.alibaba.otter.canal.protocol.CanalEntry.EntryType;import com.alibaba.otter.canal.protocol.CanalEntry.EventType;import com.alibaba.otter.canal.protocol.CanalEntry.Header;import com.alibaba.otter.canal.protocol.CanalEntry.RowChange;import com.alibaba.otter.canal.protocol.Message;import com.google.protobuf.InvalidProtocolBufferException;/** * @Auther: xbq * @Date: 2018/9/11 19:16 * @Description: */public class TestCanal &#123; public static void main(String[] args) throws InterruptedException &#123; // 第一步：与canal进行连接 CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress("192.168.204.128", 11111), "example", "", ""); connector.connect(); // 第二步：开启订阅 connector.subscribe(); // 第三步：循环订阅 while (true) &#123; try &#123; // 每次读取 1000 条 Message message = connector.getWithoutAck(1000); long batchID = message.getId(); int size = message.getEntries().size(); if (batchID == -1 || size == 0) &#123; System.out.println("当前暂时没有数据"); Thread.sleep(1000); &#125; else &#123; System.out.println("-------------------------- 有数据啦 -----------------------"); PrintEntry(message.getEntries()); &#125; // position id ack （方便处理下一条） connector.ack(batchID); &#125; catch (Exception e) &#123; // TODO: handle exception &#125; finally &#123; Thread.sleep(1000); &#125; &#125; &#125; /** * 获取每条打印的记录 * @param entrys */ public static void PrintEntry(List&lt;Entry&gt; entrys) &#123; for (Entry entry : entrys) &#123; // 第一步：拆解entry 实体 Header header = entry.getHeader(); EntryType entryType = entry.getEntryType(); // 第二步： 如果当前是RowData，那就是我需要的数据 if (entryType == EntryType.ROWDATA) &#123; String tableName = header.getTableName(); String schemaName = header.getSchemaName(); RowChange rowChange = null; try &#123; rowChange = RowChange.parseFrom(entry.getStoreValue()); &#125; catch (InvalidProtocolBufferException e) &#123; e.printStackTrace(); &#125; EventType eventType = rowChange.getEventType(); System.out.println(String.format("当前正在操作 %s.%s， Action= %s", schemaName, tableName, eventType)); // 如果是‘查询’ 或者 是 ‘DDL’ 操作，那么sql直接打出来 if (eventType == EventType.QUERY || rowChange.getIsDdl()) &#123; System.out.println("rowchange sql -----&gt;" + rowChange.getSql()); return; &#125; // 第三步：追踪到 columns 级别 rowChange.getRowDatasList().forEach((rowData) -&gt; &#123; // 获取更新之前的column情况 List&lt;Column&gt; beforeColumns = rowData.getBeforeColumnsList(); // 获取更新之后的 column 情况 List&lt;Column&gt; afterColumns = rowData.getAfterColumnsList(); // 当前执行的是 删除操作 if (eventType == EventType.DELETE) &#123; PrintColumn(beforeColumns); &#125; // 当前执行的是 插入操作 if (eventType == EventType.INSERT) &#123; PrintColumn(afterColumns); &#125; // 当前执行的是 更新操作 if (eventType == EventType.UPDATE) &#123; PrintColumn(afterColumns); &#125; &#125;); &#125; &#125; &#125; /** * 每个row上面的每一个column 的更改情况 * @param columns */ public static void PrintColumn(List&lt;Column&gt; columns) &#123; columns.forEach((column) -&gt; &#123; String columnName = column.getName(); String columnValue = column.getValue(); String columnType = column.getMysqlType(); // 判断 该字段是否更新 boolean isUpdated = column.getUpdated(); System.out.println(String.format("columnName=%s, columnValue=%s, columnType=%s, isUpdated=%s", columnName, columnValue, columnType, isUpdated)); &#125;); &#125;&#125; 运行此类。在MySQL test数据库中创建student表，对其进行增删改，可以发现控制台上打印：有数据库啦…… 参考[缓存一致性和跨服务器查询的数据异构解决方案canal 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>canal</category>
      </categories>
      <tags>
        <tag>canal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社交项目 -- 汇总]]></title>
    <url>%2F2018%2F09%2F23%2F%E7%A4%BE%E4%BA%A4%E9%A1%B9%E7%9B%AE-%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[App视频邀请]]></title>
    <url>%2F2018%2F09%2F23%2FApp%E8%A7%86%E9%A2%91%E9%82%80%E8%AF%B7%2F</url>
    <content type="text"><![CDATA[需求项目中有这么一个需求： 当用户余额不足，1分钟后，机器人进行视频邀请，当用户点击接听时，则提示用户充值；当用户点击拒绝，3分钟后，再对该用户使用机器人进行视频邀请，当用户点击接听时，则提示用户充值；当用户点击拒绝，10分钟后，再次对该用户使用机器人进行视频邀请，当用户点击接听时，则提示用户充值；当用户点击拒绝，3次诱导充值结束。 当用户余额充足，1分钟后，推荐真实用户对该用户进行视频邀请，若该用户接听，则对真实用户发送视频邀请；当用户挂断，3分钟后，继续推荐真实用户进行视频邀请，若该用户接听，则对真实用户发送视频邀请，当用户挂断，10分钟后，继续推荐真实用户进行视频邀请。 当用户余额不够时，继续走余额不够的逻辑。 分析这个需求，难点无非就是三次时间间隔，开始考虑的是使用消息队列RocketMQ，但用RocketMQ有点大材小用的意思。后面考虑用Redis，如果Redis有对过期时间的监听，那岂不美哉，我擦，谷歌了一发，还真TM有。于是，就研究了一发，也是比较简单。 Redis对过期时间的监听是这样的：使用String类型，设置Key-Value，对该Key设置过期时间，当时间过期后，触发某个事件，这就是所谓的 对过期事件的监听。过期事件是通过Redis的发布订阅功能来进行分发。 事件类型对于每个修改数据库的操作，键空间通知都会发送两种不同类型的事件消息：keyspace 和 keyevent。以 keyspace 为前缀的频道被称为键空间通知（key-space notification）， 而以 keyevent 为前缀的频道则被称为键事件通知（key-event notification）。 事件是用 keyspace@DB:KeyPattern 或者 keyevent@DB:OpsType 的格式来发布消息的。DB表示在第几个库；KeyPattern则是表示需要监控的键模式（可以用通配符，如：key:）；OpsType则表示操作类型。因此，如果想要订阅特殊的Key上的事件，应该是订阅keyspace。比如说，对 0 号数据库的键 mykey 执行 DEL 命令时， 系统将分发两条消息， 相当于执行以下两个 PUBLISH 命令：PUBLISH keyspace@0:sampleKey delPUBLISH keyevent@0:del sampleKey订阅第一个频道 keyspace@0:mykey 可以接收 0 号数据库中所有修改键 mykey 的事件，而订阅第二个频道 keyevent@0:del 则可以接收 0 号数据库中所有执行 del 命令的键。 开启配置键空间通知通常是不启用的，因为这个过程会产生额外消耗。所以在使用该特性之前，请确认一定是要用这个特性的，然后修改配置文件，或使用config配置。相关配置项如下： 输入的参数中至少要有一个 K 或者 E ， 否则的话， 不管其余的参数是什么， 都不会有任何通知被分发。上表中斜体的部分为通用的操作或者事件，而黑体则表示特定数据类型的操作。在redis的配置文件redis.conf中修改 notify-keyspace-events “Kx”，注意：这个双引号是一定要的，否则配置不成功，启动也不报错。例如，“Kx”表示想监控某个Key的失效事件。也可以在命令行通过config配置：CONFIG set notify-keyspace-events Ex （但非持久化）。 实现步骤 修改redis.conf配置文件中的 notify-keyspace-events “Kx”，redis默认是关闭的 对SpringBoot整合 Redis的发布订阅，指定监听类和监听类型 代码示例pom依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; redis工具类（部分）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.ValueOperations;import org.springframework.stereotype.Component;import java.util.concurrent.TimeUnit;/** * redis缓存客户端 */@Componentpublic class RedisCacheUtils&lt;T&gt; &#123; @Autowired private RedisTemplate&lt;String, T&gt; redisTemplate; /** * 写入单个对象到缓存(可以设置有效时间) * @param key * @param value * @param expireTime 有效时间 单位秒 * @return */ public boolean set(final String key, T value, Long expireTime) &#123; boolean result = false; try &#123; ValueOperations&lt;String, T&gt; operations = redisTemplate.opsForValue(); operations.set(key, value); redisTemplate.expire(key, expireTime, TimeUnit.SECONDS); result = true; &#125; catch (Exception e) &#123; throw e; &#125; return result; &#125; /** * 自增 * @param key * @param by * @param seconds * @return */ public Long incr(final String key, final long by,final long seconds) &#123; Long count = redisTemplate.opsForValue().increment(key, by); redisTemplate.expire(key, seconds, TimeUnit.SECONDS); return count; &#125;&#125; 监听配置1234567891011121314151617181920212223242526272829303132import com.app.common.constants.SystemConstant;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.listener.ChannelTopic;import org.springframework.data.redis.listener.RedisMessageListenerContainer;@Configurationpublic class RedisLinstenerConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; @Bean public ConsumerRedisListener consumerRedis() &#123; return new ConsumerRedisListener(); &#125; @Bean public ChannelTopic topic() &#123; return new ChannelTopic("__keyevent@0__:expired"); &#125; @Bean public RedisMessageListenerContainer redisMessageListenerContainer() &#123; RedisMessageListenerContainer container = new RedisMessageListenerContainer(); container.setConnectionFactory(redisConnectionFactory); container.addMessageListener(consumerRedis(),topic()); return container; &#125;&#125; redis监听器： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import com.app.cache.RedisCacheUtils;import org.apache.commons.lang3.StringUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.connection.Message;import org.springframework.data.redis.connection.MessageListener;import org.springframework.data.redis.core.StringRedisTemplate;public class ConsumerRedisListener implements MessageListener &#123; @Autowired private StringRedisTemplate stringRedisTemplate; @Autowired private RedisCacheUtils redisCacheUtils; @Override public void onMessage(Message message, byte[] pattern) &#123; doBusiness(message); &#125; /** * 打印 message body 内容 * @param message */ public void doBusiness(Message message) &#123; Object value = stringRedisTemplate.getValueSerializer().deserialize(message.getBody()); byte[] body = message.getBody(); byte[] channel = message.getChannel(); String topic = new String(channel); String itemValue = new String(body); System.out.println("itemValue-----------------------" + itemValue); // 如果key中包含^，则说明是 视频邀请的 if(itemValue.contains("^")) &#123; String[] keyArr = itemValue.split("\\^"); String userId = keyArr[1]; // 防止重复消费，设置一个过期时间 Long num = redisCacheUtils.incr(userId + "_incr", 1L, 60L); if(StringUtils.isBlank(userId)) &#123; return; &#125; if(num == 1)&#123; // 处理逻辑，给App推送消息，调起视频呼叫 //………… &#125; &#125; &#125;&#125; 看了上面的代码可能有点懵，貌似和上述所说的时间间隔并没有什么瓜葛，然而并不是。首先，当用户当日首次登陆App时，客户端用调用一个接口，表示用户进入App，我会在接口中判断用户是不是当日首次登陆，如果是，则使用”video” + “^” + 用户的ID + “^” + 180 作为一个Key，value无所谓，并对该key设置60秒的过期时间，当该key过期，则会进入到redis监听中，并对客户端推送消息，其中，消息体中包含一个关键字段，此关键字段就是下次需要间隔多久来发起视频邀请，即之前过期Key后面跟随的180，当客户端点击挂断，调用挂断接口时，就将此字段传过来，然后 使用”video” + “^” + 用户的ID + “^” + 600 作为一个Key，并对该key设置180秒的过期时间，后面逻辑同理…… 然而，因为项目是分布式项目，会部署多个节点，这样就存在重复订阅，因为这一部分数据老大要求不能存到数据库，所以使用了redis 的incr来记录进入过期监听器的次数，并设置过期时间为60秒，这样 多个节点即使重复订阅，也会只有一个订阅者可以处理逻辑，即对客户端推送消息，这里的推送消息使用的是融云的IM，后续对该IM进行分析。 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Social</category>
      </categories>
      <tags>
        <tag>Social</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发 -- JMM]]></title>
    <url>%2F2018%2F07%2F29%2FJava%E5%B9%B6%E5%8F%91-JMM%2F</url>
    <content type="text"><![CDATA[Java内存模型的基础 文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Java内存模型的理解 并发编程模型的两个关键问题 线程之间如何通信 线程之间如何同步 上面所说的线程指的是并发执行的活动实体。 线程之间的通信机制有两种：共享内存和消息传递 在共享内存的并发模型中，线程之间共享程序的公共状态，通过写-读内存中的公共状态进行隐式通信 在消息传递的并发模型中，线程之间没有公共状态，必须通过发送消息来显式进行通信 同步无非就是控制不同线程的执行顺序。在共享内存的并发模型中，同步是显式进行的，程序员必须显式的指定某个方法或者某段代码需要在线程之间互斥执行。在消息传递的并发模型中，同步是隐式的，因为消息的发送必须在消息的接收之间。 Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行的，整个通信过程对程序员完全透明。 Java内存模型的抽象结构在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享。局部变量和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不会受内存模型的影响。 Java线程之间的通信由Java内存模型（JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。 抽象来说，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程以读/写共享变量的副本，本地内存只是JMM的一个抽象概念，并不真实的存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 JMM的抽象示意图如下： 从上图来看，如果线程A和线程Ｂ要通信的话，需要经过两个步骤： 线程Ａ把本地内存Ａ中更新过的共享变量刷新到主内存中去 线程Ｂ到主内存中去读取线程Ａ之前已更新过的共享变量 JMM通过控制主内存和每个线程的本地内存之间的交互，来为Java程序实现内存可见性的保证。 JMM和JVM的区别JMM中的主内存、工作内存与JVM中的Java堆、栈、方法区等并不是同一个层次的内存划分，这两者基本上是没有关系的，如果两者一定要勉强对应起来，那从变量、主内存、工作内存的定义来看，主内存主要对应于Java堆中的对象实例数据部分，而工作内存则对应于虚拟机栈中的部分区域。从更底层次上说，主内存就直接对应于物理硬件的内存，而为了获取更好的运行速度，虚拟机（甚至是硬件系统本身的优化措施）可能会让工作内存优先存储于寄存器和高速缓存中，因为程序运行时主要访问读写的是工作内存。 欢迎关注我的公众号哦~搜索公众号：翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
        <tag>JMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL -- 行锁]]></title>
    <url>%2F2018%2F07%2F24%2FMySQL-%E8%A1%8C%E9%94%81%2F</url>
    <content type="text"><![CDATA[一、行锁概念及特点1.概念：给单独的一行记录加锁，主要应用于innodb表存储引擎 2.特点：在innodb存储引擎中应用比较多，支持事务、开销大、加锁慢；会出现死锁；锁的粒度小，并发情况下，产生锁等待的概率比较低，所以支持的并发数比较高。 二、数据库事务1.概念：事务是一系列操作组成的工作单元，该工作单元内的操作是不可分割的，也就是说要么全部都执行，要么全部不执行。 2.特性：ACID 原子性：事务是最小的工作单元，不可分割，要么都做，要么都不做 一致性：事务执行前和执行后的数据要保证正确性，数据完整性没有被破坏。 隔离性：在并发事务执行的时候，一个事务对其他事务不会产生影响。 持久性：一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的 三、多个事务并发执行 问题及解决方案1.问题 丢失更新：在没有事务隔离的情况下，两个事务同时更新一条数据，后一个事务 会 覆盖前面事务的更新，导致前面的事务丢失更新。 脏读：事务A先更新数据，但是没有提交，事务B读到了事务A没有提交的数据。 不可重复读：事务A中，先读到一条数据，事务A还没有结束，此时，事务B对该条数据进行了修改操作，事务A又读到了这条数据，事务A两次读到的数据不同。 幻读：事务A先读到一批数据，假设读到10条，事务B插入了一条数据，此时，事务A又读这一批数据，发现多了一条，好像幻觉一样。 注：不可重复读的重点是修改，同样的条件，你读取过的数据，再次读取出来发现值不一样。 幻读的重点在于新增或者删除，同样的条件，第 1 次和第 2 次读出来的记录数不一样。 2.解决方案–数据库隔离机制 未提交读（read uncommitted）：这是数据库最低的隔离级别，允许一个事务读另一个事务未提交的数据。 解决了丢失更新，但是会出现脏读、不可重复读、幻读。 提交读（read committed）：一个事务更新的数据 在提交之后 才可以被另一个事务读取，即一个事务不可以读取到另一个事务未提交的数据。 解决了丢失更新和脏读，但是会出现不可重复读和幻读。 可重复读（repeatale read）：这是数据库默认的事务隔离级别，保证一个事务在相同条件下前后两次读取的数据是一致的。 解决了丢失更新、脏读和不可重复读，但是会出现幻读。 序列化（serializable）：这是数据库最高的隔离级别。事务串行执行，不会交叉执行。 解决了所有的问题。 注：乐观所可以解决幻读。 四、行锁的特性查看mysql事务隔离级别：show variables like ‘tx_iso%’; 前提：set autocommit=0; // 设置自动提交事务为手动提交 123456789/* 行锁案例*/create table lock_two( id int, col int)engine=innodb;insert into lock_two(id,col) values (1,1);insert into lock_two(id,col) values (2,2);insert into lock_two(id,col) values (3,3); 1.在session1中执行update : update lock_two set col=11 where id=1; （1）分别在session1和session2中查询lock_two，看id为1的记录的col是否修改了。 发现session1 的记录修改了，session2中的记录没有被修改。 （2）在session1中执行commite后，然后再在session2中查询： 发现session2中的表数据改变了。 2.在session1中执行update：update lock_two set col=11 where id=1，不执行commit; 在session2中执行uodate ：update lock_two set col=11 where id=1，不执行commit; 发现session2中的update发生阻塞，并且超过一段时间报错。 3.在session1中执行update：update lock_two set col=22 where id = 2; 不执行commit 在session2中执行另一条update：update lock_two set col=33 where id = 3; 此时，session2中的update发生阻塞，在没发生错误的情况下，session1执行commit，session2中的update会马上执行。 4.在lock_two中创建索引， 12create index idx_id on lock_two(id);create index idx_col on lock_two(col); 然后重复第3步， 发现session2可以更新，不会产生阻塞。因为用上了索引，相当于行锁。 结论：如果没有用上索引，行锁变成表锁 五、手动锁一行记录格式12begin;select * from lock_two where id=2 for update; 在session1中执行上面语句，在ssesion2中可以查看，但是不可以修改 sesion1中的for update 的记录。 当session1中执行commit后，seesion2中的update立刻执行。 六、间隙锁1.定义：在范围查找的情况下， innodb会给范围条件中的数据加上锁，无论数据是否是否真实存在。 2.例子： 在session1中update：update lock_two set col=666 where id&gt;2 and id&lt;8; 1) 在session2中执行insert：insert into lock_two values(9,99); 插入执行成功！ 2) 在session2中执行insert：insert into lock_two values(7,77); 插入阻塞，一段时间后报错！ 执行select：select * from lock_two where id=4; 查询成功！ 建议：在innodb中，因为有间隙锁的存在，最好在where中少使用这种范围查找。 七、查看行锁的信息show status like ‘innodb_row_lock%’; 说明： Innodb_row_lock_current_waits ：当前正在等待的数量 Innodb_row_lock_time: 从启动到现在锁定的总时长，单位是ms Innodb_row_lock_time_avg :锁等待的平均时长 Innodb_row_lock_time_max：等待锁时间最长的一个时间 Innodb_row_lock_waits：总共的等待次数]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>行锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL -- 表锁]]></title>
    <url>%2F2018%2F07%2F24%2FMySQL-%E8%A1%A8%E9%94%81%2F</url>
    <content type="text"><![CDATA[前言数据库的锁主要用来保证数据的一致性的。MyISAM存储引擎只支持表锁，InnoDB存储引擎既支持行锁，也支持表锁，但默认情况下是采用行锁。 一、锁分类1.按照对数据操作的类型分：读锁，写锁 读锁：也称为共享锁。 针对同一资源，多个并发读操作可以并行执行，并且互不影响，但是不能写 写锁：也称排它锁。当前线程写数据的时候，会阻塞其它线程来读取数据 或者 写数据 注：读锁和写锁都是阻塞锁。 2.按照数据操作的粒度：表锁，行锁，页锁 表锁：开销小，加锁快，主要在myisam存储引擎中出现。特点：锁住整个表，开销小，加锁快，无死锁情况， 锁的粒度大，在并发情况下，产生锁等待的概率比较高，所以说，支持的并发数比较低，一般用于查找 行锁：开销大，加锁慢，锁定单独的某个表中的某一行记录，主要用于innodb存储引擎。特点：有死锁情况，锁定粒度最小，发生锁冲突的概率最低，支持的并发数也最高 页锁：开销和加锁时间界于表锁和行锁之间。会出现死锁，锁定粒度界于表锁和行锁之间，并发度一般 二、加锁与解锁1.手动增加表锁lock table 表名 [read|write]，表名 [read|write]… 2.解锁unlock tables; 3.查看哪些表被锁show open tables; 三、表锁案例1.读锁12345678create table lock_one( id int primary key auto_increment, col int)engine=myisam;insert into lock_one(col) values (1);insert into lock_one(col) values (2);insert into lock_one(col) values (3); 下面我们模拟两个用户，即两个线程连接数据库，开启两个xsheel窗口，连接到mysql： 1) 在会话1中对lock_one表增加读锁 lock table lock_one read; 2) 在当前会话（会话1）中是否可以select该表呢，也就是说对 lock_one增加了读锁后，在当前会话中是否可以读呢？ select * from lock_one; 答案是可以的。 3) 在另一个会话中（会话2）是否可以select该表呢？ 答案也是可以的。 4) 那么在会话1中是否可以查询其他表呢？ 例如，查询 users表：select * from users; 我们发现是不可以查询其他表的，这是因为当前会话已经对lock_one表加上了锁，即当前线程锁住了lock_one表，只可以操作lock_one表，就不可以查询其他的表。 5) 问题来了，会话2是否可以查询其他表呢？ select * from users; 我们发现是可以的。因为会话2和会话1是没有关系的，会话2查询会话1锁住的表都可以，查询没有锁住的 肯定是可以的。 6) 在会话1中是否可以更新（增删改）锁住的lock_one表呢？ update lock_one set col=66 where id=1; 发现是不可以的，因为我们对 lock_one表加了 读锁，所以是不可以 进行写操作的。 7) 在会话2中是否可以更新（增删改）会话1中锁住的lock_one表呢？ 我们发现是没有执行结果的，也就是说 正在等待更新，在阻塞等待中。因为我们在会话1中对lock_one中增加了读锁，其他人只有读的操作，没有写的操作。 8) 在会话1中 对lock_one进行解锁时，会话2中的更新（增删改）操作 就会立即执行。 2.写锁1) 在会话1中对lock_one表增加写锁 lock table lock_one write; 2) 在会话1中查询该表 select * from lock_one; 我们发现是可以的。 3) 在会话2中查询该表 我们发现是没有执行结果的，也就是说 处于阻塞状态。因为写锁是排它锁，其他用户线程不可以读取当前锁住的表，只有解锁之后 其他用户线程才可以执行select 4) 在会话1中对lock_one进行写锁后，会话1会否可以查询其他表呢？ select * from users; 我们发现是不可以的。道理和读锁的时候一样，当前会话已经对lock_one表加上了锁，即当前线程锁住了lock_one表，只可以操作lock_one表，就不可以查询其他的表。 5) 那么在会话2中是否可以查询其他表呢？ 答案肯定是可以的。因为之和锁的表有关系，和其他表没有任何关系。 6) 在会话1中是否可以进行写（增删改）操作呢？ 答案一定是可以的。因为会话1对lock_one表进行了写锁操作，也就是只可以写。 7) 在会话2中是否可以进行写（增删改）操作呢？ 我们发现是不可以的。因为写锁是排它锁，也就是只可以当前线程操作锁住的表，其他用户线程需要等到解锁之后才可以操作该表。 3.总结1) 甲对表A加了读锁 甲对表A可以执行读（查询）操作，但不可以执行写（增删改）操作 甲对其他表不可以执行读写（增删改查）操作 乙对表A可以执行读（查询）操作，但不可以执行写（增删改）操作 乙对其他表可以执行读写（增删改查）操作 2) 甲对表A加了写锁 甲对表A可以执行读写（增删改查）操作 甲对其他表不可以执行读写（增删改查）操作 乙对表A不可以执行读写（增删改查）操作 乙对其他表可以执行读写（增删改查）操作 四、MyISAM存储引擎中锁特点1.执行select语句的时候，会自动给涉及的表加上表锁，在执行更新操作时，会自动给表加上写锁 2.MyISAM存储引擎比较适合作为以查询为主的表存储引擎，不适合写为主的表存储引擎，因为加写锁后，是锁住整个表，其他用户线程不能做任何操作， 这样会导致大量用户线程阻塞的情况。 五、表锁的状态查询查询指令：show status like ‘table_lock%’; 说明： Table_locks_immediate：表示可以立即获取锁的查询次数，每获取一次锁就增加1 Table_locks_waited：锁等待的次数（重要，如果这个值的大，则说明锁表的次数多，需要优化，通过 show open tables，查看哪些表锁了，然后分析为什么会锁）。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>表锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发 -- Fork/Join框架]]></title>
    <url>%2F2018%2F07%2F23%2FJava%E5%B9%B6%E5%8F%91-Fork-Join%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Fork/Join框架的理解。 什么是Fork/Join框架Fork/Join框架是Java7提供了的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。 它的主要思想是：分而治之。 工作窃取算法工作窃取（work-stealing）算法是指某个线程从其他队列里窃取任务来执行。 什么需要使用工作窃取算法呢？假如我们需要做一个比较大的任务，我们可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，于是把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应，比如A线程负责处理A队列里的任务。但是有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。而在这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。 工作窃取算法的优点是充分利用线程进行并行计算，并减少了线程间的竞争，其缺点是在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且消耗了更多的系统资源，比如创建多个线程和多个双端队列。 介绍Fork/Join框架的设计分为两步： 第一步分割任务。首先我们需要有一个fork类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。 第二步执行任务并合并结果。分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。 Fork/Join使用两个类来完成以上两件事情： ForkJoinTask：我们要使用ForkJoin框架，必须首先创建一个ForkJoin任务。它提供在任务中执行fork()和join()操作的机制，通常情况下我们不需要直接继承ForkJoinTask类，而只需要继承它的子类，Fork/Join框架提供了以下两个子类： RecursiveAction：用于没有返回结果的任务。RecursiveTask ：用于有返回结果的任务。 ForkJoinPool ：ForkJoinTask需要通过ForkJoinPool来执行，任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进入队列的头部。当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务。 使用使用Fork/Join框架计算：1+2+3+……+100000000. 使用Fork／Join框架首先要考虑到的是如何分割任务，如果我们希望每个子任务最多执行10000个数的相加，那么我们设置分割的阈值是10000，由于是100000000个数字相加，所以会不停的分割，第一次先分割成两部分，即1~50000000 和 50000001~100000000，第二次继续将 1~50000000 分割成 1~25000000 和 25000001~50000000 ，将50000001~100000000 分割成 50000001~75000000 和 75000001~100000000 ……，一直分割，直到 开始和 结束的的差小于等于10000。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.util.concurrent.*;public class CountTask extends RecursiveTask&lt;Long&gt; &#123; /** * 阀值 */ private static final long THRESHOLD = 10000; // 开始数 private long start; // 结束数 private long end; public CountTask(long start, long end) &#123; this.start = start; this.end = end; &#125; @Override protected Long compute() &#123; long sum = 0; // 如果足够小就计算 boolean canComplute = (end - start) &lt;= THRESHOLD; if(canComplute) &#123; for(long i = start; i &lt;= end; i++) &#123; sum += i; &#125; &#125; else &#123; // 否则，对大任务进行拆分 // 对半分 long middle = (start + end) /2; // 进行递归 CountTask left = new CountTask(start, middle); CountTask right = new CountTask(middle + 1, end); // 执行子任务 invokeAll(left, right); // 获取结果 long lResult = left.join(); long rRight = right.join(); sum = lResult + rRight; &#125; return sum; &#125; public static void main(String[] args) &#123; long s = System.currentTimeMillis(); ForkJoinPool pool = ForkJoinPool.commonPool(); CountTask countTask = new CountTask(1,100000000); // 参数为起始值与结束值 Future&lt;Long&gt; result = pool.submit(countTask); // 如果任务完成 if(!((ForkJoinTask&lt;Long&gt;) result).isCompletedAbnormally()) &#123; try &#123; // 获取任务结果 System.out.println("fork/join计算为：" + result.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("fork/join计算花费时间：" + (System.currentTimeMillis() - s) + "ms"); s = System.currentTimeMillis(); long sum = 0; for(int i = 1; i &lt;= 100000000 ; i++) &#123; sum += i; &#125; System.out.println("计算结果：" + sum); System.out.println("普通计算花费时间：" + (System.currentTimeMillis() - s) + "ms"); &#125;&#125; fork/join计算为：5000000050000000fork/join计算花费时间：55ms计算结果：5000000050000000普通计算花费时间：53ms 三种提交任务到ForkJoinPool的方法： execute():异步执行，没有任何返回 。 invoke():同步执行，调用之后需要等待任务完成，才能执行后面的代码 。 submit():异步执行，当调用get方法的时候会阻塞，完成时返回一个future对象用于检查状态以及运行结果。 1ForkJoinPool commonPool = ForkJoinPool.commonPool(); 为公共池提供一个引用，使用预定义的公共池减少了资源消耗，因为这阻碍了每个任务创建一个单独的线程池。 检查任务运行的状态 无论以什么方式结束任务，isDone() 方法返回true； 如果完成任务过程中没有被取消或者发生异常，isCompletedNormally() 方法返回true； 如果任务被取消， isCancelled() 方法返回true； 如果任务被取消或者遇到异常，isCompletedAbnormally() 方法返回true 异常处理ForkJoinTask在执行的时候可能会抛出异常，但是我们没办法在主线程里直接捕获异常，所以ForkJoinTask提供了isCompletedAbnormally()方法来检查任务是否已经抛出异常或已经被取消了，并且可以通过ForkJoinTask的getException方法获取异常。使用如下代码： 123if(task.isCompletedAbnormally()) &#123; System.out.println(task.getException());&#125; getException方法返回Throwable对象，如果任务被取消了则返回CancellationException。如果任务没有完成或者没有抛出异常则返回null。 与ExecutorService 的区别Fork/Join采用“工作窃取模式”，当执行新的任务时他可以将其拆分成更小的任务执行，并将小任务加到线程队列中，然后再从一个随即线程中偷一个并把它加入自己的队列中。 就比如两个CPU上有不同的任务，这时候A已经执行完，B还有任务等待执行，这时候A就会将B队尾的任务偷过来，加入自己的队列中，对于传统的线程，ForkJoin更有效的利用的CPU资源！ 实现原理ForkJoinPool由ForkJoinTask数组和ForkJoinWorkerThread数组组成，ForkJoinTask数组负责存放程序提交给ForkJoinPool的任务，而ForkJoinWorkerThread数组负责执行这些任务。 ForkJoinTask的fork方法实现原理。当我们调用ForkJoinTask的fork方法时，程序会调用ForkJoinWorkerThread的pushTask方法异步的执行这个任务，然后立即返回结果。代码如下： 12345public final ForkJoinTask fork() &#123; ((ForkJoinWorkerThread) Thread.currentThread()) .pushTask(this); return this;&#125; pushTask方法把当前任务存放在ForkJoinTask 数组queue里。然后再调用ForkJoinPool的signalWork()方法唤醒或创建一个工作线程来执行任务。代码如下： 123456789101112final void pushTask(ForkJoinTask t) &#123; ForkJoinTask[] q; int s, m; if ((q = queue) != null) &#123; // ignore if queue removed long u = (((s = queueTop) &amp; (m = q.length - 1)) &lt;&lt; ASHIFT) + ABASE; UNSAFE.putOrderedObject(q, u, t); queueTop = s + 1; // or use putOrderedInt if ((s -= queueBase) &lt;= 2) pool.signalWork(); else if (s == m) growQueue(); &#125; &#125; 首先，它调用了doJoin()方法，通过doJoin()方法得到当前任务的状态来判断返回什么结果，任务状态有四种：已完成（NORMAL），被取消（CANCELLED），信号（SIGNAL）和出现异常（EXCEPTIONAL）。 如果任务状态是已完成，则直接返回任务结果。 如果任务状态是被取消，则直接抛出CancellationException。 如果任务状态是抛出异常，则直接抛出对应的异常。 让我们再来分析下doJoin()方法的实现代码： 12345678910111213141516171819202122private int doJoin() &#123; Thread t; ForkJoinWorkerThread w; int s; boolean completed; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) &#123; if ((s = status) &lt; 0) return s; if ((w = (ForkJoinWorkerThread)t).unpushTask(this)) &#123; try &#123; completed = exec(); &#125; catch (Throwable rex) &#123; return setExceptionalCompletion(rex); &#125; if (completed) return setCompletion(NORMAL); &#125; return w.joinTask(this); &#125; else return externalAwaitDone(); &#125; 在doJoin()方法里，首先通过查看任务的状态，看任务是否已经执行完了，如果执行完了，则直接返回任务状态，如果没有执行完，则从任务数组里取出任务并执行。如果任务顺利执行完成了，则设置任务状态为NORMAL，如果出现异常，则纪录异常，并将任务状态设置为EXCEPTIONAL。]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
        <tag>Fork/Join</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux上监控Tomcat down掉后自动重启Tomcat]]></title>
    <url>%2F2018%2F07%2F20%2FLinux%E4%B8%8A%E7%9B%91%E6%8E%A7Tomcat-down%E6%8E%89%E5%90%8E%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AFTomcat%2F</url>
    <content type="text"><![CDATA[tomcat运行一段时间后，凌晨无缘无故挂掉，看了tomcat日志、项目日志、系统日志，没有发现错误。于是想到写一个shell脚本，每隔2分钟监控一次tomcat 的状态，若挂掉，则重新启动。解决方案参考网络，同时修改成符合自己的脚本。 使用环境 操作系统：CentOS 7 JDK版本：1.8.0_161-b12 64位 Tomcat版本：8.5.29 编写脚本在win下新建：monitor.sh，内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/bin/sh# function:自动监控tomcat进程，挂了就执行重启操作# DEFINE# 环境变量export JAVA_HOME=/usr/local/jdk1.8.0_161export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar# 获取tomcat PID，要加上grep java，否则会打印多个进程IDTomcatID=$(ps -ef | grep java |grep tomcat |grep -w 'apache-tomcat-8.5.29_2'|grep -v 'grep'|awk '&#123;print $2&#125;')# tomcat_startupStartTomcat=`nohup /usr/local/apache-tomcat-8.5.29_2/bin/startup.sh &amp;`TomcatCache=usr/local/apache-tomcat-8.5.29_2/work# 定义要监控的页面地址WebUrl=http://127.0.0.1:8080/xxxxx/monitor# 日志输出GetPageInfo=/dev/nullTomcatMonitorLog=/tmp/TomcatMonitor.logMonitor()&#123; echo "[info]开始监控tomcat...[$(date +'%F %H:%M:%S')],进程ID:$TomcatID." if [ "$TomcatID" != "" ];then echo "[info]tomcat进程ID为:$TomcatID." # 获取返回状态码 TomcatServiceCode=$(curl -s -o $GetPageInfo -m 10 --connect-timeout 10 $WebUrl -w %&#123;http_code&#125;) if [ $TomcatServiceCode -eq 200 ];then echo "[info]返回码为$TomcatServiceCode,tomcat处于running状态." else echo "[error]访问出错，状态码为$TomcatServiceCode,错误日志已输出到$GetPageInfo" echo "[error]开始重启tomca,kill前进程id为:$TomcatID" kill -9 $TomcatID # 杀掉原tomcat进程 sleep 3 rm -rf $TomcatCache # 清理tomcat缓存 $StartTomcat echo "[info] tomcat启动成功." fi else echo "[error]进程不存在!tomcat自动重启..." echo "[info]$StartTomcat,请稍候......" rm -rf $TomcatCache $StartTomcat fi echo "------------------------------"&#125;Monitor&gt;&gt;$TomcatMonitorLog 在bin目录下执行：./monitor.sh（点不出的话先授权：sudo chmod a+x monitor.sh），发现报错，错误如下： 1-bash: ./monitor.sh: /bin/sh^M: bad interpreter: No such file or directory 原因是因为在windows下编辑的，然后上传到linux系统里执行。.sh文件的格式为dos格式，而linux只能执行格式为unix格式的脚本 ，解决方法： 1234# 没有 需要先安装yum install dos2unix# 修改格式dos2unix monitor.sh 添加任务12345678910111213141516# 没有安装 需要先安装crontabyum install crontab # 启动/sbin/service crond start# 停止/sbin/service crond stop# 重启服务/sbin/service crond restart# 重新加载/sbin/service crond reload# crontab其他命令要把cron设为在开机的时候自动启动，在 /etc/rc.d/rc.local 脚本中加入 /sbin/service crond start 即可查看当前用户的crontab，输入 crontab -l编辑crontab，输入 crontab -e删除crontab，输入 crontab -r crontab -e ，在文档末尾处添加（每隔2分钟执行一次）： 1*/2 * * * * /usr/local/hrfiles/apache-tomcat-8.5.29_2/bin/monitor.sh 执行日志可查看： 1tail -f /tmp/TomcatMonitor.log]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[com.netflix.zuul.exception.ZuulException: Forwarding error]]></title>
    <url>%2F2018%2F07%2F19%2Fcom-netflix-zuul-exception-ZuulException-Forwarding-error%2F</url>
    <content type="text"><![CDATA[场景项目基于SpringCloud，生产者做了负载，消费者（此处用的ribbon + restTemplcate）做了降级(hystrix)，当一个消费者关掉后，访问网关（zuul），出现如下错误： 123456789101112com.netflix.zuul.exception.ZuulException: Forwarding error at org.springframework.cloud.netflix.zuul.filters.route.RibbonRoutingFilter.handleException(RibbonRoutingFilter.java:183) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at org.springframework.cloud.netflix.zuul.filters.route.RibbonRoutingFilter.forward(RibbonRoutingFilter.java:158) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at org.springframework.cloud.netflix.zuul.filters.route.RibbonRoutingFilter.run(RibbonRoutingFilter.java:106) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at com.netflix.zuul.ZuulFilter.runFilter(ZuulFilter.java:112) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.FilterProcessor.processZuulFilter(FilterProcessor.java:193) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.FilterProcessor.runFilters(FilterProcessor.java:157) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.FilterProcessor.route(FilterProcessor.java:118) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.ZuulRunner.route(ZuulRunner.java:96) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.http.ZuulServlet.route(ZuulServlet.java:116) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.http.ZuulServlet.service(ZuulServlet.java:81) ~[zuul-core-1.3.0.jar:1.3.0]................... 省略一大片.............. 解决方法在网关服务中的配置文件中配置如下信息： 123456789101112131415161718192021222324252627zuul: okhttp: enabled: true # 使用okhttp方式请求，正常来说okhttp比较速度快一点 semaphore: max-semaphores: 500 # 并发处理数，值越大越好，但到到达一个临界点之后，就不会提高响应速度了 host: socket-timeout-millis: 30000 # socket超时时间，如果使用service-id方式是不用配置的 connect-timeout-millis: 30000 # 连接时间semaphores max-total-connections: 5000 # 最大连接数，值越大越好，但到到达一个临界点之后，就不会提高响应速度了 max-per-route-connections: 5 # 每个router最大连接数，降低请求时间，越小越好，但达到一定层级就没用了hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 30000 # Hystrix超时时间 strategy: THREADribbon: ReadTimeout: 20000 # 处理时间 ConnectTimeout: 20000 # 连接时间 MaxAutoRetries: 0 #最大自动重试次数 MaxAutoRetriesNextServer: 1 # 换实例重试次数 MaxTotalHttpConnections: 2000 # 最大http连接数，越大越好，但到到达一个临界点之后，就不会提高响应速度了 MaxConnectionsPerHost: 1000 # 每个host连接数 注：解决方案来源于网上，具体链接没有找到。]]></content>
      <categories>
        <category>Exception</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
        <tag>zuul</tag>
        <tag>Exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git基本使用]]></title>
    <url>%2F2018%2F07%2F19%2Fgit%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前提配置邮箱和用户名 12$ git config --global user.name "你的名字"$ git config --global user.email "你的邮箱" 上传项目到github123456$ git init $ git add . (添加到暂存区里面去)$ git commit -m 'first commit' (把文件提交到仓库)$ git remote add origin https://github.com/xxx/xxx.git (关联到远程库)$ git pull --rebase origin master (获取远程库与本地同步合并（如果远程库不为空必须做这一步，否则后面的提交会失败）)$ git push -u origin master (把本地库的内容推送到远程) 1.初始化1$ git init 2.提交123456789101112131415$ git pull origin master$ git touch init.txt //如果已经存在更改的文件,则这一步不是必须的$ git add .$ git commit -m "first commit"$ git push -u origin master # 第一次提交需要加 -u$ git push origin master # 后面提交就不需要-u了$ git push -u origin master -f # 强制覆盖解决(慎用)# 同步冲突如果您舍弃线上的文件，则在推送时选择强制推送，强制推送需要执行下面的命令$ git push origin master -f如果您选择保留线上的文件,则需要先执行git pull origin master然后才可以推送,如果发生冲突，则需要先解决冲突 更新项目1234$ git pull --rebase origin master(会覆盖本地文件，慎用)或者：$ git fetch --all$ git reset --hard origin/master 异常1.当出现no changes added to commit时如何正确使用git提交命令对于这个问题，最好的解决方法就是按如下步骤： 到解决方案根目录下：git add . (“.”是必须要的) git commit -m “some word” git push -u origin master]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发-volitile的应用]]></title>
    <url>%2F2018%2F07%2F16%2FJava%E5%B9%B6%E5%8F%91-volitile%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[volatile的定义 volatile是Java语言中的类型修饰符，它是被设计用来修饰被不同线程访问和修改的变量。是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”，可见性是指当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。volatile比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。 实现原理 使用volatile修饰的变量在汇编阶段，会多出一条lock前缀指令，它在多核处理器下会引发两件事： 将当前处理器缓存行的数据写回到系统内存 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 通常处理器和内存之间都有几级缓存来提高处理速度，处理器先将内存中的数据读取到内部缓存后再进行操作，但是对于缓存写回内存的时机则无法得知，因此在一个处理器里修改的变量值，不一定能及时写回缓存，这种变量修改对其他处理器变得“不可见”了。但是，使用volatile修饰的变量，在写操作的时候，会强制将这个变量所在缓存行的数据写回到内存中，但即使写回到内存，其他处理器也有可能使用内部的缓存数据，从而导致变量不一致，所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期，如果过期，就会将该缓存行设置成无效状态，下次要使用就会重新从内存中读取。]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
        <tag>volitile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The last packet successfully received from the server was 1,480 milliseconds ago.]]></title>
    <url>%2F2018%2F07%2F16%2FThe-last-packet-successfully-received-from-the-server-was-1-480-milliseconds-ago%2F</url>
    <content type="text"><![CDATA[场景：一个上传接口，需要上传几十M的文件，文件中包含10几W的数据，然后对10+W的数据进行同步批量插入，每次批量插入1W。最后返回结果。 项目上线一段时间后，上传接口出现问题，数据库用的MySQL5.7.21，报了如下错误： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172732018-07-16 01:30:03.497 ERROR com.alibaba.druid.pool.DruidDataSource Line:1594 - discard connectioncom.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failureThe last packet successfully received from the server was 1,480 milliseconds ago. The last packet sent successfully to the server was 1,480 milliseconds ago. at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1121) at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3603) at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3492) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4043) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2503) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2664) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2815) at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2155) at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379) at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:498) at org.apache.ibatis.executor.statement.PreparedStatementHandler.query(PreparedStatementHandler.java:63) at org.apache.ibatis.executor.statement.RoutingStatementHandler.query(RoutingStatementHandler.java:79) at org.apache.ibatis.executor.SimpleExecutor.doQuery(SimpleExecutor.java:63) at org.apache.ibatis.executor.BaseExecutor.queryFromDatabase(BaseExecutor.java:324) at org.apache.ibatis.executor.BaseExecutor.query(BaseExecutor.java:156) at org.apache.ibatis.executor.CachingExecutor.query(CachingExecutor.java:109) at com.github.pagehelper.PageInterceptor.intercept(PageInterceptor.java:137) at org.apache.ibatis.plugin.Plugin.invoke(Plugin.java:61) at com.sun.proxy.$Proxy63.query(Unknown Source) at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:148) at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:141) at sun.reflect.GeneratedMethodAccessor102.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.mybatis.spring.SqlSessionTemplate$SqlSessionInterceptor.invoke(SqlSessionTemplate.java:434) at com.sun.proxy.$Proxy25.selectList(Unknown Source) at org.mybatis.spring.SqlSessionTemplate.selectList(SqlSessionTemplate.java:231) at org.apache.ibatis.binding.MapperMethod.executeForMany(MapperMethod.java:137) at org.apache.ibatis.binding.MapperMethod.execute(MapperMethod.java:75) at org.apache.ibatis.binding.MapperProxy.invoke(MapperProxy.java:53) at com.sun.proxy.$Proxy51.selectReportListByReportDate(Unknown Source) at com.rxwx.service.report.impl.PrcmtReportServiceImpl.generateProcurementReport(PrcmtReportServiceImpl.java:97) at com.rxwx.service.report.impl.PrcmtReportServiceImpl$$FastClassBySpringCGLIB$$f325780.invoke(&lt;generated&gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:99) at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:280) at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:655) at com.rxwx.service.report.impl.PrcmtReportServiceImpl$$EnhancerBySpringCGLIB$$8c585594.generateProcurementReport(&lt;generated&gt;) at com.rxwx.service.report.impl.PrcmtReportServiceImpl$$FastClassBySpringCGLIB$$f325780.invoke(&lt;generated&gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:99) at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:280) at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:655) at com.rxwx.service.report.impl.PrcmtReportServiceImpl$$EnhancerBySpringCGLIB$$9a830f32.generateProcurementReport(&lt;generated&gt;) at com.rxwx.task.sup.GenarateReportJob.selfGenReportTask1(GenarateReportJob.java:37) at com.rxwx.task.sup.GenarateReportJob$$FastClassBySpringCGLIB$$64b31449.invoke(&lt;generated&gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:115) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost. at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3052) at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3503) ... 58 common frames omitted 首先百度了一发，按照网上的解决方案，修改了my.cnf中的超时时间： 12wait_timeout=31536000interactive_timeout=31536000 将原来默认的8小时改为1年。这里单位是秒。 修改了德鲁伊的：testOnBorrow = true。 跑了一天，MMP，凌晨又出现这个错误了。 一个电话把睡梦中的我吵醒，扰我周末扰我梦，没得办法，重启下服务，重启下MySQL，先上传吧，睡觉。 周一到公司谷歌了一发，修改my.cnf的net_read_timeout和net_write_timeout 参数，将这两个参数调大，改为6000。由于项目中的数据存在无效数据，所以删除了10天之前的无效数据，减少了表中的数量。明天看效果。 show global variables like “%timeout%”; connect_timeout 连接超时 mysql连接共有6次握手，3次TCP协议这个跟connect_timeout参数没有关系，另外3次跟connect_timeout参数有关系，该参数主要是为了防止网络不佳时应用重连导致连接数涨太快，一般默认即可。 delayed_insert_timeout 这是为MyISAM INSERT DELAY设计的超时参数，在INSERT DELAY中止前等待INSERT语句的时间 interactive_timeout 服务器关闭交互式连接前等待活动的秒数。交互式客户端定义为在mysql_real_connect()中使用CLIENT_INTERACTIVE选项的客户端。参数默认值：28800秒（8小时） lock_wait_timeout 锁等待超时时间 net_read_timeout / net_write_timeout 这个参数只对TCP/IP链接有效，分别是数据库等待接收客户端发送网络包和发送网络包给客户端的超时时间，这是在Activity状态下的线程才有效的参数 slave_net_timeout 这是Slave判断主机是否挂掉的超时设置，在设定时间内依然没有获取到Master的回应就人为Master挂掉了 wait_timeout 服务器关闭非交互连接之前等待活动的秒数。 效果很明显。今天没有出现问题。]]></content>
      <categories>
        <category>Exception</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis问题汇总]]></title>
    <url>%2F2018%2F06%2F10%2FRedis%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1. 什么是RedisRedis是由意大利人Salvatore Sanfilippo（网名：antirez）开发的一款内存高速缓存数据库。Redis全称为：Remote Dictionary Server（远程数据服务），该软件使用C语言编写，Redis是一个key-value存储系统，它支持丰富的数据类型，如：string、list、set、zset(sorted set)、hash。 2. Redis特点 以内存作为数据存储介质，读写数据的效率极高，远远超过数据库。以设置和获取一个256字节字符串为例，它的读取速度可高达110000次/s，写速度高达81000次/s。 支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 支持多种数据类型String、List、Hash、Set、zset 支持数据的备份，即master-slave模式的数据备份 3.Redis有哪些数据类型，以及每种数据类型使用的场景Redis支持多种数据类型，有String、List、Hash、Set、zset String(字符串)String类型是二进制安全的，意思是Redis的String可以包含任何数据，比如图片或者序列化的对象等。一个Redis中字符串的value最多可以是512M。一般做一些复杂的计数功能的缓存。 List(列表)List是按照插入顺序排序的字符串链表。 从元素插入和删除的效率视角来看，如果我们是在链表的两头插入或删除元素，这将会是非常高效的操作，即使链表中已经存储了百万条记录，该操作也可以在常量时间内完成。然而需要说明的是，如果元素插入或删除操作是作用于链表中间，那将会是非常低效的 。 Redis链表经常会被用于消息队列的服务，以完成多程序之间的消息交换。假设一个应用程序正在执行LPUSH操作向链表中添加新的元素，我们通常将这样的程序称之为”生产者(Producer)”，而另外一个应用程序正在执行RPOP操作从链表中取出元素，我们称这样的程序为”消费者(Consumer)”。如果此时，消费者程序在取出消息元素后立刻崩溃，由于该消息已经被取出且没有被正常处理，那么我们就可以认为该消息已经丢失，由此可能会导致业务数据丢失，或业务状态的不一致等现象的发生。然而通过使用RPOPLPUSH命令，消费者程序在从主消息队列中取出消息之后再将其插入到备份队列中，直到消费者程序完成正常的处理逻辑后再将该消息从备份队列中删除。同时我们还可以提供一个守护进程，当发现备份队列中的消息过期时，可以重新将其再放回到主消息队列中，以便其它的消费者程序继续处理。 可以利用 lrange 命令，做基于 Redis 的分页功能，性能极佳，用户体验好 Hash(字典)Hash是一个健值对集合，是一个String类型的key与value的映射表，特别适合用于存储对象。 可用于存储、读取、修改用户属性， Hash 结构可以使你像在数据库中 Update 一个属性一样只修改某一项属性值。 Set(集合)Set 是一个集合，集合的概念就是一堆不重复值的组合。利用 Redis 提供的 Set 数据结构，可以存储一些集合性的数据。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1) 。Redis 非常人性化的为集合提供了求交集、并集、差集等操作，那么就可以非常方便的实现如共同关注、共同喜好、二度好友等功能 。也可以做全局去重的功能。 zset(Sorted Set,有序集合)和Sets相比，Sorted Sets是将 Set 中的元素增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列。可以做排行榜应用，取 TOP N 操作。Sorted Set 可以用来做延时任务。最后一个应用就是可以做范围查找。 4.为什么要用Redis性能和并发。 性能：将一些耗时比较久，且结果不经常变动的SQL，放到Redis中，这样，请求直接从缓存中读取，使得能够迅速响应。 并发：大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用Redis做一个缓冲操作，让请求先访问到redis，而不是直接访问数据库 。 5.Redis有什么缺点 缓存和数据库双写一致性问题 缓存击穿问题 缓存雪崩问题 缓存的并发竞争问题 6.Redis为什么这么快 纯内存操作 单线程模型，避免了频繁的上下文切换 采用非阻塞I/O多路复用机制 什么是非阻塞I/O呢？ 阻塞与非阻塞可以简单理解为调用一个IO操作能不能立即得到返回应答，如果不能立即获得返回，需要等待，那就阻塞了；否则就可以理解为非阻塞。 什么是I/O多路复用机制呢？ 单个线程，通过记录跟踪每个I/O流(sock)的状态，来同时管理多个I/O流 。 I/O多路复用的优势并不是对于单个连接能处理的更快，而是在于可以在单个线程/进程中处理更多的连接。与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。 7.Redis的过期策略Redis采用的过期策略是：定期删除+惰性删除策略。 定期删除，Redis 默认每隔100ms 检查，是否有过期的 Key，有过期 Key 则删除。 需要说明的是，Redis 不是每隔100ms 将所有的 Key 检查一次，而是随机抽取进行检查(如果每隔 100ms，全部 Key 进行检查，Redis 岂不是卡死)。 123定期删除可以通过：第一、配置redis.conf 的hz选项，默认为10 （即1秒执行10次，100ms一次，值越大说明刷新频率越快，最Redis性能损耗也越大，建议不要超过100） 第二、配置redis.conf的maxmemory最大值，当已用内存超过maxmemory限定时，就会触发主动清理策略 因此，如果只采用定期删除策略，会导致很多 Key 到时间没有删除。于是，惰性删除派上用场。 也就是说在你获取某个 Key 的时候，Redis 会检查一下，这个 Key 如果设置了过期时间，如果过期了，此时就会删除。 过期策略可以参考：Redis数据过期策略详解 8.Redis内存淘汰机制在 redis.conf 中有一行配置 1# maxmemory-policy volatile-lru Redis内存淘汰策略有（触发该策略的机制是 当内存不足以容纳新写入数据时）： noeviction：谁也不删，直接在写操作时返回错误 。应该没人用吧。 allkeys-lru：在键空间中，移除最近最少使用的 Key。推荐使用，目前项目在用这种。 allkeys-random：在键空间中，随机移除某个 Key。应该也没人用吧，你不删最少使用 Key，去随机删。 volatile-lru：在设置了过期时间的键空间中，移除最近最少使用的 Key。这种情况一般是把 Redis 既当缓存，又做持久化存储的时候才用。不推荐。 volatile-random：在设置了过期时间的键空间中，随机移除某个 Key。依然不推荐。 volatile-ttl：在设置了过期时间的键空间中，有更早过期时间的 Key 优先移除。不推荐。 9.Redis和数据库双写一致性问题首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。 10.如何应对缓存穿透、缓存雪崩、缓存击穿问题缓存穿透：黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。 解决方法： 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层数据库的查询压力（推荐） 如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟（推荐，简单暴力） 利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试 采用异步更新策略，无论 Key 是否取到值，都直接返回。Value 值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。 缓存雪崩：当缓存服务器重启或者大量缓存集中在某一个时间段失效，来了一批请求，请求全部到DB，DB瞬时压力过重雪崩。 解决方法： 给缓存的失效时间加上一个随机值，避免集体失效 使用互斥锁，但是该方案吞吐量明显下降了 双缓存，我们有两个缓存，缓存 A 和缓存 B。缓存 A 的失效时间为 20 分钟，缓存 B 不设失效时间。自己做缓存预热操作。然后细分以下几个小点：从缓存 A 读数据库，有则直接返回；A 没有数据，直接从 B 读数据，直接返回，并且异步启动一个更新线程，更新线程同时更新缓存 A 和缓存 B。（推荐） 缓存击穿：对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一key缓存，前者则是很多key。 缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 解决方法： 使用互斥锁 “提前”使用互斥锁 “永不过期” 参考： ​ 缓存穿透，缓存击穿，缓存雪崩解决方案分析 ​ Redis架构之防雪崩设计：网站不宕机背后的兵法 11.若Redis中有1亿个key，其中有10w是以某个固定的已知前缀开头，怎么将它们全部找出来使用keys指令可以扫出指定模式的key列表。如果这个redis正在给线上的业务提供服务，keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 12.怎么使用Redis做异步队列一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 如果不用sleep，list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 使用pub/sub主题订阅者模式，可以实现1:N的消息队列。 也就是生产一次消费多次。但是使用pub/sub是有缺点的，在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。 redis如何实现延时队列：使用sortedset，拿时间戳作为score，消息内容作为key，调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 13.怎么用Redis实现分布式锁主要是使用了redis 的setnx命令，缓存了锁，reids缓存的key是锁的key,所有的共享, value是锁的到期时间(注意:这里把过期时间放在value了,没有时间上设置其超时时间)。 1.通过setnx尝试设置某个key的值,成功(当前没有这个锁)则返回,成功获得锁 2.锁已经存在则获取锁的到期时间,和当前时间比较,超时的话,则设置新的值 实现方法可以参考：Redis分布式锁实现 14.参考为什么分布式一定要有Redis? 天下无难试之Redis面试刁难大全]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java注解(三) - 注解的使用]]></title>
    <url>%2F2018%2F03%2F10%2FJava%E6%B3%A8%E8%A7%A3-%E4%B8%89-%E6%B3%A8%E8%A7%A3%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[学会了如何定义自定义注解，那还要会用起来才行。 其实自定义注解使用也非常简单，像我们上篇文章定义的一个 Sweet 注解。 123public @interface Sweet &#123; String sweetLevel();&#125; 要使用它只需要像下面这样就可以了。 123456public class SweetDemo &#123; @Sweet (sweetLevel="Level.05") public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 但是有时候注解会有些特殊用法，我们需要了解一下。 第一种情况：如果没有任何注解属性，那么可以省略注解的中括号。 在上面的例子中，如果 @Sweet 注解没有任何属性。 12public @interface Sweet &#123;&#125; 那么我们使用的时候就可以直接写上直接名称，不需要中括号。 123456public class SweetDemo &#123; @Sweet public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 第二种情况：注解属性有默认值，可以不进行赋值操作。 在上面的 SweetDemo 中会发现我们在使用 @Sweet 注解的时候，手动给 sweetLevel 属性赋值。如果没有赋值，那么会报错。 但是如果在 @Sweet 注解声明的时候，给 sweetLevel 属性定义一个默认值，那么在使用的时候就不需要赋值操作了。 例如我们重新定义 Sweet，让你有一个「Level.03」的默认值。 123public @interface Sweet &#123; String sweetLevel();&#125; 那么在使用的时候就可以直接这样使用： 123456public class SweetDemo &#123; @Sweet public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 这个时候，sweetLevel 属性就是默认值：Level.03。 第三种情况：注解内有且仅有一个名字为 value 的属性时，应用这个注解时可以直接接属性值填写到括号内。 例如上面的 @Sweet 注解改写成这样： 123public @interface Sweet &#123; String value();&#125; 那么在使用的时候，我们本来应该这样用： 123456public class SweetDemo &#123; @Sweet(value = "Level.03") public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 但是我们可以忽略 value 属性名的声明，直接这么用： 123456public class SweetDemo &#123; @Sweet("Level.03") public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 总结下面就来总结一下，其实自定义注解使用不复杂，但有下面三种情况比较特殊： 注解没有任何注解属性，那么可以省略注解的中括号。 注解的注解属性有默认值，可以不进行赋值操作。 注解内有且仅有一个名字为 value 的属性时，应用这个注解时可以直接接属性值填写到括号内。 原文地址注解的那些事儿（三）| 注解的使用]]></content>
      <categories>
        <category>Annotation</category>
      </categories>
      <tags>
        <tag>Java注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java注解(二) - 如何自定义注解]]></title>
    <url>%2F2018%2F03%2F10%2FJava%E6%B3%A8%E8%A7%A3-%E4%BA%8C-%E5%A6%82%E4%BD%95%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[要自定义注解，首先需要了解一个注解的构成部分。 一个注解大致可以分为三个部分：注解体、元注解、注解属性。 在这三个主要组成部分中，注解体指定了注解的名字，而元注解则标记了该注解的使用场景、留存时间等信息，而注解属性则指明该注解拥有的属性。 注解体注解体是最简单的一个组成部分，只需要实例中一样有样学样即可。与接口的声明唯一的不同是在 interface 关键字前多了一个 @ 符号。 1234//声明了一个名为sweet的注解体@Retention(RetentionPolicy.RUNTIME) public @interface sweet&#123;&#125; 元注解元注解（meta-annotation）本身也是一个注解，用来标记普通注解的存留时间、使用场景、继承属性、文档生成信息。 元注解是一个特殊的注解，它是 Java 源码中就自带的注解。在Java 中只有四个元注解，它们分别是：@Target、@Retention、@Documented、@Inherited。 @Target注解Target 注解限定了该注解的使用场景 它有下面这些取值： ElementType.ANNOTATION_TYPE 可以给一个注解进行注解 ElementType.CONSTRUCTOR 可以给构造方法进行注解 ElementType.FIELD 可以给属性进行注解 ElementType.LOCAL_VARIABLE 可以给局部变量进行注解 ElementType.METHOD 可以给方法进行注解 ElementType.PACKAGE 可以给一个包进行注解 ElementType.PARAMETER 可以给一个方法内的参数进行注解 ElementType.TYPE 可以给一个类型进行注解，比如类、接口、枚举 1234@Target(&#123;ElementType.CONSTRUCTOR, ElementType.METHOD, ElementType.PARAMETER, ElementType.FIELD, ElementType.ANNOTATION_TYPE&#125;) public @interface Autowired &#123; boolean required() default true;&#125; 在上面 Autowire的 注解中，其 Target 注解的值为 CONSTRUCTOR、METHOD、PARAMETER、FIELD、ANNOTATION_TYPE 这 5 个值。这表示 Autowired 注解只能在构造方法、方法、方法形参、属性、类型这 5 种场景下使用。 @Retention注解Retention 注解用来标记这个注解的留存时间。 它其有四个可选值： RetentionPolicy.SOURCE。注解只在源码阶段保留，在编译器进行编译时它将被丢弃忽视。 RetentionPolicy.CLASS。注解只被保留到编译进行的时候，它并不会被加载到 JVM 中。 RetentionPolicy.RUNTIME。注解可以保留到程序运行的时候，它会被加载进入到 JVM 中，所以在程序运行时可以获取到它们。 1234@Retention(RetentionPolicy.RUNTIME) public @interface Autowired &#123; boolean required() default true;&#125; 在上面 Autowire的 注解中，其 Retention 注解的值为 RetentionPolicy.RUNTIME，说明该注解会保留到程序运行的时候。 @Documented@ Documented 注解表示将注解信息写入到 javadoc 文档中。 在默认情况下，我们的注解信息是不会写入到 Javadoc 文档中的。但如果该注解有 @Documented 标识，那么该注解信息则会写入到 javadoc 文档中。 例如在下面这个例子中，我们声明了一个 @Spicy 的注解，没有 @Documented 元注解。 123public @interface Spicy &#123; String spicyLevel();&#125; 声明一个 @Sweet 注解，有 @Documented 元注解。 1234@Documentedpublic @interface Sweet &#123; String sweetLevel();&#125; 接下来写一个 SweetDemo 类，类中的 sweetWithDoc 方法使用 @Sweet 注解，spicyWithoutDoc 方法使用 @Spicy 注解。 1234567891011121314public class SweetDemo &#123; public static void main(String arg[]) &#123; new SweetDemo().sweetWithDoc(); new SweetDemo().spicyWithoutDoc(); &#125; @Sweet (sweetLevel="Level.05") public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; @Spicy (spicyLevel="Level.04") public void spicyWithoutDoc() &#123; System.out.printf("spicy Without Doc."); &#125;&#125; 最后我们使用 Javadoc 命令去生成对应的 JavaDoc 文档，打开文档你会看到：sweetWithDoc方法上面有一个注解信息，而 spicyWithoutDoc 方法上却没有注解信息。 这个就是 @Documented 这个元注解的作用。 @Inherited@ Inherited注解标识子类将继承父类的注解属性。 在下面的例子中，我们声明了一个 Sweet 注解，接着在 Peach 类使用了 @Sweet 注解，但是并没有在 RedPeach 类使用该注解。 123456789//声明一个Sweet注解，标识甜味。@Inherited@Retention(RetentionPolicy.RUNTIME)@interface Sweet &#123;&#125;//桃子有甜味@Sweetpublic class Peach &#123;&#125;//红色的水蜜桃public class RedPeach extends Peach &#123;&#125; 虽然我们没在 RedPeach 类上使用了 @Sweet 注解，但是我们在 Sweet 注解声明中使用了 @Inherited 注解，所以 RedPeach 继承了 Peach 的 @Sweet 注解。 注解属性注解属性类似于类方法的声明，注解属性里有三部分信息，分别是：属性名、数据类型、默认值。 在 @Autowired 注解中就声明了一个名为 required 的 boolean 类型数据，其默认值是 true。 123public @interface Autowired &#123; boolean required() default true;&#125; 需要注意的是，注解中定义的属性，它的数据类型必须是 8 种基本数据类型（byte、short、int、long、float、double、boolean、char）或者是类、接口、注解及它们的数组。 总结个注解大致可以分为三个部分：注解体、元注解、注解属性。在这三个主要组成部分中：注解体指定了注解的名字、元注解则标记了该注解的使用信息，注解属性指明注解的属性。 学习注解只要知道这三个部分就够了，至于那些繁杂的属性，就用下面这张图来解决吧。用到的时候翻一翻，查一查，足矣！ 原文地址注解的那些事儿（二）| 如何自定义注解]]></content>
      <categories>
        <category>Annotation</category>
      </categories>
      <tags>
        <tag>Java注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java注解(一) - 为什么要使用注解]]></title>
    <url>%2F2018%2F03%2F09%2FJava%E6%B3%A8%E8%A7%A3-%E4%B8%80-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是注解注解 是 JDK 1.5 引入的功能。可以看作是对 一个 类／方法 的一个扩展的模版，每个 类／方法 按照注解类中的规则，来为 类／方法 注解不同的参数，在用到的地方可以得到不同的 类／方法 中注解的各种参数与值。 为什么要用注解在 JDK 1.5 之前，Java 还没引入注解，这个时候如果我们要在 Spring 中声明一个 Bean，我们只能通过 XML 配置的方式。 1public class DemoService&#123; &#125; 1&lt;bean id="demoService" class="com.chenshuyi.DemoService"/&gt; 但当有了注解，我们就可以不必写一个 XML 配置文件，可以直接在 DemoService 类上完成 Bean 的声明工作。 12@Service public class DemoService&#123; &#125; 在表面上看来，我们通过注解的方式减少了一个XML配置文件，减少了开发代码量。但这真的是我们用注解而不用 XML 配置文件的原因吗？ 在回答这个问题之前，我们再来回顾一下上面两种配置方式的特点： 对于注解的方式。我们会发现它和代码结合得很紧密，所以注解比较适合做一些与代码相关度高的操作，例如将Bean对应的服务暴露出去。 对于XML配置方式。我们会发现它将配置和代码隔离开来了所以XML配置更适合做一些全局的、与具体代码无关的操作，例如全局的配置等。 我相信很多人此前对于注解的认识就是方便开发。但事实上使用注解还是XML的判断标准应该是：该配置与代码的相关度。如果代码与配置相关度高，那么使用注解配置，否则使用XML配置。 注解和配置文件的比较优点： 配置文件 遵循OCP开发原则，修改配置文件即可进行功能扩展（OCP 开闭原则 Open Closed Principle） 集中管理对象和对象之间的组合关系，易于阅读 注解 开发速度快 编译期间容易发现错误的出处 缺点： 配置文件 开发速度相对较慢 编译时很难检查出错误，运行中的错误很难定位，调试难度较大 注解 管理分散，基本每个类上都有 扩展功能时，没有遵循OCP开发原则 原文地址​ 注解的那些事儿（一）| 为什么要使用注解？]]></content>
      <categories>
        <category>Annotation</category>
      </categories>
      <tags>
        <tag>Java注解</tag>
      </tags>
  </entry>
</search>
