<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[git上统计每个人增删行数]]></title>
    <url>%2F2019%2F04%2F05%2Fgit%E4%B8%8A%E7%BB%9F%E8%AE%A1%E6%AF%8F%E4%B8%AA%E4%BA%BA%E5%A2%9E%E5%88%A0%E8%A1%8C%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1git log --format=&apos;%aN&apos; | sort -u | while read name; do echo -en &quot;$name\t&quot;; git log --author=&quot;$name&quot; --pretty=tformat: --numstat | awk &apos;&#123; add += $1; subs += $2; loc += $1 - $2 &#125; END &#123; printf &quot;added lines: %s, removed lines: %s, total lines: %s\n&quot;, add, subs, loc &#125;&apos; -; done 结果示例 123456789101112Max-laptop added lines: 1192, removed lines: 748, total lines: 444chengshuai added lines: 120745, removed lines: 71738, total lines: 49007cisen added lines: 3248, removed lines: 1719, total lines: 1529max-h added lines: 1002, removed lines: 473, total lines: 529max-l added lines: 2440, removed lines: 617, total lines: 1823mw added lines: 148721, removed lines: 6709, total lines: 142012spider added lines: 2799, removed lines: 1053, total lines: 1746thy added lines: 34616, removed lines: 13368, total lines: 21248wmao added lines: 12, removed lines: 8, total lines: 4xrl added lines: 10292, removed lines: 6024, total lines: 4268yunfei.huang added lines: 427, removed lines: 10, total lines: 417³ö added lines: 5, removed lines: 3, total lines: 2]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18个高效使用Google搜索的技巧]]></title>
    <url>%2F2019%2F03%2F23%2F18%E4%B8%AA%E9%AB%98%E6%95%88%E4%BD%BF%E7%94%A8Google%E6%90%9C%E7%B4%A2%E7%9A%84%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[本文来源：https://mp.weixin.qq.com/s/0WqC1dQBAvehYK3hlVOueQ 前言如果把浩瀚的互联网资源比喻成是一个图书馆，那么google搜索引擎毫无疑问是这个图书馆的导航中心，通过google可以轻而易举得检索到绝大多数你需要的资料，然而大多数人可能并没有充分发挥谷歌搜索的潜力。 如何才能更加高效，快速的利用好谷歌这个搜索引擎呢？这里有18个技巧用来提升你的检索效率。 1.使用Tab面板使用谷歌使用结果完成后，在搜索栏的下面会出现多个Tab面板，默认分别是全部，新闻，图片，视频，地图，更多等，这里面如果我们已经知道我们要搜索的分类是某个类目的时候，可以直接点击Tab面板将搜索结果限定在大的类目中，这样更方便我们定位检索的资源。 2.使用双引号比如我想搜索宠物狗穿的毛衣，我在输入框里面输入了三个关键词pet dog sweaters，默认情况下谷歌返回的内容是包含这个三个词可任意调换顺序的命中结果，如果我们认为这三个词是一个整体，搜索的结果里面必须保持和搜索关键字一样的出现顺序，这个时候我们双引号来告诉谷歌，我们想要更精确的查询： “pet dog sweaters”。 3.使用连字符排除指定搜索内容有时候我们搜索的关键词本身可能有多种含义，这个时候通过连字符可以排除我们不需要出现的结果。比如我们搜 apple 这个词，在谷歌里面可能是水果的意思，也能是iPhone手机相关的含义，如果我们搜的是iPhone相关的内容，这个时候可以设置搜索关键词为： apple -fruit，这样以来就可以排除掉与水果有关的apple信息。 4.使用site关键词site关键字是google索引的一个内置字段，有时我们已经明确我们要搜索的内容就在某个网站，这个时候这个关键字就会很有用。比如说我在谷歌搜索hadoop，但是我只想看官网文档的内容，不想看其他乱七八糟的网站的东西，那么我们就可以这样搜索： hadoop site:apache.org ，这样就能够只检索我们想看的目标网站的内容。 5.使用link关键词使用link关键词限定是另外一个使用比较少的功能，这个功能可以让我们找到含有指定关键词的网页是否链接了我们指定的网站，例如，我们搜索的关键词如下： csdn link:stackoverflow ， 这个关键词的含义代表含有csdn关键词的网页里面，那些有引用可以链接到stackoverflow这个网站。 6.使用通配符检索通配符检索也就是所谓的模糊检索，比如我们可以这样在google中搜索世界最大的国家， “ is the largest country in the world”。或者我们想检索”sp“开头的单词，通过通配符我们使用占位的方式来检索特定内容的结果集。 7.使用related关键词related关键字可以搜索内容相关或者类似的网站，比如我们天天用淘宝购物，现在想知道做电商的其他的网站有哪些，我们的搜索关键词可以这么输入：related:taobao.com 8.使用google去做数学运算这个特性可以在让我们在谷歌搜索框直接输入数学计算表达式，然后谷歌会直接返回一个第一条是个计算器的页面，并且计算结果也显示在计算器里面。例如直接搜索：PI , 1+2+3 , 5*100+3 , 10/3(结构是浮点数)等表达式。 9.一次搜索多个关键词注意，这里有个强调关键词的概念，大部分情况下，如果我们在谷歌搜索框输入的关键词越多，那么命中的结果集就会越来越小，有可能直接导致搜索不到数据，所以在搜索中尽量找到一个或多个关键词，默认情况谷歌使用的AND逻辑，多个关键词都必须出现才能命中结果，但一些情况下我们想要只出现其中任意一个关键词命中就可以，我们就可以使用OR关键词，例如：dog OR cat wiki，一下检索两个关键词的维基百科，注意OR必须大写。 10.搜索使用数字范围搜索范围限制功能使用也非常简单，使用两个点号就可以了，举个例子，比如我要搜索java在2013-2014年有关的文章，输入的语法如下：java 2013..2014 就可以了。如果我们不写任何关键词，直接输入比如： 33..35，那么google就会广泛的搜索在33和35之间任何有关的东西。 11.关键字尽量简单谷歌检索其实是依据关键词来检索的，这就要求描述尽量精简和准确而并不是描述详细和冗长，比如你想搜索附近的肯德基餐厅有哪些？ 如果直接输入： 我想知道附近的肯德基餐厅有哪些？ 其实是没必要的。 可以直接替换为： 肯德基 附近 就行了，这样以来既精简又准确。 12.逐步增加搜索关键词有时候，简单的关键词可能描述的确实不够完善，这个时候我们应该逐步的增加关键字来获取更好的搜索效果。比如你要进行一次演讲而不知道如何准备，那么你可以在谷歌里面搜索的阶段如下： a. 演讲 b. 准备 演讲 c. 如何 准备 演讲 注意这里面没有从第一步直接过度到第三步的原因是如果缺少了第二步，可能会漏掉某些我们想要的结果。因为很多网站描述同一件事使用不同的方式，我们通过这种方式可以尽可能全的，准的找到我们想要的信息。 13.描述替换这是一个非常重要的原则，有时候我们说的话表达同一个意思，但可能使用不同的描述。 不同的描述，虽然意思或者目的可能相同，但谷歌搜索的结果却是不一样。举个例子，某一天你开的车的轮胎坏了，如果你直接在谷歌搜索： 我的车的轮胎坏了，可能解决不了你的问题，而你真实的意思表达的是想修理轮胎，所以这个时候你应该这样描述： 修理 轮胎。 另外一个例子，如果你的头受伤了，如果你直接搜： 我的头受伤了这可能不是你真实的目的，其实你可能想要找如何减轻头痛或者缓解的方式，这个时候你应该检索： 头痛 缓解。通过这样的转换，可以帮助我们找到更准确的结果。 14.只使用最重要的关键词这个原则其实很前面说的几条有点类似，默认情况下输入谷歌搜索的关键词越多，返回的结果就会越少，如果找不到最重要的关键词，那么反而会浪费时间在切换尝试上。举个例子如果你搜索： 我在哪里可以找到一个海底捞餐厅。这样反而可能搜不到结果。相反替换成： 海底捞 餐厅 附近 可能效果会好的多。总之使用谷歌搜索的时候，尽量保持关键词简单和重要。 15.快捷搜索命令在谷歌上有一些常用的快速检索出结果的关键词，比如你搜下面的几个关键词： 中国邮政编码 中国高校 中国首都 中国历史古都 圆周率 miles to km 人民币汇率 等等都可以直接出对应的结果 16.拼写自动纠正这个功能我们在很多软件里面都有，比如word，ppt里面，当然谷歌里面也一样，如果某个单词的某个字母写错了，不管是缺少字母，还是多字母或者顺序不对，都基本不影响我们的使用。 17.使用描述性词语这个技巧和前面的几条其实是有关系的，简单的说，如果搜索某个关键词没有命中的时候，我们可以使用其同义词，或者意思相近的描述来增加搜索范围，这样就会有更多可能找到我们想要的。 举个例子，比如搜： 如何给windows系统装驱动 ？ 可以替换描述为： 解决windows驱动问题。 等等类似的。 18.使用filetype关键词filetype也是一个非常使用的功能，在寻找或者下载某一类文件时候能够快速检索我们需要的文件后缀的资源。比如搜索： 深入理解计算机系统 filetype:pdf 就可以找到某些电子书的pdf版本，同样的我们可以用来搜索ppt，word，mp3等各种格式的资源。 总结谷歌搜索是世界强大的搜索引擎没有之一，使用上面的这些搜索技巧可以让我们能够快速找到分布在互联网里面任何你需要的东西。善用谷歌搜索引擎，你可以无国界的遨游在互联网上，你可以学习到任何免费的资源，无论是哪个领域，只要你愿意，Google都可以成为你的人生导师。]]></content>
      <categories>
        <category>Google</category>
      </categories>
      <tags>
        <tag>Google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM调优实践]]></title>
    <url>%2F2019%2F02%2F25%2FJVM%E8%B0%83%E4%BC%98%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[原文地址https://blog.wangqi.love/articles/Java/JVM%E8%B0%83%E4%BC%98%E5%AE%9E%E8%B7%B5.html JVM调优是一个非常依赖实践的工作，本文就是在某些场景下对JVM调优方法的整理。 CPU占用高CPU占用高是我们在线上会遇到的场景。出现这种情况，我们首先需要定位消耗CPU资源的代码。 我们以下面的代码为例，介绍怎么定位问题： 1234567891011121314public class InfiniteLoop &#123; public static void main(String[] args) &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; long i = 0; while (true) &#123; i++; &#125; &#125; &#125;); thread.start(); &#125;&#125; 这段代码就是一个简单的死循环。 执行程序后，执行top命令： 通过top命令，我们发现PID为10995的Java进程占用CPU高达99.9%。 下一步如何定位到具体线程？ 执行以下命令显示线程列表： 1ps -mp pid -o THREAD,tid,time 找到了占用CPU最高的线程11005，占用CPU时间为02:23 然后通过以下命令将找到的线程ID转换为16进制格式： printf &quot;%x\n&quot; tid 最后通过以下命令打印线程的堆栈信息： jstack pid | grep tid -A 30 通过线程堆栈信息，我们可以定位到是InfiniteLoop中的run方法。 Full GC频繁在线上环境，频繁的执行Full GC会导致程序经常发生停顿，从而导致接口的响应时间变长，这时就需要对JVM的状态进行监控，确定Full GC发生的原因。 首先我们在启动程序的时候可以加上GC日志相关的参数，主要有以下几个： -XX:+PrintGC：输出GC日志 -XX:+PrintGCDetails：输出GC的详细日志 -XX:+PrintGCTimeStamps：输出GC的时间戳（以基准时间的形式） -XX:+PrintGCDateStamps：输出GC的时间戳（以日期的形式，如2018-08-29T19:22:48.741-0800） -XX:+PrintHeapAtGC：在进行GC的前后打印出堆的信息 -Xloggc:gc.log：日志文件的输出路径 现在通过程序来模拟Full GC频繁发生的情形： 123456789101112131415161718192021222324252627282930class Object1 &#123; int size = (10 * 1024 * 1024) / 4; int[] nums = new int[size]; public Object1() &#123; for (int i = 0; i &lt; size; i++) &#123; nums[i] = i; &#125; &#125;&#125;class Object2 &#123; int size = (1 * 1024 * 1024) / 4; int[] nums = new int[size]; public Object2() &#123; for (int i = 0; i &lt; size; i++) &#123; nums[i] = i; &#125; &#125;&#125;public class HeapOOM &#123; public static void main(String[] args) throws InterruptedException &#123; Object1 object1 = new Object1(); while (true) &#123; Object2 object2 = new Object2(); Thread.sleep(100); &#125; &#125;&#125; 我们知道Java堆被划分为新生代和老年代。默认比例为1:2（可以通过-XX:NewRatio设定）。 新生代又分为Eden、From Survivor、To Survivor。这样划分的目的是为了使JVM能够更好地管理堆内存中的对象，包括内存的分派以及回收。默认比例为Eden:From:To = 8:1:1（可以通过参数-XX:SurvivorRatio来设定，-XX:SurvivorRatio=8表示Eden与一个Survivor空间比例为8:1） 一般新建的对象会分配到Eden区。这些对象经过第一次Minor GC后，如果仍然存活，将会被移到Survivor区。在Survivor每熬过一轮Minor GC年龄就增加1 当年龄达到一定程度是（年龄阈值，默认为15，可以通过-XX:MaxTenuringThreshold来设置），就会被移动到老年代。 from和to之间会经常互换角色，from变成to，to变成from。每次GC时，把Eden存活的对象和From Survivor中存活且没超过年龄阈值的对象复制到To Survivor中，From Survivor清空，变成To Survivor。 GC分为两种： Minor GC是发生在新生代中的垃圾收集动作，所采用的是复制算法，所采用的是复制算法，因为Minor GC比较频繁，因此一般回收速度较快。 Full GC是发生在老年代的垃圾收集动作，所采用的是标记-清除算法，速度比Minor GC慢10倍以上 大对象直接进入老年代。比如很长的字符串以及数组。通过设置-XX:PretenureSizeThreshold，令大于这个值的对象直接在老年代分配。这样做是为了避免在Eden和两个Survivor之间发生大量的内存复制。 什么时候发生Minor GC？什么时候发生Full GC？ 当新生代Eden区没有足够的空间进行分配时，虚拟机将发起一次Minor GC 老年代空间不足时发起一次Full GC 我们以下面的命令来执行程序： 1java -Xms30m -Xmx30m -Xmn2m -XX:SurvivorRatio=8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=dump/dump.hprof dump.HeapOOM 以下是对上面JVM参数的说明： -Xms：堆初始大小 -Xmx：堆最大值 -Xmn：新生代大小（老年代大小=堆大小-新生代大小） -XX:+HeapDumpOnOutOfMemoryError：发生内存溢出时生成heapdump文件 -XX:HeapDumpPath：指定heapdump文件 我们之所以将新生代的大小设为2m，是因为这样新建的Object2对象就无法在新生代上分配，从而直接进入老年代，当老年代空间占满后就会触发Full GC。 程序执行之后，我们从GC日志中看到频繁发生Full GC，于是我们开始定位Full GC发生的原因。 以下面的两段GC日志，来看一下GC日志的含义： 1231.840: [GC (Allocation Failure) [PSYoungGen: 573K-&gt;432K(1536K)] 28221K-&gt;28088K(30208K), 0.0014619 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]1.842: [GC (Allocation Failure) [PSYoungGen: 432K-&gt;400K(1536K)] 28088K-&gt;28056K(30208K), 0.0005985 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]1.843: [Full GC (Allocation Failure) [PSYoungGen: 400K-&gt;0K(1536K)] [ParOldGen: 27656K-&gt;10558K(28672K)] 28056K-&gt;10558K(30208K), [Metaspace: 2657K-&gt;2657K(1056768K)], 0.0038527 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 最前面的数字1.840:、1.842:和1.843:代表了GC发生的时间，这个数字的含义是从Java虚拟机启动以来经过的秒数 GC日志开头的[GC和[Full GC说明了这次垃圾收集的停顿类型，而不是用来区分新生代GC还是老年代GC的。如果有”Full GC”，说明这次GC是发生了Stop-The-World的。 接下来的[PSYoungGen、[ParOldGen、[Metaspace表示GC发生的区域。这里显示的区域名称与使用的GC收集器是密切相关的，例如上面的PSYoungGen表示采用Parallel Scavenge收集器，ParOldGen表示采用Parallel Old收集器。如果使用Serial收集器显示[DefNew，如果使用ParNew收集器显示[ParNew。 后面方括号内部的400K-&gt;0K(1536K)含义是”GC前该内存区域已经使用容量-&gt;GC后该内存区域已使用容量（该内存区域总容量）”。而在方括号之外的28056K-&gt;10558K(30208K)表示”GC前Java堆已使用容量-&gt;GC后Java堆已使用容量（Java堆总容量）”。 再往后的0.0038527 secs表示该内存区域GC所占用的时间，单位是秒。有的收集器会给出更具体的时间数据，如[Times: user=0.01 sys=0.00, real=0.01 secs]，这里面的user、sys、real与Linux的time命令所输出的时间含义一致，分别代表用户态消耗的CPU时间、内核态消耗的CPU时间和操作从开始到结束所经过的墙钟时间（Wall Clock Time）。CPU时间与墙钟时间的区别是，墙钟时间包括各种非运算的等待耗时，例如等待磁盘IO、等待线程阻塞，而CPU时间不包括这些耗时，但当系统有多CPU或者多核的话，多线程操作会叠加这些CPU时间，所以读者看到user或sys时间超过real时间是完全正常的。 下面开始定位问题。 首先执行jps命令定位程序的进程号。 然后执行jstat命令监视Java堆的状况. 1jstat -gc 11172 1000 其中11172是进程号，1000表示每隔1000毫秒打印一次日志 S0C和S1C（Survivor0、Survivor1）：两个Survivor区的大小 S0U和S1U（Survivor0、Survivor1）：两个Survivor区的使用大小 EC（Eden）：Eden区的大小 EU（Eden）：Eden区的使用大小 OC（Old）：老年代大小 OU（Old）：老年代使用大小 MC：元数据区大小 MU：元数据区使用大小 CCSC：压缩类空间大小 CCSU：压缩类空间使用大小 YGC（Young GC）：年轻代垃圾回收次数 YGCT（Young GC Time）：年轻代垃圾回收总耗时（秒） FGC（Full GC）：老年代垃圾回收次数 FGCT（Full GC Time）：老年代垃圾回收总耗时（秒） GCT（GC Time）：所有GC总耗时（秒） 可以看到，程序在不断发生Full GC。 执行jmap把当前的堆dump下来： 1jmap -dump:live,format=b,file=dump.hprof 11172 其中11172是进程ID 然后将dump.hprof文件使用VisualVM来打开 我们可以看到，int[]对象占用的空间最大，其中int[]#1的GC Root指向了dump.Object1对象，无法被回收，这样一个大对象占用了老年代空间，因此导致了频繁发生Full GC。 解决这个问题有两种思路： 一般情况下原因都是代码问题，导致某个大对象没有及时释放，在多次GC之后进入老年代空间。我们要做的首先是定位到占用大量空间的对象，优化其中的代码，及时释放大对象，腾空老年代空间 增加新生代的大小，让对象都在新生代分配与释放，从而不进入老年代空间。这样就会大大减少Full GC的发生 https://zhangguodong.me/2017/11/25/%E7%90%86%E8%A7%A3GC%E6%97%A5%E5%BF%97/https://segmentfault.com/a/1190000002677695https://www.jianshu.com/p/45415ebe0721https://blog.csdn.net/u010862794/article/details/78020231http://www.blogjava.net/hankchen/archive/2012/05/09/377735.htmlhttp://swcdxd.iteye.com/blog/1859858http://huachao1001.github.io/article.html?C2xJwZZnhttps://www.zybuluo.com/zero1036/note/872396]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用google map实现周边搜索功能]]></title>
    <url>%2F2019%2F02%2F15%2F%E7%94%A8google-map%E5%AE%9E%E7%8E%B0%E5%91%A8%E8%BE%B9%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[项目要实现根据经纬度获取附近的建筑，由于项目在海外运营，谷歌地图首当其冲。 首先说明的是，该功能需要在服务端实现，也就是安卓的SDK不适用。 api文档地址： https://developers.google.com/places/web-service/search#PlaceSearchResults 获取秘钥key的方法： https://developers.google.com/places/web-service/get-api-key api文档地址打不开怎么办，我将文档中的东西复制下来了，如下： 附近的搜索请求默认情况下，当用户选择某个地点时，“附近搜索”会返回所选地点的所有可用数据字段，您将收到相应的结算费用。没有办法将附近搜索请求限制为仅返回特定字段。要避免请求（并支付）您不需要的数据，请改用 查找位置请求。 通过“附近搜索”，您可以搜索指定区域内的位置。您可以通过提供关键字或指定要搜索的地点类型来优化搜索请求。 附近搜索请求是以下格式的HTTP URL： 1https://maps.googleapis.com/maps/api/place/nearbysearch/output?parameters 其中output可能是以下任一值： json （推荐）表示JavaScript Object Notation（JSON）中的输出 xml 表示输出为XML 启动“附近搜索”请求需要某些参数。作为URL中的标准，所有参数都使用ampersand（&amp;）字符分隔。 必需参数 key- 您的应用程序的 API密钥。此密钥标识您的应用程序。有关 更多信息，请参阅 获取密钥。 location - 检索地点信息的纬度/经度。必须将其指定为 纬度，经度。 radius - 定义返回位置结果的距离（以米为单位）。允许的最大半径为50 000米。请注意，radius如果指定rankby=distance（在下面的可选参数下描述），则不得包括 。 如果rankby=distance（在所描述的可选参数下面）被指定，那么一个或多个 keyword，name或type是必需的。 可选参数 keyword - 与Google为此地点编入索引的所有内容匹配的字词，包括但不限于姓名，类型和地址，以及客户评论和其他第三方内容。 language - 语言代码，如果可能，指示应返回结果的语言。请参阅支持的语言 及其代码列表。请注意，我们经常更新支持的语言，因此此列表可能并非详尽无遗。 minprice和maxprice （可选） - 仅将结果限制在指定范围内的那些位置。有效值的范围介于0（最实惠）到4（最昂贵）之间。具体值表示的确切数量因地区而异。 name - 与Google为此地点编入索引的所有内容匹配的字词。相当于keyword。该 name字段不再局限于地名。此字段中的值与keyword字段中的值组合，并作为同一搜索字符串的一部分传递。我们建议仅对keyword所有搜索词使用 参数。 opennow - 仅返回在发送查询时为业务开放的那些位置。如果在查询中包含此参数，则不会返回未在Google地方信息数据库中指定营业时间的地点。 rankby - 指定列出结果的顺序。请注意，rankby如果指定了radius （在上面的必需参数中描述），则不得包括。可能的值是： prominence（默认）。此选项根据结果的重要性对结果进行排序。排名将有利于指定区域内的显着位置。地方在Google索引中的排名，全球受欢迎程度以及其他因素都会影响到突出程度。 distance。此选项按照与指定距离的距离按升序对搜索结果进行偏差location。当 distance被指定时，一个或多个keyword， name或type是必需的。 type - 将结果限制为与指定类型匹配的位置。只能指定一种类型（如果提供了多种类型，则忽略第一个条目后面的所有类型）。请参阅 支持的类型列表。 pagetoken - 返回先前运行的搜索的后20个结果。设置pagetoken参数将使用先前使用的相同参数执行搜索 - pagetoken将忽略除以外的所有参数。 Google Maps API Premium Plan客户注意事项：您必须在请求中包含API密钥。你应该不包括client或 signature参数您的要求。 附近的搜索示例以下示例是澳大利亚悉尼一个1500米半径范围内“餐馆”类型的地点的搜索请求，其中包含“游轮”一词： 1https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=-33.8670522,151.1957362&amp;radius=1500&amp;type=restaurant&amp;keyword=cruise&amp;key=YOUR_API_KEY 注意：在此示例中，您需要key 使用自己的API密钥替换，以使请求在您的应用程序中起作用。 JSON响应最多包含四个根元素： &quot;status&quot;包含请求的元数据。请参阅 下面的状态代码 &quot;results&quot;包含一系列地点，包含每个地方的信息。 有关这些结果的信息，请参见搜索结果 Places API establishment 每个查询最多返回20个结果。另外，political可以返回结果，其用于识别请求的区域。 html_attributions 可能包含一组关于此列表的归属，必须向用户显示（某些列表可能没有归属）。 next_page_token包含一个令牌，可用于返回最多20个附加结果。next_page_token如果没有要显示的其他结果，则不会返回A. 可以返回的最大结果数为60.在next_page_token发布a 和有效之间会有短暂的延迟。 状态代码该&quot;status&quot;搜索响应对象中字段包含请求的状态，并且可能会包含调试信息，以帮助您跟踪请求失败的原因。该&quot;status&quot;字段可能包含以下值： OK表示没有发生错误; 成功检测到该地点，并返回至少一个结果。 ZERO_RESULTS表示搜索成功但未返回任何结果。如果搜索是latlng在远程位置传递的，则可能会发生这种情况 。 OVER_QUERY_LIMIT 表示您已超过配额。 REQUEST_DENIED表示您的请求被拒绝，通常是因为缺少无效key参数。 INVALID_REQUEST通常表示缺少必需的查询参数（location或radius）。 UNKNOWN_ERROR表示服务器端错误; 再试一次可能会成功。 错误消息当Google商家信息服务返回其他状态代码时 OK，error_message搜索响应对象中可能还有一个附加字段。该字段包含有关给定状态代码背后原因的更多详细信息。 访问其他结果默认情况下，每个附近搜索或文本搜索establishment每个查询最多返回20个结果; 但是，每个搜索可以返回多达60个结果，分为三个页面。如果您的搜索返回超过20，那么搜索响应将包含一个额外的值 - next_page_token。将值的值传递给新搜索next_page_token的pagetoken参数以查看下一组结果。如果 next_page_token为null，或者未返回，则没有进一步的结果。在next_page_token发布a 和何时生效之间会有短暂的延迟 。在可用之前请求下一页将返回INVALID_REQUEST响应。使用相同的方法重试请求 next_page_token将返回下一页结果。 例如，在下面的查询中，我们搜索澳大利亚悉尼达令港附近的餐馆，并按距离对结果进行排名。您可以看到响应包含next_page_token属性。 1https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=-33.8670522,151.1957362&amp;rankby=distance&amp;type=food&amp;key=YOUR_API_KEY 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&#123; &quot;html_attributions&quot; : [], &quot;next_page_token&quot; : &quot;CpQCAgEAAFxg8o-eU7_uKn7Yqjana-HQIx1hr5BrT4zBaEko29ANsXtp9mrqN0yrKWhf-y2PUpHRLQb1GT-mtxNcXou8TwkXhi1Jbk-ReY7oulyuvKSQrw1lgJElggGlo0d6indiH1U-tDwquw4tU_UXoQ_sj8OBo8XBUuWjuuFShqmLMP-0W59Vr6CaXdLrF8M3wFR4dUUhSf5UC4QCLaOMVP92lyh0OdtF_m_9Dt7lz-Wniod9zDrHeDsz_by570K3jL1VuDKTl_U1cJ0mzz_zDHGfOUf7VU1kVIs1WnM9SGvnm8YZURLTtMLMWx8-doGUE56Af_VfKjGDYW361OOIj9GmkyCFtaoCmTMIr5kgyeUSnB-IEhDlzujVrV6O9Mt7N4DagR6RGhT3g1viYLS4kO5YindU6dm3GIof1Q&quot;, &quot;results&quot; : [ &#123; &quot;geometry&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : -33.867217, &quot;lng&quot; : 151.195939 &#125; &#125;, &quot;icon&quot; : &quot;http://maps.gstatic.com/mapfiles/place_api/icons/cafe-71.png&quot;, &quot;id&quot; : &quot;7eaf747a3f6dc078868cd65efc8d3bc62fff77d7&quot;, &quot;name&quot; : &quot;Biaggio Cafe - Pyrmont&quot;, &quot;opening_hours&quot; : &#123; &quot;open_now&quot; : true &#125;, &quot;photos&quot; : [ &#123; &quot;height&quot; : 600, &quot;html_attributions&quot; : [], &quot;photo_reference&quot; : &quot;CnRnAAAAmWmj0BqA0Jorm1_vjAvx1n6c7ZNBxyY-U9x99-oNyOxvMjDlo2npJzyIq7c3EK1YyoNXdMFDcRPzwLJtBzXAwCUFDGo_RtLRGBPJTA2CoerPdC5yvT2SjfDwH4bFf5MrznB0_YWa4Y2Qo7ABtAxgeBIQv46sGBwVNJQDI36Wd3PFYBoUTlVXa0wn-zRITjGp0zLEBh8oIBE&quot;, &quot;width&quot; : 900 &#125; ], &quot;place_id&quot; : &quot;ChIJIfBAsjeuEmsRdgu9Pl1Ps48&quot;, &quot;scope&quot; : &quot;GOOGLE&quot;, &quot;price_level&quot; : 1, &quot;rating&quot; : 3.4, &quot;reference&quot; : &quot;CoQBeAAAAGu0wNJjuZ40DMrRe3mpn7fhlfIK1mf_ce5hgkhfM79u-lqy0G2mnmcueTq2JGWu9wsgS1ctZDHTY_pcqFFJyQNV2P-kdhoRIeYRHeDfbWtIwr3RgFf2zzFBXHgNjSq-PSzX_OU6OT2_3dzdhhpV-bPezomtrarW4DsGl9uh773yEhDJT6R3V8Fyvl_xeE761DTCGhT1jJ3floFI5_c-bHgGLVwH1g-cbQ&quot;, &quot;types&quot; : [ &quot;cafe&quot;, &quot;bar&quot;, &quot;restaurant&quot;, &quot;food&quot;, &quot;establishment&quot; ], &quot;vicinity&quot; : &quot;48 Pirrama Rd, Pyrmont&quot; &#125;, &#123; &quot;geometry&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : -33.866786, &quot;lng&quot; : 151.195633 &#125; &#125;, &quot;icon&quot; : &quot;http://maps.gstatic.com/mapfiles/place_api/icons/generic_business-71.png&quot;, &quot;id&quot; : &quot;3ef986cd56bb3408bc1cf394f3dad9657c1d30f6&quot;, &quot;name&quot; : &quot;Doltone House&quot;, &quot;photos&quot; : [ &#123; &quot;height&quot; : 1260, &quot;html_attributions&quot; : [ &quot;From a Google User&quot; ], &quot;photo_reference&quot; : &quot;CnRwAAAAeM-aLqAm573T44qnNe8bGMkr_BOh1MOVQaA9CCggqtTwuGD1rjsviMyueX_G4-mabgH41Vpr8L27sh-VfZZ8TNCI4FyBiGk0P4fPxjb5Z1LrBZScYzM1glRxR-YjeHd2PWVEqB9cKZB349QqQveJLRIQYKq2PNlOM0toJocR5b_oYRoUYIipdBjMfdUyJN4MZUmhCsTMQwg&quot;, &quot;width&quot; : 1890 &#125; ], &quot;place_id&quot; : &quot;ChIJ5xQ7szeuEmsRs6Kj7YFZE9k&quot;, &quot;scope&quot; : &quot;GOOGLE&quot;, &quot;reference&quot; : &quot;CnRvAAAA22k1PAGyDxAgHZk6ErHh_h_mLUK_8XNFLvixPJHXRbCzg-gw1ZxdqUwA_8EseDuEZKolBs82orIQH4m6-afDZV9VcpggokHD9x7HdMi9TnJDmGb9Bdh8f-Od4DK0fASNBL7Me3CsAWkUMWhlNQNYExIQ05W7VbxDTQe2Kh9TiL840hoUZfiO0q2HgDHSUyRdvTQx5Rs2SBU&quot;, &quot;types&quot; : [ &quot;food&quot;, &quot;establishment&quot; ], &quot;vicinity&quot; : &quot;48 Pirrama Rd, Pyrmont&quot; &#125;, &#123; &quot;aspects&quot; : [ &#123; &quot;rating&quot; : 23, &quot;type&quot; : &quot;overall&quot; &#125; ], ... ], &quot;status&quot; : &quot;OK&quot;&#125; 要查看下一组结果，您可以提交新查询，并将结果传递next_page_token给pagetoken 参数。例如： 1https://maps.googleapis.com/maps/api/place/nearbysearch/json?pagetoken=CpQCAgEAAFxg8o-eU7_uKn7Yqjana-HQIx1hr5BrT4zBaEko29ANsXtp9mrqN0yrKWhf-y2PUpHRLQb1GT-mtxNcXou8TwkXhi1Jbk-ReY7oulyuvKSQrw1lgJElggGlo0d6indiH1U-tDwquw4tU_UXoQ_sj8OBo8XBUuWjuuFShqmLMP-0W59Vr6CaXdLrF8M3wFR4dUUhSf5UC4QCLaOMVP92lyh0OdtF_m_9Dt7lz-Wniod9zDrHeDsz_by570K3jL1VuDKTl_U1cJ0mzz_zDHGfOUf7VU1kVIs1WnM9SGvnm8YZURLTtMLMWx8-doGUE56Af_VfKjGDYW361OOIj9GmkyCFtaoCmTMIr5kgyeUSnB-IEhDlzujVrV6O9Mt7N4DagR6RGhT3g1viYLS4kO5YindU6dm3GIof1Q&amp;key=YOUR_API_KEY 设置pagetoken将导致忽略任何其他参数。查询将执行与之前相同的搜索，但将返回一组新结果。您可以在原始查询后最多两次请求新页面。必须依次显示每页结果。搜索结果的两页或多页不应作为单个查询的结果显示。请注意，每次搜索都会计入针对您的使用限制的单个请求。 但是，比较坑的一点是同一个key一天调用的次数最多150000次，如果用户量较大时，要专门交费调整限制次数 。 代码示例pom依赖 12345&lt;dependency&gt; &lt;groupId&gt;de.taimos&lt;/groupId&gt; &lt;artifactId&gt;httputils&lt;/artifactId&gt; &lt;version&gt;1.11&lt;/version&gt;&lt;/dependency&gt; 测试方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import de.taimos.httputils.WS;import org.apache.http.HttpResponse;/** * @author: xbq * @date: 2019/2/13 10:47 * @description: */public class GoogleMap &#123; public static final String NEARBY_SEARCH_URL = "https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=%s&amp;radius=%s&amp;key=%s"; public static final String NEXT_PAGE_URL = "https://maps.googleapis.com/maps/api/place/nearbysearch/json?pagetoken=%s&amp;key=%s"; public static final String PHOTO_URL = "https://maps.googleapis.com/maps/api/place/photo?maxwidth=480&amp;photoreference=%s&amp;key=%s"; // 秘钥 public static final String KEY = "你申请的KEY"; public static void main(String[] args) &#123; HttpResponse response = null; String resStr = null; // 调用接口进行请求地址数据 String location = "-33.8670522,151.1957362"; String radius = "1500"; response = WS.url(String.format(NEARBY_SEARCH_URL, location, radius, KEY)).get(); resStr = WS.getResponseAsString(response); System.out.println("resStr==" + resStr); /** * 请求返回的数据中有nextPageToken字段不为空时，就有多页数据，请求下一页数据时需要将该字段携带去请求 * 查询下一页的内容 */ String nextPageToken = "CoQE_QEAAK74QaEIfb5nG5Zfjuk0Dk2zRojKXhXr1-XhgzEY8xJrwPDvWTs82dGJuQ9JNIERxYWjadPeP-XwPqiKMWbuUpRw0vYfp7xwoj7YOhoYyF9yXwREjhKiRT_F-gaBJnvG_6FqqPbql6f4vBlzclrgu5pjSh4rUFgyU_lpHSRKSHmDaoSvVFynQe7G29-xRT54QXq35_dzIYRVEiHGhv-8qX2b8R_G237_dHIaZr5LpXbLWA7Y6j_78USKHy1t0Mpa2kKLK-bjmYlPniyX-CMocX_KwfQJplnrpLet-4vZiXo9HaPP_jaVOm6HSj-O3vdsra0Dn1fFBIt10kBpt0j1LuQlShjT2ivDgS1UjhiwGqtXRvj_iAN1SKWYuV2CXMqAFg4lkHCtfcPF0H_YUiHYiVup-xQI0cnBtbVmVR7VlvJs3S98H0hhuVyfNfp0b7KoFqwbDaw6Cfc3ohxRD-pnn5ZAfqcKFbuEYyqsHbiUAdtaFIgF07hQTNk-cswO0zaw8jQofrAkS_GjR4QCL1HY2mvWnl1g6fpi4yR28n5O6jRbtcs6MSxILh2QJhZBttmHkKYGDr218971kvmPWL9gcS981xfSNPxEjzd6IcCUrgh4ObV19OLr1JUgigqH2mD2g1JcEmgvX5SQuxIhDOQKnFPuF4AEYFO7Y58ZjTls4GTTEhBTOpSDTtIN1OYtspW9OjpXGhSVH5BjnBGwuG5HYPf-SERJMC1Pkg"; response = WS.url(String.format(NEXT_PAGE_URL, nextPageToken, KEY)).get(); resStr = WS.getResponseAsString(response); System.out.println("resStr2==" + resStr); /** * 获取某个地点对应的照片。然后将得到的 url 复制到 浏览器中查看 * photoreference 这个值 是在上面的请求中得到的 */ String photoreference = "CmRaAAAAdxvDbnaBSQO4MDseo-3SB_TZ4pd2c1EC765iu_Vu3-2XOA-LFgaZ6iiTY5sYCKxh_ZiQP0ds6qVDP4RAQn4Lxw6OEonSpgzzBBU0BrwrYMP91shZA1HIkaQVZaxKtYeqEhCKrYqwlzcV8dVs4Xv7AY2KGhRdpmMGa3n0S6d2n5cN6SckW49D6g"; resStr = String.format(PHOTO_URL, photoreference, KEY); System.out.println("resStr3==" + resStr); &#125;&#125; 得到的返回结果为（有点长，省略了一部分json。。需要什么值 直接解析下面的JSON即可）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899resStr==&#123; &quot;html_attributions&quot; : [], &quot;next_page_token&quot; : &quot;CoQE_QEAAFOBxJ8gWNrKkExgSbivQYAd46cKlMl8LNTu-ZIx8y3JmpiYhh5dqi3m9sUUwxcQ-MH7eKUgfljdH4RSlU1ExLyzw0vhfvYwfOfdmnbnIHz9xPM1hh5vBgYW2xn85NDRHNYjvB7uGTKoT3w7J5ZIy0pGCesQMjb3ritCHMT-y24DazvRsMEYgsyHk5H7TZWi6169xPyPgKi5uH5wSBpAB0zZdbuGGUY9979sVzRor32JEx6K-hkH6nfo8b1-gIrXBg_YASS81wkkHlqhMj04RDROXBpe_5Y9s4DMqGZNJuAem1Y3O0aEspDI-tT-swpUUYpe3Nvve9seDO7tqkwPWaAGMKGynt5ZlF0GSUXcEao-6dKuvRE0RaiOpdwNTNvGN7kYu6ACfAmDPnXmmIGoo-kdLhzxB4YVXgFYzHiaRIGE6Agj5kCoZ2HNaq-WHQWxx3CquyRV6bb77O_L-rds48ukOVYZ2QkhmfaJAfHGeGF0AyuPcUSip9PkjztzVzRw0oT7m3cASeTyWhci6e97h4chEKhddAc2xAqYZQ-hC8G4uyjp6L0uO_9ENEKYRRFRYWTtYtlpC_mn_xwyr76WN_IjYIyQDci2fFa9y12sPa3fow0W0FOlACfUc-RmU7f_ml1gRAYzyAQIHbDBYxYSS3dhuGDSIBnZfC3Upgh_O51jEhCkXggwwuSE1-t2CtLV_CotGhRCej6bWUC0D4ObssTvgnosi2LDHA&quot;, &quot;results&quot; : [ &#123; &quot;geometry&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : -33.8688197, &quot;lng&quot; : 151.2092955 &#125;, &quot;viewport&quot; : &#123; &quot;northeast&quot; : &#123; &quot;lat&quot; : -33.5781409, &quot;lng&quot; : 151.3430209 &#125;, &quot;southwest&quot; : &#123; &quot;lat&quot; : -34.118347, &quot;lng&quot; : 150.5209286 &#125; &#125; &#125;, &quot;icon&quot; : &quot;https://maps.gstatic.com/mapfiles/place_api/icons/geocode-71.png&quot;, &quot;id&quot; : &quot;044785c67d3ee62545861361f8173af6c02f4fae&quot;, &quot;name&quot; : &quot;Sydney&quot;, &quot;photos&quot; : [ &#123; &quot;height&quot; : 1536, &quot;html_attributions&quot; : [ &quot;\u003ca href=\&quot;https://maps.google.com/maps/contrib/115027288387975928704/photos\&quot;\u003eAlan Chen\u003c/a\u003e&quot; ], &quot;photo_reference&quot; : &quot;CmRaAAAA2CmGfo6miJRR93a5XHlI8CUC8ms65rFBpvc5S6UZaKDffZkd3ACtDfnmemKl-AiCeYtev2l3-e8TSVK79B12jINbfk7pzmydQv2auPDTzCqpuGaFqSnwmn6wbzolzZcfEhBcKU3f6uUEPh6gtA3qlq_ZGhS3kXjaoHny1qtBO7YcDcIUmryV-g&quot;, &quot;width&quot; : 2300 &#125; ], &quot;place_id&quot; : &quot;ChIJP3Sa8ziYEmsRUKgyFmh9AQM&quot;, &quot;reference&quot; : &quot;ChIJP3Sa8ziYEmsRUKgyFmh9AQM&quot;, &quot;scope&quot; : &quot;GOOGLE&quot;, &quot;types&quot; : [ &quot;colloquial_area&quot;, &quot;locality&quot;, &quot;political&quot; ], &quot;vicinity&quot; : &quot;Sydney&quot; &#125; ], &quot;status&quot; : &quot;OK&quot;&#125;resStr2==&#123; &quot;html_attributions&quot; : [], &quot;next_page_token&quot; : &quot;CqQFnQIAACwVVBaTf9R5PKhw3tFgSYiobH1a7xJR4sAj8tMXbjdPG8iyUafylxU9Ve1LsQncixVCFUprcYeSpjPzpk1JMCHRzPcEQ0UkJsSSt4Gnmwbqn2sJ6EnUywGul-rVN9tg2No3KGx3ezIBc8ITnogFZAxXCkfGsP6ty4y4wC1Zqc4d4JjYD-P_JIwlSQmO5kjCKLfrnMzbAIaQrYWcUVnqcbtYdxiv41u7UL3zaEly7wDEK4d0kGnOriD1GCFD5Drl9KnGeNJ5kdCT3tiUduNQMioiU5XkKrb8DYLpEfBmCpgqCxL2_AKssad-WBGGu_OU5yIz1NOqi1g78q_-R7JF3rfKo2ZQ2KF8XV3vOrhoK_Y6699-kZ7XQI0ztxoRTUTssiBnpjBRhkDWI-IACrj9FwPyogE0qCB0BEDyuXUk05tR9GqSoLy56JLMSimFVCiUSHz2dXnStHl7Sg-wia562jQacFgjq_w1_wJmHvYr-QRwKt-YPCZuS5x7Fo7dzz5qJniulQ_FB5UwVVw-DHuXF5KJJ0x8uFLbiCm-9B4q7XjF3Y8rTX94tnpfR9ow92GTXL1GHnQZRC67cQqEZG7w5OSHnsyv0rbagM-DYfBfB4dxP1CBrXvZxVs713Eh3kwNqfSJaBZDkOJKzF-ObMuVfTLL3giugfx1knM4j_--0e_117MzeC4skxQRV6Q67kQqpf3lDQmMPUZX87BULX6Lx394G6DBQYf2XUwPGrRF9c_CckUo2_OOg5KbdAUQqlfTE0-Wk6l8b4njzJ8_BluMHMZLwGqa_SllxEsY_4F8vMRw-ml_6gsc_lapkrh9MylaPD7FeHioTEDKPB_UcJ5O67pNDRfyFee1jW2MHyfAev-3RpyT18kgk_QVNQhkjsO9NBIQh41tRo68GtQhXHfxlx1MJxoUcP_2zUJbBnv0cKwt44vWPiTWo4A&quot;, &quot;results&quot; : [ &#123; &quot;geometry&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : -33.86536760000001, &quot;lng&quot; : 151.2090887 &#125;, &quot;viewport&quot; : &#123; &quot;northeast&quot; : &#123; &quot;lat&quot; : -33.86401861970851, &quot;lng&quot; : 151.2104376802915 &#125;, &quot;southwest&quot; : &#123; &quot;lat&quot; : -33.8667165802915, &quot;lng&quot; : 151.2077397197085 &#125; &#125; &#125;, &quot;icon&quot; : &quot;https://maps.gstatic.com/mapfiles/place_api/icons/lodging-71.png&quot;, &quot;id&quot; : &quot;7966e287de7b33958cd5fac4bdcab4c3c8a7cf75&quot;, &quot;name&quot; : &quot;Radisson Blu Plaza Hotel Sydney&quot;, &quot;opening_hours&quot; : &#123; &quot;open_now&quot; : true &#125;, &quot;photos&quot; : [ &#123; &quot;height&quot; : 1243, &quot;html_attributions&quot; : [ &quot;\u003ca href=\&quot;https://maps.google.com/maps/contrib/113323939240497973930/photos\&quot;\u003eRadisson Blu Plaza Hotel Sydney\u003c/a\u003e&quot; ], &quot;photo_reference&quot; : &quot;CmRaAAAAGqGsjVf5XLDhg0WDZJHoo6K2GD6145vEVOOPL6oRSYJOTnQ7IB7xg_HR7VAx5Txkv_xdKRg9X5qJ9dHluAXV-eMifF4oqizMSnzjSw62OzCBLySWJmVw-SpievEfp-GrEhAfho_Cnggh1UjsWhaW1l1SGhQccEY-CZKWsXSuZFifHGWttUyKpA&quot;, &quot;width&quot; : 1244 &#125; ], &quot;place_id&quot; : &quot;ChIJI6ovxEGuEmsRAdcebtTwTrU&quot;, &quot;plus_code&quot; : &#123; &quot;compound_code&quot; : &quot;46M5+VH Sydney, New South Wales, Australia&quot;, &quot;global_code&quot; : &quot;4RRH46M5+VH&quot; &#125;, &quot;rating&quot; : 4.5, &quot;reference&quot; : &quot;ChIJI6ovxEGuEmsRAdcebtTwTrU&quot;, &quot;scope&quot; : &quot;GOOGLE&quot;, &quot;types&quot; : [ &quot;lodging&quot;, &quot;point_of_interest&quot;, &quot;establishment&quot; ], &quot;user_ratings_total&quot; : 1080, &quot;vicinity&quot; : &quot;27 O&apos;Connell Street, Sydney&quot; &#125; ], &quot;status&quot; : &quot;OK&quot;&#125;resStr3==https://maps.googleapis.com/maps/api/place/photo?maxwidth=480&amp;photoreference=CmRaAAAAdxvDbnaBSQO4MDseo-3SB_TZ4pd2c1EC765iu_Vu3-2XOA-LFgaZ6iiTY5sYCKxh_ZiQP0ds6qVDP4RAQn4Lxw6OEonSpgzzBBU0BrwrYMP91shZA1HIkaQVZaxKtYeqEhCKrYqwlzcV8dVs4Xv7AY2KGhRdpmMGa3n0S6d2n5cN6SckW49D6g&amp;key=AIzaSyD-M2PDsTZaEP28taVQD9wysAJLyZxkUDM]]></content>
      <categories>
        <category>Social</category>
      </categories>
      <tags>
        <tag>google</tag>
        <tag>map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的会高效的在GitHub搜索开源项目吗?]]></title>
    <url>%2F2019%2F02%2F13%2F%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BC%9A%E9%AB%98%E6%95%88%E7%9A%84%E5%9C%A8GitHub%E6%90%9C%E7%B4%A2%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%90%97%2F</url>
    <content type="text"><![CDATA[文章来源于【纯洁的微笑】公众号 GitHub的流行， GitHub在开源世界的受欢迎程度自不必多言。再加上今天，GitHub官方又搞了个大新闻：私有仓库也改为免费使用，这在原来可是需要真金白银的买的。可见微软收购后，依然没有改变 GitHub 的定位，甚至还更进一步。 花开两朵，各表一枝。我们今天想要聊的并不是 GitHub 多么重要，而是要说一下 GitHub 的搜索功能。 你在 GitHub上搜索代码时，是怎么样操作的呢？是不是也是像我这样，直接在搜索框里输入要检索的内容，然后不断在列表里翻页找自己需要的内容？ 或者是简单筛选下，在左侧加个语言的过滤项。 再或者改变一下列表的排序方式 这就是「全部」了吗？ 一般的系统检索功能，都会有一个「高级搜索」的功能。需要在另外的界面里展开，进行二次搜索之类的。 GitHub 有没有类似的呢？ 答案是「肯定的」。做为一个为万千工程师提供服务的网站，不仅要有，而且还要技术范儿。 如果我们自己开发一个类似的应用，会怎样实现呢？ 带着思路，咱们一起来看看，GitHub 是怎样做的。 这里我们假设正要学习 Spring Cloud，要找一个 Spring Cloud 的 Demo 参考练手。 1. 明确搜索仓库标题、仓库描述、README GitHub 提供了便捷的搜索方式，可以限定只搜索仓库的标题、或者描述、README等。 以Spring Cloud 为例，一般一个仓库，大概是这样的 其中，红色箭头指的两个地方，分别是仓库的名称和描述。咱们可以直接限定关键字只查特定的地方。比如咱们只想查找仓库名称包含 spring cloud 的仓库，可以使用语法 in:name关键词 如果想查找描述的内容，可以使用这样的方式： in:descripton 关键词 这里就是搜索上面项目描述的内容。 一般项目，都会有个README文件，如果要查该文件包含特定关键词的话，我想你猜到了 in:readme 关键词 2. 明确搜索 star、fork 数大于多少的 一个项目 star 数的多少，一般代表该项目有受欢迎程度。虽然现在也有垃圾项目刷 star ，但毕竟是少数， star 依然是个不错的衡量标准。 stars: &gt;数字 关键字。 比如咱们要找 star 数大于 3000 的Spring Cloud 仓库，就可以这样 stars:&gt;3000 spring cloud 如果不加 &gt;= 的话，是要精确找 star 数等于具体数字的，这个一般有点困难。 如果要找在指定数字区间的话，使用 stars: 10..20 关键词 fork 数同理，将上面的 stars 换成 fork，其它语法相同 3. 明确搜索仓库大小的 比如你只想看个简单的 Demo，不想找特别复杂的且占用磁盘空间较多的，可以在搜索的时候直接限定仓库的 size 。 使用方式： size:&gt;=5000 关键词 这里注意下，这个数字代表K, 5000代表着5M。 4. 明确仓库是否还在更新维护 我们在确认是否要使用一些开源产品，框架的时候，是否继续维护是很重要的一点。如果已经过时没人维护的东西，踩了坑就不好办了。而在 GitHub 上找项目的时候，不再需要每个都点到项目里看看最近 push 的时间，直接在搜索框即可完成。 元旦刚过，比如咱们要找临近年底依然在勤快更新的项目，就可以直接指定更新时间在哪个时间前或后的 通过这样一条搜索pushed:&gt;2019-01-03 spring cloud 咱们就找到了1月3号之后，还在更新的项目。 你是想找指定时间之前或之后创建的仓库也是可以的，把 pushed 改成 created就行。 5. 明确搜索仓库的 LICENSE 咱们经常使用开源软件，一定都知道，开源软件也是分不同的「门派」不同的LICENSE。开源不等于一切免费，不同的许可证要求也大不相同。 2018年就出现了 Facebook 修改 React 的许可协议导致各个公司纷纷修改自己的代码，寻找替换的框架。 例如咱们要找协议是最为宽松的 Apache License 2 的代码，可以这样 license:apache-2.0 spring cloud 其它协议就把apache-2.0替换一下即可，比如换成 mit 之类的。 6. 明确搜索仓库的语言 比如咱们就找 Java 的库， 除了像上面在左侧点击选择之外，还可以在搜索中过滤。像这样： language:java 关键词 7.明确搜索某个人或组织的仓库 比如咱们想在 GitHub 上找一下某个大神是不是提交了新的功能，就可以指定其名称后搜索，例如咱们看下 Josh Long 有没有提交新的 Spring Cloud 的代码，可以这样使用 user:joshlong 组合使用一下，把 Java 项目过滤出来，多个查询之间「空格」分隔即可。 user:joshlong language:java 找某个组织的代码话，可以这样： org:spring-cloud 就可以列出具体org 的仓库。 这个搜索使用起来是不是相当的便捷? 比起直接搜一个关键词不停的翻页点开找效率高多了吧。 推荐阅读： 孤独求败张小龙 在流感爆发的季节里 官方攻略： 高级查询：https://github.com/search/advanced 查询帮助：https://help.github.com/articles/about-searching-on-github]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(7) - 运算符]]></title>
    <url>%2F2019%2F01%2F05%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-7-%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[运算符用于在程序运行时执行数学或逻辑运算。 Go语言内置的运算符有： 算术运算符 关系运算符 逻辑运算符 位运算符 赋值运算符 其他运算符 算术运算符 算术运算符主要有：+、-、*、/、%（求余）、++（自增）、–（自减） 关系运算符 关系运算符主要有：==、!=、&gt;、&lt;、&gt;=、&lt;= 逻辑运算符 逻辑运算符主要有：&amp;&amp;（逻辑AND）、|| （逻辑OR）、! （逻辑NOT） 位运算符位运算符对整数在内存中的二进制进行操作。 位运算符比一般的算术运算符速度要快，而且可以实现一些算术运算符不能实现的功能。如果要开发高效率程序，位运算符是必不可少的。位运算符用来对二进制进行操作，包括：按位与（&amp;）、按位或（|）、按位异或（^）、按位左移（&lt;&lt;）、按位右移（&gt;&gt;） 按位与按位与（&amp;）：对两个数进行操作，然后返回一个新的数，这个数的每个位都需要两个输入数的同一位都为1时 才为1。简单的说：同一位同时为1 则为1 按位或按位或（|）：比较两个数，然后返回一个新的数，这个数的每一位置设置1的条件是任意一个数的同一位为1 则为1。简单的说：同一位其中一个为1 则为1 按位异或按位异或（^）：比较两个数，然后返回一个数，这个数的每一位设为1的条件是两个输入数的同一位不同 则为1，如果相同就设为 0 。简单的说：同一位不相同 则为1 左移运算符（&lt;&lt;）按二进制形式把所有的数字向左移动对应的位数，高位移出（舍弃），低位的空位补零 语法格式： 需要移位的数字 &lt;&lt; 移位的次数 例如：3 &lt;&lt; 4，则是将 数字3 左移了4位 计算过程： 3 &lt;&lt; 4 首先把 3 转换为 二进制数字 0000 0000 0000 0000 0000 0000 0000 0011，然后把该数字高位（左侧）的两个零移出。其他的数字都朝左平移4位，最后在 低位（右侧）的两个空位补零，则得到的最终结果为 0000 0000 0000 0000 0000 0000 0011 0000，则转换为十进制为 48 用 3 * 2 ^4 计算 更加方便，3 乘以 2 的4次方 数字意义 在数字没有溢出的前提下，对于正数 和 负数，左移一位 都相当于 乘以 2的一次方，左移 n 位就相当于 乘以2的 n 次方 右移运算符（&gt;&gt;）按二进制形式把所有的数字都向右移动对应 位移位数，低位移出（舍弃），高位的空位补符号位，即正数补零，负数 补1 语法格式 需要移位的数字 &gt;&gt; 移位的次数 例如：11 &gt;&gt; 2，则是将数字 11 右移2位 计算过程 11的二进制形式为：0000 0000 0000 0000 0000 0000 0000 1011，然后把低位的最后两个数字移出，因为该数字是正数，所有在高位补零，则得到的最终结果为 0000 0000 0000 0000 0000 0000 0000 0010，转化为十进制为2 用 11 / (2 ^ 2) 计算 更加方便，11 除以 2 的2次方 数字意义 右移一位相当于 除2，右移 n 位，则相当于 除以2的 n 次方 赋值运算符赋值运算符有：=、+=、-=、*=、/=、%=、&lt;&lt;=、&gt;&gt;=、&amp;=、^=、|=。 DEMO示例以下是上面知识点的代码演示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137package mainimport "fmt"var a = 21.0var b = 5.0var c float64func main() &#123; fmt.Println("算术运算符 --- start---") Arithmetic() fmt.Println() fmt.Println("关系运算符 --- start---") Relational() fmt.Println() fmt.Println("逻辑运算符 --- start---") Logical() fmt.Println() fmt.Println("位运算符 --- start---") Bitwise() fmt.Println() fmt.Println("赋值运算符 --- start---") Assignment() fmt.Println()&#125;// 算术运算符func Arithmetic() &#123; c = a + b fmt.Printf("第一行 - c 的值为 %.2f\n", c) c = a - b fmt.Printf("第二行 - c 的值为 %.2f\n", c) c = a * b fmt.Printf("第三行 - c 的值为 %.2f\n", c) c = a / b fmt.Printf("第四行 - c 的值为 %.2f\n", c) //c = a % b fmt.Printf("第五行 - c 的值为 %d\n", int(a)%int(b)) a++ fmt.Printf("第六行 - a 的值为 %f\n", a) a = 21 a-- fmt.Printf("第七行 - a 的值为 %f\n", a)&#125;// 关系运算符func Relational() &#123; if (a == b) &#123; fmt.Printf("第一行 - a 等于 b \n") &#125; else &#123; fmt.Printf("第一行 - a 不等于 b \n") &#125; if (a &lt; b) &#123; fmt.Printf("第二行 - a 小于 b \n") &#125; else &#123; fmt.Printf("第二行 - a 不小于 b \n") &#125; if (a &gt; b) &#123; fmt.Printf("第三行 - a 大于 b \n") &#125; else &#123; fmt.Printf("第三行 - a 不大于 b \n") &#125;&#125;// 逻辑运算符func Logical() &#123; a := true b := false if (a &amp;&amp; b) &#123; fmt.Printf("第一行 - 条件为 true \n") &#125; if (a || b) &#123; fmt.Printf("第二行 - 条件为 true \n") &#125; if (!(a &amp;&amp; b)) &#123; fmt.Printf("第三行 - 条件为 true \n") &#125;&#125;// 位运算符func Bitwise() &#123; fmt.Println(252 &amp; 63) fmt.Println(178 | 94) fmt.Println(20 ^ 5) fmt.Println(3 &lt;&lt; 4) // 3 * 2 ^ 4 fmt.Println(11 &gt;&gt; 2) // 11 / (2 ^ 2)&#125;// 赋值运算符func Assignment() &#123; c = a fmt.Printf("第1行 - =运算符实例，c的值为 %f \n", c) c += a fmt.Printf("第2行 - +=运算符实例，c的值为 %f \n", c) c -= a fmt.Printf("第3行 - -=运算符实例，c的值为 %f \n", c) c *= a fmt.Printf("第4行 - *=运算符实例，c的值为 %f \n", c) c /= a fmt.Printf("第5行 - /=运算符实例，c的值为 %f \n", c) c = a fmt.Printf("第6行 - =运算符实例，c的值为 %f \n", c) d := 200 d &lt;&lt;= 2 fmt.Printf("第7行 - &lt;&lt;=运算符实例，d的值为 %d \n", d) // d = d * 2^2 = 200 * 2^2 = 800 d &gt;&gt;= 2 fmt.Printf("第8行 - &gt;&gt;=运算符实例，d的值为 %d \n", d) // d = d / (2^2) = 800 / (2^2) = 200 d &amp;= 2 fmt.Printf("第9行 - &amp;=运算符实例，d的值为 %d \n", d) // d = d &amp; 2 = 0 d |= 2 fmt.Printf("第10行 - |=运算符实例，d的值为 %d \n", d) // d = d | 2 = 2 d ^= 2 fmt.Printf("第11行 - ^=运算符实例，d的值为 %d \n", d) // d = d ^ 2 = 0&#125; 运行结果为： 12345678910111213141516171819202122232425262728293031323334353637算术运算符 --- start---第一行 - c 的值为 26.00第二行 - c 的值为 16.00第三行 - c 的值为 105.00第四行 - c 的值为 4.20第五行 - c 的值为 1第六行 - a 的值为 22.000000第七行 - a 的值为 20.000000关系运算符 --- start---第一行 - a 不等于 b 第二行 - a 不小于 b 第三行 - a 大于 b 逻辑运算符 --- start---第二行 - 条件为 true 第三行 - 条件为 true 位运算符 --- start---6025417482赋值运算符 --- start---第1行 - =运算符实例，c的值为 20.000000 第2行 - +=运算符实例，c的值为 40.000000 第3行 - -=运算符实例，c的值为 20.000000 第4行 - *=运算符实例，c的值为 400.000000 第5行 - /=运算符实例，c的值为 20.000000 第6行 - =运算符实例，c的值为 20.000000 第7行 - &lt;&lt;=运算符实例，d的值为 800 第8行 - &gt;&gt;=运算符实例，d的值为 200 第9行 - &amp;=运算符实例，d的值为 0 第10行 - |=运算符实例，d的值为 2 第11行 - ^=运算符实例，d的值为 0]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>运算符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(6) - 数据类型转换和常量以及iota]]></title>
    <url>%2F2018%2F12%2F31%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-6-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E5%92%8C%E5%B8%B8%E9%87%8F%E4%BB%A5%E5%8F%8Aiota%2F</url>
    <content type="text"><![CDATA[数据类型转换的格式1.T（表达式） 采用数据类型前置加括号的方式进行类型转换。T表示要转换的类型；表达式包括变量、数值、函数返回值等 类型转换时，需要考虑采用两种类型之间的关系和范围，是否会发生数值截断 布尔型无法与其他类型转换 2.float与int之间的转换 float转int会导致精度损失 3.int转string 相当于是 byte 或 rune 转string 该int 值是ASCII吗的编号或者Unicode字符集的编号，转成 string 就是讲根据字符集，将对应编号的字符查找出来 当该值超过Unicode编号的返回，则转成的字符串显示为乱码 例如，当19968转string，就是“一” 【备注】 ASCII字符集中数字的10进制范围是 [30 - 39] ASCII字符集中大写字母的10进制范围是 [65 - 90] ASCII字符集中小写字母的10进制范围是 [97 - 122] Unicode字符集中汉字的范围是 [4e00 - 9fa5]，10进制的范围是[19968 - 40869] 1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport "fmt"func main() &#123; /* 语文成绩是 90，英语成绩是 80.5，计算平均值 */ chinese := 90 english := 80.5 // float转int avg := (chinese + int(english)) / 2 fmt.Println("平均值：" , avg) // 平均值： 85 // int转float avg2 := (float64(chinese) + english) / 2 fmt.Println("平均值2：" , avg2) // 平均值2： 85.25 // 布尔型不能转换为其他 // flag := true // int(flag) // 字符串不能转为int // str := "Joe" // int(str) result := string(chinese) fmt.Println("语文成绩转string，结果为：", result) // 语文成绩转string，结果为： Z x := 'Z' fmt.Println("字符Z转string，结果为：" , string(x)) // 字符Z转string，结果为： Z X := '一' fmt.Println("字符 一 转string，结果为：" , string(X)) // 字符 一 转string，结果为： 一 XX := 19968 fmt.Println("数值19968 转string，结果为：" , string(XX)) // 数值19968 转string，结果为： 一&#125; 常量声明方式1.相对于变量，常量是恒定不变的值，例如圆周率 常量是一个简单值的标识符，在程序运行时，不会被修改 2.常量中的数据类型只可以是布尔型、数字型（整数型、浮点型和复数）和字符串型 3.常量的定义格式： const 标识符 [类型] = 值 可以省略类型说明符 [type]，因为编译器可以根据变量的值来自动推断其类型 显示类型定义： const B string = “Joe” 隐式类型定义： const C = “Joe” 4.多个相同类型的声明可以简写为： const WIDTH , HEIGHT = value1, value2 5.常量定义未被使用，不会再编译时出错 常量用于枚举（常量组）例如如下格式： 12345const ( Unkonwn = 0 Female = 1 Male = 2) 数字0、1、2分别表示未知性别、女性、男性 常量组中如果不指定类型和初始值，则与上一行非空常量的值相同 12345const ( a = 10 b c) 打印 a b c ，输出 ：10 10 10 iota1.iota ，特殊常量值，是一个系统定义的可以被编译器修改的常量值。iota只能出现在常量中。 2.在每一个const 关键字出现时，被重置为0，然后每出现一个常量，iota所代表的数值会自动增加1。iota可以理解成常量组中的常量的计数器，不论该常量的值是什么，只要有一个常量，那么iota就加1 3.iota可以被用作枚举值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport &quot;fmt&quot;func main() &#123; const ( L = iota M = iota N = iota ) fmt.Println(L, M, N) // 输出：0 1 2 const ( L1 = iota M1 N1 ) fmt.Println(L1, M1, N1) // 输出：0 1 2 const ( L2 = &quot;Joe&quot; M2 = iota N2 ) fmt.Println(L2, M2, N2) // 输出：Joe 1 2 const ( i = 1 &lt;&lt; iota // 1 * 2^iota j = 3 &lt;&lt; iota // 3 * 2^iota k l ) fmt.Println(i, j, k ,l) // 输出：1 6 12 24 const ( a1 = &apos;一&apos; b1 c1 = iota d1 ) fmt.Println(a1, b1, c1 ,d1) // 输出：19968 19968 2 3 const name = iota fmt.Println(&quot;name=&quot;, name) // 输出：name= 0&#125;]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>数据类型转换</tag>
        <tag>常量</tag>
        <tag>iota</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次从删库到恢复的经历]]></title>
    <url>%2F2018%2F12%2F31%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BB%8E%E5%88%A0%E5%BA%93%E5%88%B0%E6%81%A2%E5%A4%8D%E7%9A%84%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[公司正在做社交项目，涉及到的各个端人员比较多，开发、运维、产品、测试20多个，当然，对大公司来说，20其实不算多，进入正题，我们数据库使用的是MySQL，运维搭建的主从，而正是由于这个所谓的主从，害苦了开发的兄弟们；缓存使用的是Redis，MySQL数据同步到Redis使用的是阿里开源的Canal，了解Canal的都知道，Canal是读取MySQL的二进制文件，进而得到数据的变更，然后同步到Redis。 某天傍晚，后台开发兄弟们日常更新，由于产品要求用户的昵称支持输入表情，我们不得不修改MySQL的配置文件，使其支持表情，修改了Master配置文件，自然就需要修改Slave 的配置文件，将Master重启后，然后重启Slave，都重启后，发现master上的数据不能同步到slave了，我擦，不过由于项目刚刚起步，访问量不大，加上天色已晚，后台开发兄弟决定，不能同步就不能同步吧，明天找运维同事搞定。 第二天，运维同事了解情况后，连上服务器，一顿操作。下午三点左右， 产品正在演示APP，说APP首页没有数据了，我们说，呵呵，怎么可能，然后登上数据库后，我我我擦，master 用户表的数据被清空了，瞬间有点发毛，难道被攻击了，赶紧看下 slave，我*，比master上的数据都干净，而此时的master 里面的用户表 还有几条刚刚注册进来的几条数据，冷汗直流。缓存，对，看下缓存中，赶紧连上缓存服务器，疑问了，缓存里面 还有数据，master 用户表的数据没有了，而缓存又是读取的master里面的二进制文件，先不考虑这个问题，目前的紧急问题是赶紧恢复数据。由于开启了binlog，那就从这里恢复，一同事赶紧下载binlog，玛德，文件太大，拉下来 将近40分钟，这个方法暂缓。诶，缓存不是有数据嘛，那就把缓存里面的数据重新写到MySQL，赶紧写一个for循环，从 丢失的数据的 ID 往前读，然后一个个insert到数据库，当然，这种方法会导致数据有点旧，不过，和数据完全丢失比起来，那就毛毛雨了。终于，历经将近一小时，数据恢复。 数据恢复后，我们首要的目标就是查原因，原因无非就是两个，一是被攻击，二是内部人员所为。查看MySQL的二进制日志，数据丢失大约是在3点左右，而服务器是在曼谷，时间比北京时间晚一个小时，也就是服务器上的时间是2点左右，拉二进制文件找原因，呵，太慢了，几行命令搞定， 12345-- 查看某个时间段的二进制日志，并且输出到指定的文件mysqlbinlog --no-defaults --start-datetime=&quot;2018-12-12 13:00:00&quot; --stop-datetime=&quot;2018-12-12 14:40:00&quot; mysql-bin.000085 -vv --base64-output=decode-rows | more &gt;&gt; target.txt-- 将@1、@2等一系列看不懂的符号转换为SQL语句cat target.txt | sed -n &apos;/###/p&apos; | sed &apos;s/### //g;s/\/\*.*/,/g;s/DELETE FROM/INSERT INTO/g;&apos; | sed -r &apos;s/(@4.*),/\1;/g&apos; | sed &apos;s/@[0-9]*\=//g&apos; &gt; test.sql 然后将 test.sql 文件拉下来，查看，发现两处可疑点（即两条SQL），如下： 1234-- 创建一个像user一样的表，user_bakcreate table user_bak like user;-- 删除 user_bakdrop table user_bak; 但是，没有清空或者删除user 表的sql，奇了怪了。突然，技术总监过来问，有没有找到原因，我说还没有，只是找到了两条可疑的SQL，但是还没发现有清空user的SQL，因为一直在看master上的日志，因为是主从，看master 上的日志和看slave上的一样，然而，总监却说，slave上的日志也要查下，好吧，虽然觉得看了没什么用，谁让他是老大呢。登上slave服务器，却发现，为什么 slave 上也有二进制日志呢，先不管，看下日志，发现有如下SQL： 1234567891011-- 删除 userdrop table user;-- 创建usercreate table user( ...省略字段)COMMENT=&apos;用户表&apos;COLLATE=&apos;utf8_general_ci&apos;ENGINE=InnoDBAUTO_INCREMENT=4xxxxxx; 虽然打脸了，但是为什么slave上也会有二进制日志呢，赶紧找运维问下到底是不是主从，运维说是，算了，还是看下配置文件，呵呵，竟然是主主，不是说好的主从呢，怪不得在 slave上执行了 删除用户表，然后创建用户表，创建的下一个自增点还是丢失前的自增点，master上用户表的数据就不存在了，这样Redis中数据存在的原因也破解了，Redis 读取的是master中的二进制文件，正是由于从所谓的“从”删除然后重建user表后（记录到“从”的二进制日志），“主”上就有了user 表（记录到“主”的中继日志，并没有在这个“主”的二进制日志，所以 Canal根本没有读取到在“从”上的操作），原因是找到了，但是 是谁执行的呢？？当我们正在讨论的时候，运维说他在做测试，测试昨天的主从有没有正常了，他在修复昨天不能主从复制的问题。 崩溃，修复就修复，执行删除干嘛，而说好的主从，咋还变成主主了，问题 原因找到了，意味着不用加班到天亮了。 这种问题一定要杜绝： 非特殊情况，禁止使用ROOT账户； 相关人员分配MySQL账户，制定权限； 禁止执行不带条件的DELETE、UPDATE，TRUNCATE，DROP]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>删库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[根据IP定位地理位置]]></title>
    <url>%2F2018%2F12%2F26%2F%E6%A0%B9%E6%8D%AEIP%E5%AE%9A%E4%BD%8D%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[背景： 项目在海外运行，需要根据IP获取国家，城市，经纬度等信息，但是，百度地图、高德地图、淘宝等API的使用不了，而谷歌地图的又有频率限制，于是网上各种搜索，找到 GeoLiteCity.dat，GeoLiteCity.dat就好比一个本地的数据库文件，方法如下： 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.maxmind.geoip&lt;/groupId&gt; &lt;artifactId&gt;geoip-api&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 测试类如下： 12345678910111213141516171819public class IPTest &#123; public static void main(String[] args) &#123; try &#123; LookupService cl = new LookupService("C:\\GeoLiteCity.dat", LookupService.GEOIP_MEMORY_CACHE); Location l2 = cl.getLocation("128.1.35.120"); System.out.println( "countryCode: " + l2.countryCode +"\n"+ "countryName: " + l2.countryName +"\n"+ "region: " + l2.region +"\n"+ "city: " + l2.city +"\n"+ "latitude: " + l2.latitude +"\n"+ "longitude: " + l2.longitude); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果如下： 123456countryCode: THcountryName: Thailandregion: 40city: Bangkoklatitude: 13.753998longitude: 100.5014 项目中使用如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import com.maxmind.geoip.Location;import com.maxmind.geoip.LookupService;/** * @author: xbq * @Date: 2018/8/1 09:35 */@Servicepublic class LoadIp &#123; private LookupService cl; @PostConstruct public void init() &#123; try &#123; cl = new LookupService(systemConstants.getIpDb(), LookupService.GEOIP_MEMORY_CACHE); &#125; catch (IOException e) &#123; CusLogger.error("加载ip纯真库异常：" + e.getMessage(), e); &#125; &#125; /** * 使用 * @param ip * @return */ public void fun(String ip) &#123; // 根据ip来判定国家地区 if(cl != null) &#123; Location l2 = null; l2 = cl.getLocation(ip); if(l2 != null) &#123; // 获取国家编码 String countryCode = l2.countryCode; // 获取国家名称 String countryName = l2.countryName; // 获取城市 String city = l2.city; // 业务处理 ... &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Social</category>
      </categories>
      <tags>
        <tag>Social</tag>
        <tag>定位</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(5) - 打印格式化]]></title>
    <url>%2F2018%2F12%2F07%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-5-%E6%89%93%E5%8D%B0%E6%A0%BC%E5%BC%8F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[打印格式化中我们常常 需要用到的格式化的标记，每个标记实际来源于我们的单词，本文介绍Go语言中的打印格式化。 通用 %v：值的默认格式，对应英文为：value %T：值的类型，对应英文为：Type 布尔值%t：单词 true或者 false，对应英文：true 整型 %b：表示为二进制，对应英文：binary %c：该值对应的inicode码值，对应英文：char %d：表示为十进制，对应英文：digital %8d：表示该整型长度为8，不足8位，则在数值前补空格，超过8，则以实际为准 %08d：数字长度为8，不足8位，则在数值前补0，超过8，则以实际为准 %o：表示为八进制，对应英文：octal %q：该值对应的单引号 括起来的的Go语法字符字面值，必要时 会采用安全的转义表示 %x：表示为 十六进制，使用 a-f，对应英文：hex %X：表示为 十六进制，使用 A-F，对应英文：hex %U 表示为 unicode格式：U+1234，等价于“U+%04X” 浮点与复数 %f (=%.6f) 有6位小数部分，如果想保留两位小数，则 %.2f 即可（使用的是四舍五入） %e (=%.6e) 有6位小数部分的科学计数法，如果想保留两位小数的科学计数，则 %.2e 即可 字符串和byte[] %s 直接输出字符串 或者 byte[] %q 该值的对应的双引号括起来的Go语法字符串字面值，必要时采用安全的转义表示 说了这么多文字，不如来点实际的，上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package mainimport "fmt"type Student struct &#123; x , y int&#125;func main() &#123; // 通用 a := 12 fmt.Printf("%T , a = %v \n", a, a) // 相当于实例化 p := Student&#123;1,2&#125; fmt.Printf("%T , a = %v \n", p, p) var s rune = '一' fmt.Printf("%T , a = %v \n", s, s) // 布尔 b := true fmt.Printf("%T , b = %t \n", b, b) // 整数 c := 123 // %b 表示为 二进制 fmt.Printf("%T , c = %b \n", c, c) // %d 表示为 十进制 fmt.Printf("%T , c = %d \n", c, c) // %8d 表示该整型长度为8，不足8则在数值前补空格，如果超出8，以实际为准 fmt.Printf("%T , c = %8d \n", c, c) // %08d 数字长度为8，不足8在前面补零，超出8，以实际为准 fmt.Printf("%T , c = %08d \n", c, c) // %o 表示为 八进制 fmt.Printf("%T , c = %o \n", c, c) // %x 表示为 十六进制，使用 a-f fmt.Printf("%T , c = %x \n", c, c) // %x 表示为 十六进制，使用 A-F fmt.Printf("%T , c = %X \n", c, c) // %U 表示为 unicode格式：U+1234，等价于“U+%04X” fmt.Printf("%T , c = %U \n", c, c) cc := '一' fmt.Printf("%T , cc = %U \n", cc, cc) d := 97 // %c 对应的是 unicode码值 fmt.Printf("%T , d = %c \n", d, d) // %q 该值对应的单引号 括起来的的Go语法字符字面值，必要时 会采用安全的转义表示 fmt.Printf("%T , d = %q \n", d, d) // 浮点数 e := 123.23456 // %f (=%.6f) 有6位小数部分，如果想保留两位小数，则 %.2f 即可（使用的是四舍五入） fmt.Printf("%T , e = %f \n", e, e) fmt.Printf("%T , e = %.2f \n", e, e) // %e (=%.6e) 有6位小数部分的科学计数法，如果想保留两位小数的科学计数，则 %.2e 即可 fmt.Printf("%T , e = %e \n", e, e) fmt.Printf("%T , e = %.2e \n", e, e) // %E 科学计数法 fmt.Printf("%T , %E \n", e, e) // 字符串和byte[] f := "测试" // %s 直接输出字符串 或者 byte[] fmt.Printf("%T , f = %s \n", f, f) // %q 该值的对应的双引号括起来的Go语法字符串字面值，必要时采用安全的转义表示 fmt.Printf("%T , f = %q \n", f, f) arr := [3]byte&#123;97, 98, 99&#125; fmt.Printf("%T , arr = %s \n", arr, arr) arr2 := [3]byte&#123;'a', 'b', 'c'&#125; fmt.Printf("%T , arr2 = %s \n", arr2, arr2) fmt.Printf("%T , arr2 = %x \n", arr2, arr2) fmt.Printf("%T , arr2 = %X \n", arr2, arr2) // 变量赋值 g := fmt.Sprintf(f) fmt.Println("g==" + g)&#125; 运行结果如下： 123456789101112131415161718192021222324252627int , a = 12 main.Student , a = &#123;1 2&#125; int32 , a = 19968 bool , b = true int , c = 1111011 int , c = 123 int , c = 123 int , c = 00000123 int , c = 173 int , c = 7b int , c = 7B int , c = U+007B int32 , cc = U+4E00 int , d = a int , d = &apos;a&apos; float64 , e = 123.234560 float64 , e = 123.23 float64 , e = 1.232346e+02 float64 , e = 1.23e+02 float64 , 1.232346E+02 string , f = 测试 string , f = &quot;测试&quot; [3]uint8 , arr = abc [3]uint8 , arr2 = abc [3]uint8 , arr2 = 616263 [3]uint8 , arr2 = 616263 g==测试]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>打印格式化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(2) - HelloWorld]]></title>
    <url>%2F2018%2F11%2F26%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-2-HelloWorld%2F</url>
    <content type="text"><![CDATA[下载地址 https://golang.org/dl ，这个地址需要翻墙下载 https://studygolang.com/dl ，可直接在此网站下载，windows下载如下： 安装配置环境变量1.假设将安装包安装在D盘，新建 如下环境变量： GPROOT：Go的安装路径 GOPATH：Go的工程路径（如果有多个，就以分号分隔添加） 在PATH中增加：D:\Go\bin;%GOPATH%\bin; 注：需要把GOPATH中的可执行目录页配置到环境变量中，否则下载的第三方go工具就无法使用了。 2.查看是否安装成功 go env：查看得到的go的配置信息 go version：查看go的版本号 编译工具安装这里使用的编译编译工具是goland，比较方便好用，用惯了IDEA的，用这个很顺手。 1.下载goland，地址见百度网盘： 链接：https://pan.baidu.com/s/1xEUsFpnfjOAb9ceZ4IzcHA ，提取码：t5mt 。 2.安装 安装属于傻瓜式安装，一路next即可，破解方法也在百度网盘中。 Hello World12345678package mainimport &quot;fmt&quot;func main() &#123; /* 这是一个简单的程序 */ fmt.Println(&quot;Hello World&quot;)&#125; 1.第一行 package main 定义了包名。必须在源文件非注释的第一行指明这个文件属于哪个包，如：package main，表示一个科独立执行的程序，每个Go应用程序都包含一个名为main的包。 2.下一行 import “fmt”，告诉Go编译器这个程序需要使用fmt包，fmt包中实现了格式化IO的函数。 3.下一行的 func main() 是程序的入口。main函数式每一个可执行程序必须包含的，一般来说都是在启动后的第一个可执行函数，如果有 init() 函数，则先执行 init() 函数。 4.下一行/ … / 是注释，在程序执行时被忽略。 5.下一行 fmt.Println( .. ) 可以将字符串输出到控制台上，并在最后增加换行符 \n 。 编码规范注释 单行注释是最常见的注释形式，可以再任何地方使用以 // 开头的单行注释 多行注释也叫块注释，均已 / 开头，并以 / 结尾，且不可以嵌套使用，多行注释一般用于文档描述或注释成块的代码片段。 标识符 标识符是用来命名变量、类型等程序实体。一个标识符实际上就是一个或者多个字母数字、下划线组成的序列，但是第一个字符必须是以字母或者下划线，而不能是数字。 Go不允许在标识符中使用@、$和&amp;等标识符。 Go是一种区分大小写的语言。 空格 Go语言中变量的声明必须使用空格隔开，如：var age int 语句中适当使用空格可以让程序更简易阅读 在变量与运算符间增加空格，程序会更加美观。 语句的结尾 在Go程序中，一行代表一个语句结束，不用使用分号结尾 如果需要将多个语句写在一行，必须使用分号区分 可见性规则 Go语言中，使用大小来决定标识符（常量、变量。接口、类型、结构或者函数）是否可以被外部包所调用 以大写字母开头，表示可以被外部包的代码所调用，此时就类似于Java中加了public 以小写字母开头，则对包外是不可见的，就类型与Java中加了private Go程序结构组成Go一般程序 12345678910111213141516171819202122232425// 当前的包名package main// 导入包名import . &quot;fmt&quot;// 常量定义const PI = 3.14// 全局类型的声明和赋值var name = &quot;Joe&quot;// 一般类型声明type newType int// 结构的声明type Joe struct &#123;&#125;// 接口的声明type xbq interface &#123;&#125;// 由main函数作为程序入口点启动func main() &#123; Println(&quot;hello World&quot;)&#125; Go文件的基本组成 包声明 引入包 函数 变量 语句 &amp; 表达式 注释 Go文件结构组成 Go程序是通过 package 来组织的 只有 package 名称为 main 的包可以包含 main 函数 一个可执行程序有且仅有一个 main 包 通过 import 关键字来导入其他非main包 可以通过 import 关键字单个导入，也可以多个导入 程序一般由关键字、常量、变量、运算符、类型和函数组成 程序中可能会使用这些分隔符，括号、中括号 和 大括号 程序中可能会使用这些标点符号，点（.）、逗号（,）、分号（;）、冒号（:）、省略号（…） 通过在函数体外部使用 var 关键字来进行全局变量的声明和赋值 通过 type 关键字来进行结构（struct）和接口（interface）的声明 通过 func 关键字来进行函数的声明]]></content>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(4) - 基本数据类型]]></title>
    <url>%2F2018%2F11%2F26%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-4-%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Go语言中的数据类型包含两种： 基本数据类型（原生数据类型）：整型、浮点型、布尔型、字符串、字符（byte、rune） 复合数据类型（派生数据类型）：指针（pointer）、数组（array）、切片（slice）、映射（map）、函数（function）、结构体（struct）、通道（channel） 整型整型分为两大类 按长度分：int8、int16、int32、int64、int 无符号整型：uint8、uint16、uint32、uint64、uint 其中，uint8就是byte型、int16对应C语言中的short型、int64对应C语言中的long型 序号 类型和描述 1 uint8：无符号8位整型（0到255）【2的8次方】 2 uint16：无符号16位整型（0到65535）【2的16次方】 3 uint32：无符号32位整型（0到4292967295）【2的32次方】 4 uint64：无符号位64位整型（0到 18446744073709551615）【2的64次方】 5 int8：有符号8位整型（-128到127） 6 int16：有符号16位整型（-32768到32767） 7 uint32：有符号32位整型（-2147483648到2147483647） 8 uint64：有符号位64位整型（-9223372036854775808到 9223372036854775807） 还有其他数字类型 序号 类型和描述 1 byte 类似 uint8 2 rune 类似int32 3 uint 32或者64位，根据电脑机器的位数决定 4 int 与uint一样大小 5 uintptr 无符号整型，用于存放一个指针 当超过数据类型所要求的长度时，就会报错：constant xxx overflows byte。 字符下面看一下 byte 和 rune 到底是不是 我们上面描述的，分别 对应 uint8 和 int32，代码如下，我们还是打印出变量的类型 和 值： 12345678910111213package mainimport "fmt"func main() &#123; a := 100 var b byte = 100 var c rune = 200 fmt.Printf("%T %v \n", a , a) fmt.Printf("%T %v \n", b , b) fmt.Printf("%T %v \n", c , c)&#125; 输出结果如下： 123int 100 uint8 100 int32 200 事实证明，我们上面描述的是没有问题的。实际上，我们通常不会将 byte 和 rune 直接赋值为 数字类型，我们通常 赋值为 字符，类似于 Java 中的 char，如下： 1234var e byte = 'a'var f rune = 'A'fmt.Printf("%T %v \n", e , e)fmt.Printf("%T %v \n", f , f) 输出结果如下： 12uint8 97 int32 65 那么，byte 和 rune 有什么区别呢？我们先来看一个例子，假设 将 上面的 f 值 赋值为 汉字 大写的 一，那么 是怎么样的结果呢，然后将 e 的 值也赋值为 “一”，结果是什么呢？ 我们会发现，将 f 赋值为 “一”，输出 为：int32 19968；将 e 赋值为 “一”，会报一个异常：constant 19968 overflows byte，这是因为 byte 即 uint8 的最大值 是 255，而 “一” 对应的值 为 19968，远远大于 uint8 的最大值。 byte 和 rune 的区别如下： byte型：其实就是 uint8 的别名，代表了一个ASCLL 码的一个字符 rune型：其实就是 int32 ，代表了一个UTF-8字符，当需要处理中文等 unicode 字符集时就要用到 rune 类型 字符串字符串在Go语言中是以基本数据类型出现的。 在Go语言中，字符串 即可以单行定义，又可以多行定义，单行定义 不需要阐述，下面我们介绍下定义多行字符串 双引号书写字符串被称为字符串字面量，这种字面量不能跨行； 多行字符串需要使用 “·” 反引号（位于Tab键上面的一个），多用于内嵌源码 和 内嵌数据； 在反引号中的所有代码不会被编译器识别，而只是作为字符串的一部分 当我们想 将 一段代码作为 字符串输出的时候，我们 发现 要输出 就只能 调为一行，但Go语言给我们提供了 多行字符串，如下： 12345678910temp := ` a := 100 var b byte = 256 var c rune = 200 fmt.Printf("%T %v \n", a , a) fmt.Printf("%T %v \n", b , b) fmt.Printf("%T %v \n", c , c)`fmt.Println(temp) 输出结果如下： 1234567a := 100var b byte = 256var c rune = 200fmt.Printf(&quot;%T %v \n&quot;, a , a)fmt.Printf(&quot;%T %v \n&quot;, b , b)fmt.Printf(&quot;%T %v \n&quot;, c , c) 这样就可以讲 我们拷贝的一段代码 原样输出。 以上主要讲解了 基本数据类型，后续 会继续讲解 派生数据类型。]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(3) - 变量与初始化]]></title>
    <url>%2F2018%2F11%2F11%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-3-%E5%8F%98%E9%87%8F%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[变量的概念变量是计算机语言中存储数据的抽象概念，变量通过变量名访问 变量的本质是计算机分配的一小块内存，专门用于存放指定数据，在程序运行过程中该数值可以改变 变量的存储往往具有瞬时性、或者说是临时存储，当程序运行结束，存放该数据的内存就会释放，该变量就会消息 Go语言的变量名由数字、字母、下划线组成，首个字符不能是数字 声明变量声明变量有多种形式： 1.未初始化话的标准格式 1var 变量名 变量类型 2.未初始化的批量格式 不用每行都用 var 声明 12345678910var( a int b string c []float32 d func() bool e struct&#123; x int y string &#125;) 未初始化变量的默认值 整型和浮点型变量默认值为 0 字符串默认值为空字符串 布尔型默认值为false 函数、指针变量默认值为 nil 123456789101112131415161718192021222324package mainimport "fmt"func main() &#123; var( a int b string c []float32 d bool e []int f [3]int h int32 = 100 g func() string ) fmt.Printf("%T , % v \n", a , a) fmt.Printf("%T , % v \n", b , b) fmt.Printf("%T , % v \n", c , c) fmt.Printf("%T , % v \n", d , d) fmt.Printf("%T , % v \n", e , e) fmt.Printf("%T , % v \n", f , f) fmt.Printf("%T , % v \n", h , h) fmt.Printf("%T , % v \n", g , g)&#125; 输出如下： 12345678int , 0 string , []float32 , [] bool , false []int , [] [3]int , [ 0 0 0] int32 , 100 func() string , &lt;nil&gt; 3.初始化变量的标准格式 var 变量名 类型 = 表格式 4.初始化变量的编译器自动推断类型格式 var 变量名 = 表达式 5.初始化变量的简短声明格式（短变量声明格式） 变量名 := 表达式 使用 := 赋值操作符， := 可以高效的创建一个新的变量，称之为初始化声明 声明语句省略了 var 关键字 声明类型将由编译器自动推断 这是声明变量的首选方式，但是它只能被用在函数体内，而不可以用于全局变量的声明与赋值 该变量名必须是没有定义过的变量，若定义过，将发生编译错误 在多个短变量声明和赋值中，至少有一个新声明的变量出现在左侧中，那么即便有其他变量名可能是重复声明的，编译器也不会报错 变量多重赋值 Go语法中，变量初始化和变量赋值是两个不同的概念，Go语言的变量赋值与其他语言一样，但是Go提供了其他程序员期待已久的多重赋值功能，可以实现变量替换，多重赋值让Go语言比其他语言减少了代码量 如想要对 q 和 w 变量的值进行互换： 123456q := 10w := 20fmt.Println(q , " " , w)q, w = w, qfmt.Println(q , " " , w) 得到的结果为： 1210 2020 10 有了变量的多重赋值，就不用像Java中引入第三个变量，来进行两个变量值的互换。 匿名变量 Go语言的函数可以返回多个值，而事实上我们并不是对所有的返回值都用得上，那么就可以使用匿名变量，用“_”下划线替换即可。 匿名变量不占用命名空间，不会分配内存]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习(1) - 简介]]></title>
    <url>%2F2018%2F11%2F04%2FGo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-1-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Go语言的三个作者是：Robert Giresemer，Rob Pike 和 Ken Thompson Robert 在开发Go之前是Google V8、Chubby和HotSpot JVM的主要贡献者； Rob主要是Unix、UTF-8、plan 9的作者； Ken主要是B语言、C语言的作者、Unix之父。 Go语言的主要发展过程 2007年9月，Rob Pike正式命名为Go 2008年5月，Google全力支持该项目； 2009年11月，Go将代码全部开源，它获得了当年的年度语言； 2012年3月28日，Go发布第一个正式的稳定版 Go语言的特点1.设计Go语言是为了解决当时Google开发者遇到的问题 大量的 C++代码，同时又引入了Java和Python 成千上万行的代码 分布式的编译系统 数百万的服务器 2.Google开发中的痛点： 编译慢 失控的依赖 每个工程师只是用了一个语言里面的一部分 程序难以维护 交叉编译困难 3.如何解决当时的问题和痛点 Go希望成为互联网时代的C语言，因此，Go语言也是足够简单 设计Go语言的目标是为了消除各种缓慢和笨重、改进各种低效和扩展性 4.Go语言的特点 没有继承多态的面向对象 强一致性类型 interface不需要显示声明 没有异常处理 基于首字母的可访问特性 不用的import或者变量引起编译错误 完美而卓越的标准库包 Go语言的优势1.学习曲线容易 Go语言的语法简单，包含了类C语法。所以Go语言容易学习 2.快速的编译时间、开发效率和运行效率高 Go语言拥有接近C的运行效率和接近PHP的开发效率 3.出身名门、血统纯正 Go语言出自Google公司，Google对这个新的宠儿还是很看重的 4.自由高效：组合的思想、无侵入式的接口 Go语言可以说是开发效率和运行效率的完美结合。天生的并发编程支持，Go语言支持所有的编程范式，包含过程式编程、面向对象编程、面向接口编程、函数式编程 5.强大的标准库 包括互联网应用、系统编程和网络编程，它里面的标准库基本上非常稳定了 6.部署方便：二进制文件、拷贝部署 7.简单的并发 Go是一种非常高效的语言，高度支持并发性。Go是为大数据、微服务、并发而生的一种编程语言。 Go作为一门语言致力于使事情简单化，他并未引入很多新概念，而是聚焦于打造一门简单的语言，他使用起来异常快速和简单，其唯一的创新之处是goroutine和通道。Goroutines是Go面向线程的轻量级方法，而通道是goroutines之间通信的优先方式。 创建Goroutines的成本很低，只需要几千个字节的额外内存，正因为如此，才使得同时运行数百个甚至数千个goroutines成为可能。可以借助通道实现Gotoutines之间的通信。Gotoutines以及基于通道的并发性方法使其非常容易使用所有的CPU内核，并处理并发的IO。 8.稳定性 Go拥有强大的编译检查、严格的编码规范和完整的软件生命周期工具，具有很强的稳定性。Go提供了软件生命周期（开发、测试、部署、维护等等）的各个环节的工具，如：go tool、gofmt、go test。 Go语言的核心特性和优势Go主要有静态语言、天生并发、内置GC、安全性高、语法简单、编译快速这几个方面的特性，这些特性决定了Go的三个高富帅特性：运行快、开发快、部署快。 Go语言能开发什么 服务器编程，以前用C或者C++做的事情，用Go来做很合适，例如处理日志、数据打包、虚拟机处理、文件系统等。 分布式系统，数据库代理器等，例如：Etcd。 网络编程，包括Web应用，API应用，下载应用等 数据库操作 开发云平台 欢迎关注我的公众号，第一时间接收文章推送~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Go学习</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>简介</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[object '/usr/local/lib/libdns.so' from /etc/ld.so.preload cannot be preloaded: ignored.]]></title>
    <url>%2F2018%2F10%2F19%2Fobject-usr-local-lib-libdns-so-from-etc-ld-so-preload-cannot-be-preloaded-ignored%2F</url>
    <content type="text"><![CDATA[做了如下操作后： 12rm -rf xxx.jar kill -9 xx 重启Jar包，出现如下错误： 1ld.so: object &apos;/usr/local/lib/libdns.so&apos; from /etc/ld.so.preload cannot be preloaded: ignored. 不晓得什么原因，咨询了下运维，运维给出的方法是清空 /etc/ld.so.preload 文件的内容： 1echo "" &gt; /etc/ld.so.preload 果然，重启成功。 记录一下linux中快速清空文件内容的几种方法： 12345: &gt; filename &gt; filename echo "" &gt; filename echo &gt; filename cat /dev/null &gt; filename]]></content>
      <categories>
        <category>Exception</category>
      </categories>
      <tags>
        <tag>Exception</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysqldump导出完整sql脚本]]></title>
    <url>%2F2018%2F10%2F18%2Fmysqldump%E5%AF%BC%E5%87%BA%E5%AE%8C%E6%95%B4sql%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[#导出某个数据库－－结构+数据 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt db_name |gzip -9 &gt; /db_bakup/db_name.gz #导出某个数据库的表－－结构+数据+函数+存储过程 shell&gt;mysqldump –no-defaults -h192.168.161.124 -uroot -pxxxxxx –opt -R db_name |gzip -9 &gt; /db_backup/db_name.gz #导出多个数据库 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –databases db_name1 db_name2 db_name3 |gzip -9 &gt; /db_backup/mul_db.gz #导出所有的数据库 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –all-databases |gzip -9 &gt; /db_bak/all_db.gz #导出某个数据库的结构 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –no-data db_name|gzip -9 &gt; /db_bak/db_name.strcut.gz #导出某个数据库的数据 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –no-create-info db_name|gzip -9 &gt; /db_bak/db_naem.data.gz #导出某个数据库的某张表 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt db_name tbl_name |gzip -9 &gt; /db_bak/db_name.tal_name.gz # 导出某个数据库的某张表的结构 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –no-data db_name tal_name | gzip -9 &gt; /db_bak/db_name.tal_name.struct.gz #导出某个数据库的某张表的数据 shell&gt;mysqldump -h192.168.161.124 -uroot -pxxxxxx –opt –no-create-info db_name tbl_name | gzip -9 &gt; /db_bak/db_name.tbl_name.data.gz ##–opt==–add-drop-table + –add-locks + –create-options + –disables-keys + –extended-insert + –lock-tables + –quick + –set+charset ##默认使用–opt，–skip-opt禁用–opt参数]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>mysqldump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot中使用AOP统一处理Web请求日志]]></title>
    <url>%2F2018%2F10%2F12%2FSpring-Boot%E4%B8%AD%E4%BD%BF%E7%94%A8AOP%E7%BB%9F%E4%B8%80%E5%A4%84%E7%90%86Web%E8%AF%B7%E6%B1%82%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[本文由 程序猿DD-翟永超 创作，采用 CC BY 3.0 CN协议 进行许可。 可自由转载、引用，但需署名作者且注明文章出处 AOP为Aspect Oriented Programming的缩写，意为：面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。AOP是Spring框架中的一个重要内容，它通过对既有程序定义一个切入点，然后在其前后切入不同的执行内容，比如常见的有：打开数据库连接/关闭数据库连接、打开事务/关闭事务、记录日志等。基于AOP不会破坏原来程序逻辑，因此它可以很好的对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。 下面主要讲两个内容，一个是如何在Spring Boot中引入Aop功能，二是如何使用Aop做切面去统一处理Web请求的日志。 以下所有操作基于chapter4-2-2工程进行。 准备工作因为需要对web请求做切面来记录日志，所以先引入web模块，并创建一个简单的hello请求的处理。 pom.xml中引入web模块 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 实现一个简单请求处理：通过传入name参数，返回“hello xxx”的功能。 12345678910@RestControllerpublic class HelloController &#123; @RequestMapping(value = "/hello", method = RequestMethod.GET) @ResponseBody public String hello(@RequestParam String name) &#123; return "Hello " + name; &#125;&#125; 下面，我们可以对上面的/hello请求，进行切面日志记录。 引入AOP依赖在Spring Boot中引入AOP就跟引入其他模块一样，非常简单，只需要在pom.xml中加入如下依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 在完成了引入AOP依赖包后，一般来说并不需要去做其他配置。也许在Spring中使用过注解配置方式的人会问是否需要在程序主类中增加@EnableAspectJAutoProxy来启用，实际并不需要。 可以看下面关于AOP的默认配置属性，其中spring.aop.auto属性默认是开启的，也就是说只要引入了AOP依赖后，默认已经增加了@EnableAspectJAutoProxy。 1234# AOPspring.aop.auto=true # Add @EnableAspectJAutoProxy.spring.aop.proxy-target-class=false # Whether subclass-based (CGLIB) proxies are to be created (true) as opposed to standard Java interface-based proxies (false). 而当我们需要使用CGLIB来实现AOP的时候，需要配置spring.aop.proxy-target-class=true，不然默认使用的是标准Java的实现。 实现Web层的日志切面实现AOP的切面主要有以下几个要素： 使用@Aspect注解将一个java类定义为切面类 使用@Pointcut定义一个切入点，可以是一个规则表达式，比如下例中某个package下的所有函数，也可以是一个注解等。 根据需要在切入点不同位置的切入内容 使用@Before在切入点开始处切入内容 使用@After在切入点结尾处切入内容 使用@AfterReturning在切入点return内容之后切入内容（可以用来对处理返回值做一些加工处理） 使用@Around在切入点前后切入内容，并自己控制何时执行切入点自身的内容 使用@AfterThrowing用来处理当切入内容部分抛出异常之后的处理逻辑 12345678910111213141516171819202122232425262728293031@Aspect@Componentpublic class WebLogAspect &#123; private Logger logger = Logger.getLogger(getClass()); @Pointcut("execution(public * com.didispace.web..*.*(..))") public void webLog()&#123;&#125; @Before("webLog()") public void doBefore(JoinPoint joinPoint) throws Throwable &#123; // 接收到请求，记录请求内容 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); // 记录下请求内容 logger.info("URL : " + request.getRequestURL().toString()); logger.info("HTTP_METHOD : " + request.getMethod()); logger.info("IP : " + request.getRemoteAddr()); logger.info("CLASS_METHOD : " + joinPoint.getSignature().getDeclaringTypeName() + "." + joinPoint.getSignature().getName()); logger.info("ARGS : " + Arrays.toString(joinPoint.getArgs())); &#125; @AfterReturning(returning = "ret", pointcut = "webLog()") public void doAfterReturning(Object ret) throws Throwable &#123; // 处理完请求，返回内容 logger.info("RESPONSE : " + ret); &#125;&#125; 可以看上面的例子，通过@Pointcut定义的切入点为com.didispace.web包下的所有函数（对web层所有请求处理做切入点），然后通过@Before实现，对请求内容的日志记录（本文只是说明过程，可以根据需要调整内容），最后通过@AfterReturning记录请求返回的对象。 通过运行程序并访问：http://localhost:8080/hello?name=didi，可以获得下面的日志输出 1234562016-05-19 13:42:13,156 INFO WebLogAspect:41 - URL : http://localhost:8080/hello2016-05-19 13:42:13,156 INFO WebLogAspect:42 - HTTP_METHOD : http://localhost:8080/hello2016-05-19 13:42:13,157 INFO WebLogAspect:43 - IP : 0:0:0:0:0:0:0:12016-05-19 13:42:13,160 INFO WebLogAspect:44 - CLASS_METHOD : com.didispace.web.HelloController.hello2016-05-19 13:42:13,160 INFO WebLogAspect:45 - ARGS : [didi]2016-05-19 13:42:13,170 INFO WebLogAspect:52 - RESPONSE:Hello didi 优化：AOP切面中的同步问题在WebLogAspect切面中，分别通过doBefore和doAfterReturning两个独立函数实现了切点头部和切点返回后执行的内容，若我们想统计请求的处理时间，就需要在doBefore处记录时间，并在doAfterReturning处通过当前时间与开始处记录的时间计算得到请求处理的消耗时间。 那么我们是否可以在WebLogAspect切面中定义一个成员变量来给doBefore和doAfterReturning一起访问呢？是否会有同步问题呢？ 的确，直接在这里定义基本类型会有同步问题，所以我们可以引入ThreadLocal对象，像下面这样进行记录： 1234567891011121314151617181920212223242526@Aspect@Componentpublic class WebLogAspect &#123; private Logger logger = Logger.getLogger(getClass()); ThreadLocal&lt;Long&gt; startTime = new ThreadLocal&lt;&gt;(); @Pointcut("execution(public * com.didispace.web..*.*(..))") public void webLog()&#123;&#125; @Before("webLog()") public void doBefore(JoinPoint joinPoint) throws Throwable &#123; startTime.set(System.currentTimeMillis()); // 省略日志记录内容 &#125; @AfterReturning(returning = "ret", pointcut = "webLog()") public void doAfterReturning(Object ret) throws Throwable &#123; // 处理完请求，返回内容 logger.info("RESPONSE : " + ret); logger.info("SPEND TIME : " + (System.currentTimeMillis() - startTime.get())); &#125;&#125; 优化：AOP切面的优先级由于通过AOP实现，程序得到了很好的解耦，但是也会带来一些问题，比如：我们可能会对Web层做多个切面，校验用户，校验头信息等等，这个时候经常会碰到切面的处理顺序问题。 所以，我们需要定义每个切面的优先级，我们需要@Order(i)注解来标识切面的优先级。i的值越小，优先级越高。假设我们还有一个切面是CheckNameAspect用来校验name必须为didi，我们为其设置@Order(10)，而上文中WebLogAspect设置为@Order(5)，所以WebLogAspect有更高的优先级，这个时候执行顺序是这样的： 在@Before中优先执行@Order(5)的内容，再执行@Order(10)的内容 在@After和@AfterReturning中优先执行@Order(10)的内容，再执行@Order(5)的内容 所以我们可以这样子总结： 在切入点前的操作，按order的值由小到大执行 在切入点后的操作，按order的值由大到小执行 完整代码如下：以下是修改上文后项目中使用的完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import com.alibaba.fastjson.JSON;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;import org.springframework.web.context.request.RequestContextHolder;import org.springframework.web.context.request.ServletRequestAttributes;import javax.servlet.http.HttpServletRequest;import java.util.Enumeration;import java.util.HashMap;import java.util.Map;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * @date: 2018/10/9 19:39 * @description: 统一处理Web请求日志 */@Aspect@Order(5)@Componentpublic class WebLogAspect &#123; private static Logger logger = LoggerFactory.getLogger(WebLogAspect.class); ThreadLocal&lt;Long&gt; startTime = new ThreadLocal&lt;Long&gt;(); private static final String POST = "POST"; private static final String GET = "GET"; @Pointcut("execution(public * com.app.api..*.*(..))") public void webLog()&#123;&#125; @Before("webLog()") public void doBefore(JoinPoint joinPoint) throws Throwable &#123; // 接收到请求，记录请求内容 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); if(attributes == null) &#123; return; &#125; startTime.set(System.currentTimeMillis()); HttpServletRequest request = attributes.getRequest(); String method = request.getMethod(); Object[] args = joinPoint.getArgs(); String queryString = request.getQueryString(); String params = null; //获取请求参数集合并进行遍历拼接 if(args.length&gt;0)&#123; if(POST.equals(method))&#123; Object object = args[0]; //获取所有的请求参数 Map&lt;String, Object&gt; paramMap = new HashMap&lt;String, Object&gt;(); Enumeration&lt;String&gt; paraNames = request.getParameterNames(); for(Enumeration&lt;String&gt; e = paraNames;e.hasMoreElements();)&#123; String thisName = e.nextElement().toString(); String thisValue = request.getParameter(thisName); paramMap.put(thisName, thisValue); &#125; params = JSON.toJSONString(paramMap); &#125;else if(GET.equals(method))&#123; params = queryString; &#125; &#125; CusLogger.info("请求路径:" + request.getServletPath() + ",IP:" + request.getRemoteAddr() +",请求参数:" + params); &#125; @AfterReturning(returning = "ret", pointcut = "webLog()") public void doAfterReturning(Object ret) throws Throwable &#123; if(startTime.get() == null) &#123; return; &#125; // 处理完请求，返回内容 CusLogger.info("返回结果: " + ret + ",花费时间：" + (System.currentTimeMillis() - startTime.get()) + "ms"); &#125;&#125; 本文完整示例Chapter4-2-4]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子更新字段类]]></title>
    <url>%2F2018%2F10%2F07%2F%E5%8E%9F%E5%AD%90%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[原子更新某个类里的某个字段，Atomic包提供了以下3个类进行原子字段更新： AtomicIntegerFieldUpdater： AtomicLongFieldUpdater： AtomicStampedReference：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子更新引用类型]]></title>
    <url>%2F2018%2F10%2F07%2F%E5%8E%9F%E5%AD%90%E6%9B%B4%E6%96%B0%E5%BC%95%E7%94%A8%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Java原子操作的理解 原子更新引用类型包含3个类： AtomicReference：原子更新引用类型 AtomicReferenceFieldUpdater：原子更新引用类型里的字段 AtomicMarkableReference：原子更新带有标记位的引用类型。可以原子更新一个布尔类型的标记位和引用类型。构造方法是 AtomicMarkableReference（V inittialRef，boolean initialMark）。 AtomicReference示例： 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.concurrent.atomic.AtomicReference;/** * @author: xbq * @date: 2018/10/7 16:56 * @description: */public class AtomicReferenceTest &#123; public static AtomicReference&lt;User&gt; atomicUserRef = new AtomicReference&lt;User&gt;(); public static void main(String[] args) &#123; User initUser = new User(); initUser.setName("张山"); initUser.setOld(20); atomicUserRef.set(initUser); atomicUserRef.compareAndSet(initUser, new User("王五" ,21)); System.out.println(atomicUserRef.get().getName()); System.out.println(atomicUserRef.get().getOld()); &#125; /** * 用户实体 */ static class User &#123; private String name; private int old; public User() &#123;&#125; public User(String name, int old) &#123; this.name = name; this.old = old; &#125; // 省略 get set &#125;&#125; 运行结果如下： 12name：王五old：21 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[判断线程池中的线程是否全部执行完毕]]></title>
    <url>%2F2018%2F10%2F07%2F%E5%88%A4%E6%96%AD%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%98%AF%E5%90%A6%E5%85%A8%E9%83%A8%E6%89%A7%E8%A1%8C%E5%AE%8C%E6%AF%95%2F</url>
    <content type="text"><![CDATA[在使用多线程的时候有时候我们会使用 java.util.concurrent.Executors的线程池，当多个线程异步执行的时候，我们往往不好判断是否线程池中所有的子线程都已经执行完毕，但有时候这种判断却很有用，例如我有个方法的功能是往一个文件异步地写入内容，我需要在所有的子线程写入完毕后在文件末尾写“—END—”及关闭文件流等，这个时候我就需要某个标志位可以告诉我是否线程池中所有的子线程都已经执行完毕，我使用这种方式来判断。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MySemaphore &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; final File stream = new File("c:\\temp\\stonefeng\\stream.txt"); final OutputStream os = new FileOutputStream(stream); final OutputStreamWriter writer = new OutputStreamWriter(os); final Semaphore semaphore = new Semaphore(10); ExecutorService exec = Executors.newCachedThreadPool(); final long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) &#123; final int num = i; Runnable task = new Runnable() &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); writer.write(String.valueOf(num)+"\n"); semaphore.release(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; exec.submit(task); &#125; // 这里 exec.shutdown(); while(true)&#123; if(exec.isTerminated())&#123; writer.write("---END---\n"); writer.close(); System.out.println("所有的子线程都结束了！"); break; &#125; Thread.sleep(1000); &#125; // -------- final long end = System.currentTimeMillis(); System.out.println((end-start)/1000); &#125;&#125; 当调用ExecutorService.shutdown方法的时候，线程池不再接收任何新任务，但此时线程池并不会立刻退出，直到添加到线程池中的任务都已经处理完成，才会退出。在调用shutdown方法后我们可以在一个死循环里面用isTerminated方法判断是否线程池中的所有线程已经执行完毕，如果子线程都结束了，我们就可以做关闭流等后续操作了。 判断线程池中的线程是否全部执行完毕的另外一种解决方案则是使用闭锁(CountDownLatch)来实现，CountDownLatch是一种灵活的闭锁实现，它可以使一个或多个线程等待一组事件发生。闭锁状态包括一个计数器，该计数器被初始化为一个正数，表示需要等待的事件数量。countDown方法递减计数器，表示有一个事件已经发生了，而await方法等待计数器达到零，即表示需要等待的事情都已经发生。可以使用闭锁来这样设计程序达到目的： 1234567891011121314151617181920212223242526272829303132333435public class CountDownLatchApproach &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; final int nThreads = 10; // 这里 final CountDownLatch endGate = new CountDownLatch(nThreads); // -------- final File stream = new File("c:\\temp\\stonefeng\\stream.txt"); final OutputStream os = new FileOutputStream(stream); final OutputStreamWriter writer = new OutputStreamWriter(os); ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i &lt; nThreads; i++) &#123; final int num = i; Runnable task = new Runnable() &#123; @Override public void run() &#123; try &#123; writer.write(String.valueOf(num)+"\n"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 这里 endGate.countDown(); // -------- &#125; &#125; &#125;; exec.submit(task); &#125; // 这里 endGate.await(); // -------- writer.write("---END---\n"); writer.close(); &#125;&#125; 这种解决方案虽然可以达到目的但是性能差到没朋友，我更倾向于使用第一种方案。 现在我们有了更优雅的第三种方案，它的执行性能也不错。 1234567891011121314151617181920212223242526272829303132333435363738public class MySemaphore &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; final File stream = new File("c:\\temp\\stonefeng\\stream.txt"); final OutputStream os = new FileOutputStream(stream); final OutputStreamWriter writer = new OutputStreamWriter(os); final Semaphore semaphore = new Semaphore(10); ExecutorService exec = Executors.newCachedThreadPool(); final long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) &#123; final int num = i; Runnable task = new Runnable() &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); writer.write(String.valueOf(num)+"\n"); semaphore.release(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; exec.submit(task); &#125; // 这里 exec.shutdown(); exec.awaitTermination(1, TimeUnit.HOURS); // -------- writer.write("---END---\n"); writer.close(); final long end = System.currentTimeMillis(); System.out.println((end-start)/1000); &#125;&#125; 原文地址： 判断线程池中的线程是否全部执行完毕]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子更新数组]]></title>
    <url>%2F2018%2F10%2F07%2F%E5%8E%9F%E5%AD%90%E6%9B%B4%E6%96%B0%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Java原子操作的理解 通过原子的方式更新数组中的某个元素，Atomic包提供了以下3个类。 AtomicIntegerArray：原子更新整型数组里的元素。 AtomicLongArray ：原子更新长整型数组里的元素。 AtomicReferenceArray：原子更新引用类型数组里的元素。 AtomicIntegerArray类常用方法如下： int addAndGet（int i，int delta）：以原子方式将输入值与数组中索引 i 的元素相加 boolean compareAndSet（int i，int expect，int update）：如果当前值等于预期值，则以原子方式将数组位置i的元素设置成update值. AtomicIntegerArray示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.util.concurrent.atomic.AtomicIntegerArray;/** * @author: xbq * @date: 2018/10/7 10:58 * @description: */public class AtomicIntegerArrayTest &#123; static int[] value = new int[] &#123;1, 2&#125;; static AtomicIntegerArray array = new AtomicIntegerArray(value); public static void main(String[] args) &#123; System.out.println("---------------getAndSet---------------"); // 获取索引为0的值，并且赋值为3 System.out.println(array.getAndSet(0, 3)); System.out.println(array.get(0)); System.out.println("---------------addAndGet---------------"); // 获取索引为1的值 并且 加10 System.out.println(array.addAndGet(1, 10)); System.out.println(array.get(1)); System.out.println("-------------compareAndSet-----------------"); // 获取索引为1的值，和期望的值比较，若相等，就更新为 新值，否则，返回 false System.out.println(array.compareAndSet(1,12,20)); System.out.println(array.get(1)); System.out.println("-------------getAndIncrement-----------------"); // 获取索引为1的值，并且加1，返回 原值 System.out.println(array.getAndIncrement(1)); System.out.println(array.get(1)); System.out.println("-------------incrementAndGet-----------------"); // 获取索引为1的值，并且加1，返回 加1后的值 System.out.println(array.incrementAndGet(1)); System.out.println(array.get(1)); System.out.println("--------------decrementAndGet----------------"); // 获取索引为1的值，并且减1，返回减1 后的值 System.out.println(array.decrementAndGet(1)); System.out.println(array.get(1)); &#125;&#125; 结果如下： 123456789101112131415161718---------------getAndSet---------------13---------------addAndGet---------------1212-------------compareAndSet-----------------true20-------------getAndIncrement-----------------2021-------------incrementAndGet-----------------2222--------------decrementAndGet----------------2121 值得注意的是,数组value通过构造方法传递进去,然后AtomicIntegerArray会将当前数组复制一份,所以当AtomicIntegerArray对内部的数组元素进行修改时,不会影响传入的数组。 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子更新基本类型类]]></title>
    <url>%2F2018%2F10%2F06%2F%E5%8E%9F%E5%AD%90%E6%9B%B4%E6%96%B0%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Java原子操作的理解 当程序更新一个变量时，如果多线程同时更新这个变量。可能得到期望之外的值，比如变量 i=1，A线程更新 i+1，B线程也更新 i+1，最后得到的可能不是3，而是2。这是因为线程A和B在更新变量 i 的时候 拿到的 i 都是 1，这就是线程不安全的操作，通常我们会使用synchronized 来解决这个问题，synchronized 会保证多线程不会同时更新变量 i 。 而Java从JDK 1.5开始提供了java.util.concurrent.atomic包，这个包中的原子操作类提供了一种用法简单、性能高效、线程安全地一个变量的方式。 因为变量的类型有很多种，所以Atomic包里一共提供了13个类，属于4中类型的原子更新方式，分别是原子更新基本类型、原子更新数组、原子更新引用和原子更新属性（字段）。Atomic包里面的类基本都是使用Unsafe实现的包装类。 使用原子的方式更新基本类型，Atomic包提供了以下3个类。 AtomicBoolean：原子更新布尔类型 AtomicInteger：原子更新整型 AtomicLong：原子更新长整型 以上3个类提供的方法几乎一样。此处仅以AtomicInteger为例进行说明，其常用方法如下： int addAndGet（int dalta）：以原子方式见刚输入的数值与实例中的值（AtomicIntger里的value）相加，并返回结果。 boolean compareAndSet（ int expect， int update）：如果输入的数值等于期望值，则以原子方式将该值设置为输入的值。 int getAndIncrement（）：以原子方式将当前值加1，注意，这里返回的是自增前的值。 void lazySet（int newValue）：最终会设置成newValue，使用lazySet设置值后，可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 int getAndSet（ int newValue）：以原子方式设置为newValue的值，并返回旧值。 12345678910111213141516171819202122232425262728293031323334import java.util.concurrent.atomic.AtomicInteger;/** * @author: xbq * @date: 2018/10/6 18:31 * @description: */public class AtomicIntegerTest &#123; static AtomicInteger atomicInteger = new AtomicInteger(2); public static void main(String[] args) &#123; // addAndGet 以原子方式将输入的数值与实例中的数值相加，并返回结果 System.out.println("&gt;&gt;0&gt;&gt;&gt;" + atomicInteger.addAndGet(1)); // 如果输入的数值等于预期值，则以原子方式将该值设置为输入的值 System.out.println("&gt;&gt;1&gt;&gt;&gt;" + atomicInteger.compareAndSet(3, 8080)); // 获取值 System.out.println("&gt;&gt;2&gt;&gt;&gt;" + atomicInteger.get()); // 以原子方式将当前值加1，返回的值 为自增前的值 System.out.println("&gt;&gt;3&gt;&gt;&gt;" + atomicInteger.getAndIncrement()); // 获取值 System.out.println("&gt;&gt;4&gt;&gt;&gt;" + atomicInteger.get()); // 以原子方式将当前值加1，返回的值 为自增后的值 System.out.println("&gt;&gt;5&gt;&gt;&gt;" + atomicInteger.incrementAndGet()); // 获取值 System.out.println("&gt;&gt;6&gt;&gt;&gt;" + atomicInteger.get()); // 以原子方式设置为newValue的值，并返回旧值 System.out.println("&gt;&gt;7&gt;&gt;&gt;" + atomicInteger.getAndSet(86)); // 获取值 System.out.println("&gt;&gt;8&gt;&gt;&gt;" + atomicInteger.get()); // 最终会设置成newValue,使用lazySet设置值后,可能导致其他线程在之后的一小段时间内还是可以读到旧的值 atomicInteger.lazySet(10); System.out.println("&gt;&gt;9&gt;&gt;&gt;" + atomicInteger.get()); &#125;&#125; 输出结果如下： 12345678910&gt;&gt;0&gt;&gt;&gt;3&gt;&gt;1&gt;&gt;&gt;true&gt;&gt;2&gt;&gt;&gt;8080&gt;&gt;3&gt;&gt;&gt;8080&gt;&gt;4&gt;&gt;&gt;8081&gt;&gt;5&gt;&gt;&gt;8082&gt;&gt;6&gt;&gt;&gt;8082&gt;&gt;7&gt;&gt;&gt;8082&gt;&gt;8&gt;&gt;&gt;86&gt;&gt;9&gt;&gt;&gt;10 在JDK1.7中，AtomicInteger的getAndIncrement是这样的 ： 123456789101112public final int getAndIncrement() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return current; &#125;&#125;public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 而在JDK1.8中，是这样的： 123public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125; 可以看出，在JDK1.7中， 依靠的是我们熟悉的CAS算法，首先先循环获取当前值，然后对当前值加1 得到新值，对 当前值和新值进行比较并替换，成功的话返回当前值。而在JDK1.8中，直接使用了Unsafe的getAndAddInt 方法，在JDK1.7的Unsafe中，没有此方法。 JDK1.8中是对CAS算法的增强。 在Java的基本类型中除了Atomic包中提供原子更新的基本类型外，还有char、float和double。那么这些在Atomic包中没有提供原子更新的基本类型怎么保证其原子更新呢? 从AtomicBoolean源码中我们可以得到答案：首先将Boolean转换为整型，然后使用comareAndSwapInt进行CAS，所以原子更新char、float、double同样可以以此实现。 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的13个原子操作类]]></title>
    <url>%2F2018%2F10%2F06%2FJava%E4%B8%AD%E7%9A%8413%E4%B8%AA%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%E7%B1%BB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[使用融云发送消息]]></title>
    <url>%2F2018%2F10%2F01%2F%E4%BD%BF%E7%94%A8%E8%9E%8D%E4%BA%91%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[社交项目中难免会遇到发送消息，客户端发送消息暂时不作介绍，这里讲述的是Java服务端发送消息，其中，消息类型包括：单聊消息、系统消息和自定义消息。 当然，这些内容在融云官网上也有，这里只做记录以及遇到的坑。其中，这里涉及的API主要有：获取融云tokem、注册用户、更新用户、发送单聊消息、给多人发送消息、给所有用户发送消息、检查用户在线状态。 pom依赖1234567891011&lt;dependency&gt; &lt;groupId&gt;cn.rongcloud.im&lt;/groupId&gt; &lt;artifactId&gt;server-sdk-java&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.taimos&lt;/groupId&gt; &lt;artifactId&gt;httputils&lt;/artifactId&gt; &lt;version&gt;1.11&lt;/version&gt;&lt;/dependency&gt; IM接口定义1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import com.app.exception.CusException;import com.app.im.model.MediaMessage;import io.rong.messages.BaseMessage;/** * IM相关操作 */public interface IMService &#123; /** * 注册IM用户 * @param id * @param name * @param portrait * @return * @throws CusException */ boolean addUser(String id,String name,String portrait) throws CusException; /** * 修改IM用户信息 * @param id * @param name * @param portrait * @return * @throws CusException */ boolean updateUser(String id,String name,String portrait)throws CusException; /** * 单聊模块 发送文本、图片、图文消息 * @param fromId 发送人 Id * @param targetIds 接收人 Id * @param msg 消息体 * @param pushContent push 内容, 分为两类 内置消息 Push 、自定义消息 Push * @param pushData iOS 平台为 Push 通知时附加到 payload 中，Android 客户端收到推送消息时对应字段名为 pushData * @return * @throws CusException */ boolean sendPrivateMsg(String fromId, String[] targetIds,BaseMessage msg, String pushContent, String pushData) throws CusException; /** * 系统消息，发送给多人 * @param fromId 发送人 Id * @param targetIds 接收方 Id * @param msg 消息 * @param msg 消息内容 * @param pushContent push 内容, 分为两类 内置消息 Push 、自定义消息 Push * @param pushData iOS 平台为 Push 通知时附加到 payload 中，Android 客户端收到推送消息时对应字段名为 pushData * @return * @throws CusException */ boolean sendSystemMax100Msg(String fromId,String[] targetIds,BaseMessage msg,String pushContent,String pushData)throws CusException; /** * 发送消息给系统所有人 * @param fromId * @param msg * @param pushContent * @param pushData * @return * @throws CusException */ boolean sendSystemBroadcastMsg(String fromId, BaseMessage msg, String pushContent, String pushData)throws CusException; /** * 获取融云token * @param userId * @param name * @param portraitUri * @return * @throws CusException */ String getToken(String userId, String name, String portraitUri) throws CusException; /** * 单聊模块 发送自定义消息 * @param fromId 发送人 Id * @param targetIds 接收人 Id * @param msg 自定义 消息体 * @param pushContent 定义显示的 Push 内容，如果 objectName 为融云内置消息类型时，则发送后用户一定会收到 Push 信息 * @param pushData 针对 iOS 平台为 Push 通知时附加到 payload 中 * @return * @throws CusException */ boolean sendUserDefinedMsg(String fromId, String[] targetIds, MediaMessage msg, String pushContent, String pushData) throws CusException; /** * 检查用户在线状态方法 * 调用频率：每秒钟限 100 次 * @param userId * @return * @throws CusException */ Integer checkOnline(String userId) throws CusException;&#125; IM接口实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import com.app.exception.CusException;import com.app.im.model.MediaMessage;import com.app.im.service.IMService;import com.app.util.CusLogger;import de.taimos.httputils.HTTPRequest;import de.taimos.httputils.WS;import io.rong.RongCloud;import io.rong.messages.BaseMessage;import io.rong.methods.message.system.MsgSystem;import io.rong.models.Result;import io.rong.models.message.BroadcastMessage;import io.rong.models.message.PrivateMessage;import io.rong.models.message.SystemMessage;import io.rong.models.response.ResponseResult;import io.rong.models.response.TokenResult;import io.rong.models.user.UserModel;import org.apache.http.HttpResponse;import org.springframework.stereotype.Component;import javax.annotation.PostConstruct;import java.io.Reader;import java.security.MessageDigest;import java.util.HashMap;import java.util.Map;/** * IM相关操作 实现类 */@Componentpublic class IMServiceImpl implements IMService &#123; // 融云AppKey String appKey = "XXXXXXXXX"; // 融云AppSecret String appSecret = "XXXXXXXX"; RongCloud imClient; @PostConstruct public void init() &#123; imClient = RongCloud.getInstance(appKey, appSecret); &#125; @Override public boolean addUser(String id, String name, String portrait) throws CusException &#123; try &#123; UserModel user = new UserModel(id,name,portrait); TokenResult result = imClient.user.register(user); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("901","同步注册im用户出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public boolean updateUser(String id, String name, String portrait) throws CusException &#123; try &#123; UserModel user = new UserModel(id,name,portrait); Result result = imClient.user.update(user); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("902","同步更新im用户出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public boolean sendPrivateMsg(String fromId, String[] targetIds,BaseMessage msg, String pushContent, String pushData) throws CusException &#123; Reader reader = null ; PrivateMessage privateMessage = new PrivateMessage() .setSenderId(fromId) .setTargetId(targetIds) .setObjectName(msg.getType()) .setContent(msg) .setPushContent(pushContent) .setPushData(pushData) .setVerifyBlacklist(0) .setIsPersisted(0) .setIsCounted(0) .setIsIncludeSender(0); ResponseResult result = null; try &#123; result = imClient.message.msgPrivate.send(privateMessage); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("903","发送系统消息出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public boolean sendSystemMax100Msg(String fromId, String[] targetIds,BaseMessage msg, String pushContent, String pushData) throws CusException &#123; try &#123; MsgSystem system = imClient.message.system; SystemMessage systemMessage = new SystemMessage() .setSenderId(fromId) .setTargetId(targetIds) .setObjectName(msg.getType()) .setContent(msg) .setPushContent(pushData) .setPushData(pushData) .setIsPersisted(0) .setIsCounted(0) .setContentAvailable(0); ResponseResult result = system.send(systemMessage); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("903","发送系统消息出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public boolean sendSystemBroadcastMsg(String fromId,BaseMessage msg, String pushContent, String pushData) throws CusException &#123; try &#123; BroadcastMessage message = new BroadcastMessage() .setSenderId(fromId) .setObjectName(msg.getType()) .setContent(msg) .setPushContent(pushContent) .setPushData(pushData); ResponseResult result = imClient.message.system.broadcast(message); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("903","发送系统消息出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public String getToken(String userId, String name, String portraitUri) throws CusException &#123; try &#123; HTTPRequest req = WS.url("http://api.cn.ronghub.com/user/getToken.json"); Map&lt;String,String&gt; params = new HashMap&lt;String,String&gt;(); params.put("userId",userId); params.put("name",name); params.put("portraitUri",portraitUri); java.util.Random r= new java.util.Random(); String nonce = (r.nextInt(100000)+1)+""; String timestamp = System.currentTimeMillis()+""; String signature =string2Sha1(appSecret+nonce+timestamp); HttpResponse res = req.form(params).header("App-Key",appKey).header("Nonce",nonce).header("Timestamp",timestamp).header("Signature",signature).post(); String body = WS.getResponseAsString(res); JSONObject jo = JSONObject.parseObject(body); if(null!=jo &amp;&amp; jo.getInteger("code")==200)&#123; return jo.getString("token"); &#125;else&#123; new CusException("904","获取IM token 出现问题"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; return null; &#125; @Override public boolean sendUserDefinedMsg(String fromId, String[] targetIds, MediaMessage msg, String pushContent, String pushData) throws CusException &#123; Reader reader = null ; PrivateMessage privateMessage = new PrivateMessage() .setSenderId(fromId) .setTargetId(targetIds) .setObjectName(msg.getType()) .setContent(msg) .setPushContent(pushContent) .setPushData(pushData) .setCount("1") .setVerifyBlacklist(0) .setIsPersisted(0) .setIsCounted(0) .setIsIncludeSender(0); ResponseResult result = null; try &#123; // 发送单聊方法 result = imClient.message.msgPrivate.send(privateMessage); if(result.code == 200)&#123; return true; &#125;else&#123; throw new CusException("903","发送自定义单聊消息出错"); &#125; &#125; catch (Exception e) &#123; throw new CusException("99","系统异常"); &#125; &#125; @Override public Integer checkOnline(String userId) throws CusException &#123; HTTPRequest req = WS.url("http://api.cn.ronghub.com/user/checkOnline.json"); Map&lt;String,String&gt; params = new HashMap&lt;String,String&gt;(); params.put("userId",userId); java.util.Random r = new java.util.Random(); String nonce = (r.nextInt(100000)+1)+""; String timestamp = System.currentTimeMillis()+""; String signature =string2Sha1(appSecret + nonce + timestamp); HttpResponse res = req.timeout(3000).form(params).header("App-Key",appKey).header("Nonce",nonce).header("Timestamp",timestamp).header("Signature",signature).post(); String result = WS.getResponseAsString(res); Map&lt;String,Object&gt; resMap = JSON.parseObject(result, Map.class); Integer code = (Integer) resMap.get("code"); if(code != 200) &#123; CusLogger.error(userId + "调用是否在线接口结果为：" + result); return 2; &#125; String status = (String)resMap.get("status"); Integer resStatus = 0; if("0".equals(status)) &#123; resStatus = 0; &#125; else if("1".equals(status)) &#123; resStatus = 1; &#125; else &#123; resStatus = 2; &#125; return resStatus; &#125; private static String string2Sha1(String str)&#123; if(str==null||str.length()==0)&#123; return null; &#125; char hexDigits[] = &#123;'0','1','2','3','4','5','6','7','8','9', 'a','b','c','d','e','f'&#125;; try &#123; MessageDigest mdTemp = MessageDigest.getInstance("SHA1"); mdTemp.update(str.getBytes("UTF-8")); byte[] md = mdTemp.digest(); int j = md.length; char buf[] = new char[j*2]; int k = 0; for (int i = 0; i &lt; j; i++) &#123; byte byte0 = md[i]; buf[k++] = hexDigits[byte0 &gt;&gt;&gt; 4 &amp; 0xf]; buf[k++] = hexDigits[byte0 &amp; 0xf]; &#125; return new String(buf); &#125; catch (Exception e) &#123; // TODO: handle exception return null; &#125; &#125;&#125; 自定义消息实体1234567891011121314151617181920212223242526272829303132333435363738import io.rong.messages.BaseMessage;import io.rong.util.GsonUtil;public class MediaMessage extends BaseMessage &#123; // 自定义消息标志 private static final transient String TYPE = "go:media"; private String content = ""; // 以下是自定义参数 private String targetId ; private String sendId; private Long sendTime; private Long receptTime; private String userAge; private String userCount; public MediaMessage() &#123; &#125; public MediaMessage(String content) &#123; this.content = content; &#125; // 省略get set …………………… @Override public String getType() &#123; return "rx:media"; &#125; @Override public String toString() &#123; return GsonUtil.toJson(this, MediaMessage.class); &#125;&#125; 上面提到的坑就是发送自定义消息。和客户端定义的是发送JSON格式，那好，我就把定义好的JSON赋值到content中，然而，客户端获取到的值都为空，后面，一同事提示，试一下把定义的消息字段放到自定义实体中，我擦，真的可以了。虽然字段的值可以获取到了，但是 有些值获取到的不对，其中，这些字段的类型都是int 或者 Integer，定义的字段为 age和count，怀疑是 字段类型 或者 字段名 定义的不支持，于是，将 这两种都改掉，类型 改为String，字段 改为userAge和userCount，完美解决。 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Social</category>
      </categories>
      <tags>
        <tag>Social</tag>
        <tag>融云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cannal基本使用]]></title>
    <url>%2F2018%2F09%2F27%2Fcannal%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前提安装完MySQL（我安装的是5.7），安装JDK（canal依赖） 开启MySQL的binlog开启binlog，并且将binlog的格式改为Row，这样就可以获取到CURD的二进制内容。配置/etc/my.cnf，在[mysqld]增加 123log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=1 # 唯一，不能和其他集群MySQL的server_id一样 验证binlog是否开启登录MySQL，使用命令： 1show variables like &apos;log_%&apos;； 若 log_bin显示为 on ，则说明开启。 给canal分配MySQL的账号给canal分配一个MySQL的账号，方便canal偷取MySQL的binlog。 123CREATE USER canal IDENTIFIED BY &apos;canal&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;%&apos;;FLUSH PRIVILEGES; 查看是否给canal账号分配权限 1show grants for &apos;canal&apos; 下载解压canal地址：https://github.com/alibaba/canal/releases ，目前稳定版是 v1.1.0，下载 canal.deployer-1.1.0.tar.gz。解压到 canal目录下（没有该目录 就新建） 注：canal 是纯Java写的，所有需要依赖JDK环境，我这边使用的是：1.8.0_65-b17 123456# 下载wget https://github.com/alibaba/canal/releases/download/canal-1.1.0/canal.deployer-1.1.0.tar.gz# 创建canal目录mkdir canal# 解压tar -zxvf https://github.com/alibaba/canal/releases/download/canal-1.1.0/canal.deployer-1.1.0.tar.gz canal和instance配置文件一个canal里面可能会有多个instance，也就说一个instance可以监控一个mysql实例，多个instance也就可以对应多台服务器的mysql实例。也就是一个canal就可以监控分库分表下的多机器MySQL。 （1）canal.propertiescanal/config 中的canal.properties文件，是全局性的canal服务器配置 ，修改内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109########################################################## common argument ############# ################################################## id唯一，不可与mysql的server_id重复canal.id= 2 canal.ip=canal.port=11111canal.metrics.pull.port=11112canal.zkServers=# flush data to zkcanal.zookeeper.flush.period = 1000canal.withoutNetty = false# flush meta cursor/parse position to filecanal.file.data.dir = $&#123;canal.conf.dir&#125;canal.file.flush.period = 1000## memory store RingBuffer size, should be Math.pow(2,n)canal.instance.memory.buffer.size = 16384## memory store RingBuffer used memory unit size , default 1kbcanal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZEcanal.instance.memory.batch.mode = MEMSIZE## detecing configcanal.instance.detecting.enable = false#canal.instance.detecting.sql = insert into retl.xdual values(1,now()) on duplicate key update x=now()canal.instance.detecting.sql = select 1canal.instance.detecting.interval.time = 3canal.instance.detecting.retry.threshold = 3canal.instance.detecting.heartbeatHaEnable = false# support maximum transaction size, more than the size of the transaction will be cut into multiple transactions deliverycanal.instance.transaction.size = 1024# mysql fallback connected to new master should fallback timescanal.instance.fallbackIntervalInSeconds = 60# network configcanal.instance.network.receiveBufferSize = 16384canal.instance.network.sendBufferSize = 16384canal.instance.network.soTimeout = 30# binlog filter configcanal.instance.filter.druid.ddl = truecanal.instance.filter.query.dcl = falsecanal.instance.filter.query.dml = falsecanal.instance.filter.query.ddl = falsecanal.instance.filter.table.error = falsecanal.instance.filter.rows = falsecanal.instance.filter.transaction.entry = false# binlog format/image checkcanal.instance.binlog.format = ROW,STATEMENT,MIXED canal.instance.binlog.image = FULL,MINIMAL,NOBLOB# binlog ddl isolationcanal.instance.get.ddl.isolation = false# parallel parser configcanal.instance.parser.parallel = true## concurrent thread number, default 60% available processors, suggest not to exceed Runtime.getRuntime().availableProcessors()# parallelThreadSize默认是注释掉的，原值为16，因为canal装在本地VM上，分配了1个CPU，导致报错，改为1canal.instance.parser.parallelThreadSize = 1## disruptor ringbuffer size, must be power of 2canal.instance.parser.parallelBufferSize = 256# table meta tsdb infocanal.instance.tsdb.enable=truecanal.instance.tsdb.dir=$&#123;canal.file.data.dir:../conf&#125;/$&#123;canal.instance.destination:&#125;canal.instance.tsdb.url=jdbc:h2:$&#123;canal.instance.tsdb.dir&#125;/h2;CACHE_SIZE=1000;MODE=MYSQL;canal.instance.tsdb.dbUsername=canalcanal.instance.tsdb.dbPassword=canal# rds oss binlog accountcanal.instance.rds.accesskey =canal.instance.rds.secretkey =########################################################## destinations ############# #################################################canal.destinations= example# conf root dircanal.conf.dir = ../conf# auto scan instance dir add/remove and start/stop instancecanal.auto.scan = truecanal.auto.scan.interval = 5canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml#canal.instance.tsdb.spring.xml=classpath:spring/tsdb/mysql-tsdb.xmlcanal.instance.global.mode = spring canal.instance.global.lazy = false#canal.instance.global.manager.address = 127.0.0.1:1099#canal.instance.global.spring.xml = classpath:spring/memory-instance.xmlcanal.instance.global.spring.xml = classpath:spring/file-instance.xml#canal.instance.global.spring.xml = classpath:spring/default-instance.xml# position info，需要改成自己的数据库信息 canal.instance.master.address = 127.0.0.1:3306 canal.instance.master.journal.name =canal.instance.master.position =canal.instance.master.timestamp =# username/password，需要改成自己的数据库信息 canal.instance.dbUsername = canalcanal.instance.dbPassword = canalcanal.instance.defaultDatabaseName = testcanal.instance.connectionCharset = UTF-8 # table regex canal.instance.filter.regex = .*\\..* （2）instance.properties位于 canal/example/instance.properties，是具体的某个instances实例的配置，未涉及到的配置都会从canal.properties上继承，内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243################################################### mysql serverId , v1.0.26+ will autoGen # canal.instance.mysql.slaveId=0# enable gtid use true/falsecanal.instance.gtidon=false# position info address修改为自己的mysql地址canal.instance.master.address=192.168.204.128:3306canal.instance.master.journal.name=canal.instance.master.position=canal.instance.master.timestamp=canal.instance.master.gtid=# rds oss binlogcanal.instance.rds.accesskey=canal.instance.rds.secretkey=canal.instance.rds.instanceId=# table meta tsdb infocanal.instance.tsdb.enable=true#canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb#canal.instance.tsdb.dbUsername=canal#canal.instance.tsdb.dbPassword=canal#canal.instance.standby.address =#canal.instance.standby.journal.name =#canal.instance.standby.position = #canal.instance.standby.timestamp =#canal.instance.standby.gtid=# username/password 修改为在mysql中给canal同步数据的账号 密码canal.instance.dbUsername=canalcanal.instance.dbPassword=canal# 监听的数据库canal.instance.defaultDatabaseName=testcanal.instance.connectionCharset=UTF-8# table regexcanal.instance.filter.regex=.*\\..*# table black regexcanal.instance.filter.black.regex=################################################# 创建test数据库查看MySQL上是否有test数据库，没有则创建 开启canal进入canal/bin，执行：./startup.sh。 使用 ps -ef|grep canal 验证是否开启。 Java client代码创建SpringBoot工程，引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; 创建TestCanal类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package com.xbq.canal.test;import java.awt.Event;import java.net.InetSocketAddress;import java.util.List;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.protocol.CanalEntry.Column;import com.alibaba.otter.canal.protocol.CanalEntry.Entry;import com.alibaba.otter.canal.protocol.CanalEntry.EntryType;import com.alibaba.otter.canal.protocol.CanalEntry.EventType;import com.alibaba.otter.canal.protocol.CanalEntry.Header;import com.alibaba.otter.canal.protocol.CanalEntry.RowChange;import com.alibaba.otter.canal.protocol.Message;import com.google.protobuf.InvalidProtocolBufferException;/** * @Auther: xbq * @Date: 2018/9/11 19:16 * @Description: */public class TestCanal &#123; public static void main(String[] args) throws InterruptedException &#123; // 第一步：与canal进行连接 CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress("192.168.204.128", 11111), "example", "", ""); connector.connect(); // 第二步：开启订阅 connector.subscribe(); // 第三步：循环订阅 while (true) &#123; try &#123; // 每次读取 1000 条 Message message = connector.getWithoutAck(1000); long batchID = message.getId(); int size = message.getEntries().size(); if (batchID == -1 || size == 0) &#123; System.out.println("当前暂时没有数据"); Thread.sleep(1000); &#125; else &#123; System.out.println("-------------------------- 有数据啦 -----------------------"); PrintEntry(message.getEntries()); &#125; // position id ack （方便处理下一条） connector.ack(batchID); &#125; catch (Exception e) &#123; // TODO: handle exception &#125; finally &#123; Thread.sleep(1000); &#125; &#125; &#125; /** * 获取每条打印的记录 * @param entrys */ public static void PrintEntry(List&lt;Entry&gt; entrys) &#123; for (Entry entry : entrys) &#123; // 第一步：拆解entry 实体 Header header = entry.getHeader(); EntryType entryType = entry.getEntryType(); // 第二步： 如果当前是RowData，那就是我需要的数据 if (entryType == EntryType.ROWDATA) &#123; String tableName = header.getTableName(); String schemaName = header.getSchemaName(); RowChange rowChange = null; try &#123; rowChange = RowChange.parseFrom(entry.getStoreValue()); &#125; catch (InvalidProtocolBufferException e) &#123; e.printStackTrace(); &#125; EventType eventType = rowChange.getEventType(); System.out.println(String.format("当前正在操作 %s.%s， Action= %s", schemaName, tableName, eventType)); // 如果是‘查询’ 或者 是 ‘DDL’ 操作，那么sql直接打出来 if (eventType == EventType.QUERY || rowChange.getIsDdl()) &#123; System.out.println("rowchange sql -----&gt;" + rowChange.getSql()); return; &#125; // 第三步：追踪到 columns 级别 rowChange.getRowDatasList().forEach((rowData) -&gt; &#123; // 获取更新之前的column情况 List&lt;Column&gt; beforeColumns = rowData.getBeforeColumnsList(); // 获取更新之后的 column 情况 List&lt;Column&gt; afterColumns = rowData.getAfterColumnsList(); // 当前执行的是 删除操作 if (eventType == EventType.DELETE) &#123; PrintColumn(beforeColumns); &#125; // 当前执行的是 插入操作 if (eventType == EventType.INSERT) &#123; PrintColumn(afterColumns); &#125; // 当前执行的是 更新操作 if (eventType == EventType.UPDATE) &#123; PrintColumn(afterColumns); &#125; &#125;); &#125; &#125; &#125; /** * 每个row上面的每一个column 的更改情况 * @param columns */ public static void PrintColumn(List&lt;Column&gt; columns) &#123; columns.forEach((column) -&gt; &#123; String columnName = column.getName(); String columnValue = column.getValue(); String columnType = column.getMysqlType(); // 判断 该字段是否更新 boolean isUpdated = column.getUpdated(); System.out.println(String.format("columnName=%s, columnValue=%s, columnType=%s, isUpdated=%s", columnName, columnValue, columnType, isUpdated)); &#125;); &#125;&#125; 运行此类。在MySQL test数据库中创建student表，对其进行增删改，可以发现控制台上打印：有数据库啦…… 参考[缓存一致性和跨服务器查询的数据异构解决方案canal 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>canal</category>
      </categories>
      <tags>
        <tag>canal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社交项目 -- 汇总]]></title>
    <url>%2F2018%2F09%2F23%2F%E7%A4%BE%E4%BA%A4%E9%A1%B9%E7%9B%AE-%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[App视频邀请]]></title>
    <url>%2F2018%2F09%2F23%2FApp%E8%A7%86%E9%A2%91%E9%82%80%E8%AF%B7%2F</url>
    <content type="text"><![CDATA[需求项目中有这么一个需求： 当用户余额不足，1分钟后，机器人进行视频邀请，当用户点击接听时，则提示用户充值；当用户点击拒绝，3分钟后，再对该用户使用机器人进行视频邀请，当用户点击接听时，则提示用户充值；当用户点击拒绝，10分钟后，再次对该用户使用机器人进行视频邀请，当用户点击接听时，则提示用户充值；当用户点击拒绝，3次诱导充值结束。 当用户余额充足，1分钟后，推荐真实用户对该用户进行视频邀请，若该用户接听，则对真实用户发送视频邀请；当用户挂断，3分钟后，继续推荐真实用户进行视频邀请，若该用户接听，则对真实用户发送视频邀请，当用户挂断，10分钟后，继续推荐真实用户进行视频邀请。 当用户余额不够时，继续走余额不够的逻辑。 分析这个需求，难点无非就是三次时间间隔，开始考虑的是使用消息队列RocketMQ，但用RocketMQ有点大材小用的意思。后面考虑用Redis，如果Redis有对过期时间的监听，那岂不美哉，我擦，谷歌了一发，还真TM有。于是，就研究了一发，也是比较简单。 Redis对过期时间的监听是这样的：使用String类型，设置Key-Value，对该Key设置过期时间，当时间过期后，触发某个事件，这就是所谓的 对过期事件的监听。过期事件是通过Redis的发布订阅功能来进行分发。 事件类型对于每个修改数据库的操作，键空间通知都会发送两种不同类型的事件消息：keyspace 和 keyevent。以 keyspace 为前缀的频道被称为键空间通知（key-space notification）， 而以 keyevent 为前缀的频道则被称为键事件通知（key-event notification）。 事件是用 keyspace@DB:KeyPattern 或者 keyevent@DB:OpsType 的格式来发布消息的。DB表示在第几个库；KeyPattern则是表示需要监控的键模式（可以用通配符，如：key:）；OpsType则表示操作类型。因此，如果想要订阅特殊的Key上的事件，应该是订阅keyspace。比如说，对 0 号数据库的键 mykey 执行 DEL 命令时， 系统将分发两条消息， 相当于执行以下两个 PUBLISH 命令：PUBLISH keyspace@0:sampleKey delPUBLISH keyevent@0:del sampleKey订阅第一个频道 keyspace@0:mykey 可以接收 0 号数据库中所有修改键 mykey 的事件，而订阅第二个频道 keyevent@0:del 则可以接收 0 号数据库中所有执行 del 命令的键。 开启配置键空间通知通常是不启用的，因为这个过程会产生额外消耗。所以在使用该特性之前，请确认一定是要用这个特性的，然后修改配置文件，或使用config配置。相关配置项如下： 输入的参数中至少要有一个 K 或者 E ， 否则的话， 不管其余的参数是什么， 都不会有任何通知被分发。上表中斜体的部分为通用的操作或者事件，而黑体则表示特定数据类型的操作。在redis的配置文件redis.conf中修改 notify-keyspace-events “Kx”，注意：这个双引号是一定要的，否则配置不成功，启动也不报错。例如，“Kx”表示想监控某个Key的失效事件。也可以在命令行通过config配置：CONFIG set notify-keyspace-events Ex （但非持久化）。 实现步骤 修改redis.conf配置文件中的 notify-keyspace-events “Kx”，redis默认是关闭的 对SpringBoot整合 Redis的发布订阅，指定监听类和监听类型 代码示例pom依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; redis工具类（部分）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.ValueOperations;import org.springframework.stereotype.Component;import java.util.concurrent.TimeUnit;/** * redis缓存客户端 */@Componentpublic class RedisCacheUtils&lt;T&gt; &#123; @Autowired private RedisTemplate&lt;String, T&gt; redisTemplate; /** * 写入单个对象到缓存(可以设置有效时间) * @param key * @param value * @param expireTime 有效时间 单位秒 * @return */ public boolean set(final String key, T value, Long expireTime) &#123; boolean result = false; try &#123; ValueOperations&lt;String, T&gt; operations = redisTemplate.opsForValue(); operations.set(key, value); redisTemplate.expire(key, expireTime, TimeUnit.SECONDS); result = true; &#125; catch (Exception e) &#123; throw e; &#125; return result; &#125; /** * 自增 * @param key * @param by * @param seconds * @return */ public Long incr(final String key, final long by,final long seconds) &#123; Long count = redisTemplate.opsForValue().increment(key, by); redisTemplate.expire(key, seconds, TimeUnit.SECONDS); return count; &#125;&#125; 监听配置1234567891011121314151617181920212223242526272829303132import com.app.common.constants.SystemConstant;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.listener.ChannelTopic;import org.springframework.data.redis.listener.RedisMessageListenerContainer;@Configurationpublic class RedisLinstenerConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; @Bean public ConsumerRedisListener consumerRedis() &#123; return new ConsumerRedisListener(); &#125; @Bean public ChannelTopic topic() &#123; return new ChannelTopic("__keyevent@0__:expired"); &#125; @Bean public RedisMessageListenerContainer redisMessageListenerContainer() &#123; RedisMessageListenerContainer container = new RedisMessageListenerContainer(); container.setConnectionFactory(redisConnectionFactory); container.addMessageListener(consumerRedis(),topic()); return container; &#125;&#125; redis监听器： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import com.app.cache.RedisCacheUtils;import org.apache.commons.lang3.StringUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.connection.Message;import org.springframework.data.redis.connection.MessageListener;import org.springframework.data.redis.core.StringRedisTemplate;public class ConsumerRedisListener implements MessageListener &#123; @Autowired private StringRedisTemplate stringRedisTemplate; @Autowired private RedisCacheUtils redisCacheUtils; @Override public void onMessage(Message message, byte[] pattern) &#123; doBusiness(message); &#125; /** * 打印 message body 内容 * @param message */ public void doBusiness(Message message) &#123; Object value = stringRedisTemplate.getValueSerializer().deserialize(message.getBody()); byte[] body = message.getBody(); byte[] channel = message.getChannel(); String topic = new String(channel); String itemValue = new String(body); System.out.println("itemValue-----------------------" + itemValue); // 如果key中包含^，则说明是 视频邀请的 if(itemValue.contains("^")) &#123; String[] keyArr = itemValue.split("\\^"); String userId = keyArr[1]; // 防止重复消费，设置一个过期时间 Long num = redisCacheUtils.incr(userId + "_incr", 1L, 60L); if(StringUtils.isBlank(userId)) &#123; return; &#125; if(num == 1)&#123; // 处理逻辑，给App推送消息，调起视频呼叫 //………… &#125; &#125; &#125;&#125; 看了上面的代码可能有点懵，貌似和上述所说的时间间隔并没有什么瓜葛，然而并不是。首先，当用户当日首次登陆App时，客户端用调用一个接口，表示用户进入App，我会在接口中判断用户是不是当日首次登陆，如果是，则使用”video” + “^” + 用户的ID + “^” + 180 作为一个Key，value无所谓，并对该key设置60秒的过期时间，当该key过期，则会进入到redis监听中，并对客户端推送消息，其中，消息体中包含一个关键字段，此关键字段就是下次需要间隔多久来发起视频邀请，即之前过期Key后面跟随的180，当客户端点击挂断，调用挂断接口时，就将此字段传过来，然后 使用”video” + “^” + 用户的ID + “^” + 600 作为一个Key，并对该key设置180秒的过期时间，后面逻辑同理…… 然而，因为项目是分布式项目，会部署多个节点，这样就存在重复订阅，因为这一部分数据老大要求不能存到数据库，所以使用了redis 的incr来记录进入过期监听器的次数，并设置过期时间为60秒，这样 多个节点即使重复订阅，也会只有一个订阅者可以处理逻辑，即对客户端推送消息，这里的推送消息使用的是融云的IM，后续对该IM进行分析。 欢迎关注我的公众号~ 搜索公众号： 翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Social</category>
      </categories>
      <tags>
        <tag>Social</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发 -- JMM]]></title>
    <url>%2F2018%2F07%2F29%2FJava%E5%B9%B6%E5%8F%91-JMM%2F</url>
    <content type="text"><![CDATA[Java内存模型的基础 文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Java内存模型的理解 并发编程模型的两个关键问题 线程之间如何通信 线程之间如何同步 上面所说的线程指的是并发执行的活动实体。 线程之间的通信机制有两种：共享内存和消息传递 在共享内存的并发模型中，线程之间共享程序的公共状态，通过写-读内存中的公共状态进行隐式通信 在消息传递的并发模型中，线程之间没有公共状态，必须通过发送消息来显式进行通信 同步无非就是控制不同线程的执行顺序。在共享内存的并发模型中，同步是显式进行的，程序员必须显式的指定某个方法或者某段代码需要在线程之间互斥执行。在消息传递的并发模型中，同步是隐式的，因为消息的发送必须在消息的接收之间。 Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行的，整个通信过程对程序员完全透明。 Java内存模型的抽象结构在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享。局部变量和异常处理器参数不会在线程之间共享，它们不会有内存可见性问题，也不会受内存模型的影响。 Java线程之间的通信由Java内存模型（JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。 抽象来说，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程以读/写共享变量的副本，本地内存只是JMM的一个抽象概念，并不真实的存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 JMM的抽象示意图如下： 从上图来看，如果线程A和线程Ｂ要通信的话，需要经过两个步骤： 线程Ａ把本地内存Ａ中更新过的共享变量刷新到主内存中去 线程Ｂ到主内存中去读取线程Ａ之前已更新过的共享变量 JMM通过控制主内存和每个线程的本地内存之间的交互，来为Java程序实现内存可见性的保证。 JMM和JVM的区别JMM中的主内存、工作内存与JVM中的Java堆、栈、方法区等并不是同一个层次的内存划分，这两者基本上是没有关系的，如果两者一定要勉强对应起来，那从变量、主内存、工作内存的定义来看，主内存主要对应于Java堆中的对象实例数据部分，而工作内存则对应于虚拟机栈中的部分区域。从更底层次上说，主内存就直接对应于物理硬件的内存，而为了获取更好的运行速度，虚拟机（甚至是硬件系统本身的优化措施）可能会让工作内存优先存储于寄存器和高速缓存中，因为程序运行时主要访问读写的是工作内存。 欢迎关注我的公众号哦~搜索公众号：翻身码农把歌唱 或者 扫描下方二维码：]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
        <tag>JMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL -- 行锁]]></title>
    <url>%2F2018%2F07%2F24%2FMySQL-%E8%A1%8C%E9%94%81%2F</url>
    <content type="text"><![CDATA[一、行锁概念及特点1.概念：给单独的一行记录加锁，主要应用于innodb表存储引擎 2.特点：在innodb存储引擎中应用比较多，支持事务、开销大、加锁慢；会出现死锁；锁的粒度小，并发情况下，产生锁等待的概率比较低，所以支持的并发数比较高。 二、数据库事务1.概念：事务是一系列操作组成的工作单元，该工作单元内的操作是不可分割的，也就是说要么全部都执行，要么全部不执行。 2.特性：ACID 原子性：事务是最小的工作单元，不可分割，要么都做，要么都不做 一致性：事务执行前和执行后的数据要保证正确性，数据完整性没有被破坏。 隔离性：在并发事务执行的时候，一个事务对其他事务不会产生影响。 持久性：一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的 三、多个事务并发执行 问题及解决方案1.问题 丢失更新：在没有事务隔离的情况下，两个事务同时更新一条数据，后一个事务 会 覆盖前面事务的更新，导致前面的事务丢失更新。 脏读：事务A先更新数据，但是没有提交，事务B读到了事务A没有提交的数据。 不可重复读：事务A中，先读到一条数据，事务A还没有结束，此时，事务B对该条数据进行了修改操作，事务A又读到了这条数据，事务A两次读到的数据不同。 幻读：事务A先读到一批数据，假设读到10条，事务B插入了一条数据，此时，事务A又读这一批数据，发现多了一条，好像幻觉一样。 注：不可重复读的重点是修改，同样的条件，你读取过的数据，再次读取出来发现值不一样。 幻读的重点在于新增或者删除，同样的条件，第 1 次和第 2 次读出来的记录数不一样。 2.解决方案–数据库隔离机制 未提交读（read uncommitted）：这是数据库最低的隔离级别，允许一个事务读另一个事务未提交的数据。 解决了丢失更新，但是会出现脏读、不可重复读、幻读。 提交读（read committed）：一个事务更新的数据 在提交之后 才可以被另一个事务读取，即一个事务不可以读取到另一个事务未提交的数据。 解决了丢失更新和脏读，但是会出现不可重复读和幻读。 可重复读（repeatale read）：这是数据库默认的事务隔离级别，保证一个事务在相同条件下前后两次读取的数据是一致的。 解决了丢失更新、脏读和不可重复读，但是会出现幻读。 序列化（serializable）：这是数据库最高的隔离级别。事务串行执行，不会交叉执行。 解决了所有的问题。 注：乐观所可以解决幻读。 四、行锁的特性查看mysql事务隔离级别：show variables like ‘tx_iso%’; 前提：set autocommit=0; // 设置自动提交事务为手动提交 123456789/* 行锁案例*/create table lock_two( id int, col int)engine=innodb;insert into lock_two(id,col) values (1,1);insert into lock_two(id,col) values (2,2);insert into lock_two(id,col) values (3,3); 1.在session1中执行update : update lock_two set col=11 where id=1; （1）分别在session1和session2中查询lock_two，看id为1的记录的col是否修改了。 发现session1 的记录修改了，session2中的记录没有被修改。 （2）在session1中执行commite后，然后再在session2中查询： 发现session2中的表数据改变了。 2.在session1中执行update：update lock_two set col=11 where id=1，不执行commit; 在session2中执行uodate ：update lock_two set col=11 where id=1，不执行commit; 发现session2中的update发生阻塞，并且超过一段时间报错。 3.在session1中执行update：update lock_two set col=22 where id = 2; 不执行commit 在session2中执行另一条update：update lock_two set col=33 where id = 3; 此时，session2中的update发生阻塞，在没发生错误的情况下，session1执行commit，session2中的update会马上执行。 4.在lock_two中创建索引， 12create index idx_id on lock_two(id);create index idx_col on lock_two(col); 然后重复第3步， 发现session2可以更新，不会产生阻塞。因为用上了索引，相当于行锁。 结论：如果没有用上索引，行锁变成表锁 五、手动锁一行记录格式12begin;select * from lock_two where id=2 for update; 在session1中执行上面语句，在ssesion2中可以查看，但是不可以修改 sesion1中的for update 的记录。 当session1中执行commit后，seesion2中的update立刻执行。 六、间隙锁1.定义：在范围查找的情况下， innodb会给范围条件中的数据加上锁，无论数据是否是否真实存在。 2.例子： 在session1中update：update lock_two set col=666 where id&gt;2 and id&lt;8; 1) 在session2中执行insert：insert into lock_two values(9,99); 插入执行成功！ 2) 在session2中执行insert：insert into lock_two values(7,77); 插入阻塞，一段时间后报错！ 执行select：select * from lock_two where id=4; 查询成功！ 建议：在innodb中，因为有间隙锁的存在，最好在where中少使用这种范围查找。 七、查看行锁的信息show status like ‘innodb_row_lock%’; 说明： Innodb_row_lock_current_waits ：当前正在等待的数量 Innodb_row_lock_time: 从启动到现在锁定的总时长，单位是ms Innodb_row_lock_time_avg :锁等待的平均时长 Innodb_row_lock_time_max：等待锁时间最长的一个时间 Innodb_row_lock_waits：总共的等待次数]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>行锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL -- 表锁]]></title>
    <url>%2F2018%2F07%2F24%2FMySQL-%E8%A1%A8%E9%94%81%2F</url>
    <content type="text"><![CDATA[前言数据库的锁主要用来保证数据的一致性的。MyISAM存储引擎只支持表锁，InnoDB存储引擎既支持行锁，也支持表锁，但默认情况下是采用行锁。 一、锁分类1.按照对数据操作的类型分：读锁，写锁 读锁：也称为共享锁。 针对同一资源，多个并发读操作可以并行执行，并且互不影响，但是不能写 写锁：也称排它锁。当前线程写数据的时候，会阻塞其它线程来读取数据 或者 写数据 注：读锁和写锁都是阻塞锁。 2.按照数据操作的粒度：表锁，行锁，页锁 表锁：开销小，加锁快，主要在myisam存储引擎中出现。特点：锁住整个表，开销小，加锁快，无死锁情况， 锁的粒度大，在并发情况下，产生锁等待的概率比较高，所以说，支持的并发数比较低，一般用于查找 行锁：开销大，加锁慢，锁定单独的某个表中的某一行记录，主要用于innodb存储引擎。特点：有死锁情况，锁定粒度最小，发生锁冲突的概率最低，支持的并发数也最高 页锁：开销和加锁时间界于表锁和行锁之间。会出现死锁，锁定粒度界于表锁和行锁之间，并发度一般 二、加锁与解锁1.手动增加表锁lock table 表名 [read|write]，表名 [read|write]… 2.解锁unlock tables; 3.查看哪些表被锁show open tables; 三、表锁案例1.读锁12345678create table lock_one( id int primary key auto_increment, col int)engine=myisam;insert into lock_one(col) values (1);insert into lock_one(col) values (2);insert into lock_one(col) values (3); 下面我们模拟两个用户，即两个线程连接数据库，开启两个xsheel窗口，连接到mysql： 1) 在会话1中对lock_one表增加读锁 lock table lock_one read; 2) 在当前会话（会话1）中是否可以select该表呢，也就是说对 lock_one增加了读锁后，在当前会话中是否可以读呢？ select * from lock_one; 答案是可以的。 3) 在另一个会话中（会话2）是否可以select该表呢？ 答案也是可以的。 4) 那么在会话1中是否可以查询其他表呢？ 例如，查询 users表：select * from users; 我们发现是不可以查询其他表的，这是因为当前会话已经对lock_one表加上了锁，即当前线程锁住了lock_one表，只可以操作lock_one表，就不可以查询其他的表。 5) 问题来了，会话2是否可以查询其他表呢？ select * from users; 我们发现是可以的。因为会话2和会话1是没有关系的，会话2查询会话1锁住的表都可以，查询没有锁住的 肯定是可以的。 6) 在会话1中是否可以更新（增删改）锁住的lock_one表呢？ update lock_one set col=66 where id=1; 发现是不可以的，因为我们对 lock_one表加了 读锁，所以是不可以 进行写操作的。 7) 在会话2中是否可以更新（增删改）会话1中锁住的lock_one表呢？ 我们发现是没有执行结果的，也就是说 正在等待更新，在阻塞等待中。因为我们在会话1中对lock_one中增加了读锁，其他人只有读的操作，没有写的操作。 8) 在会话1中 对lock_one进行解锁时，会话2中的更新（增删改）操作 就会立即执行。 2.写锁1) 在会话1中对lock_one表增加写锁 lock table lock_one write; 2) 在会话1中查询该表 select * from lock_one; 我们发现是可以的。 3) 在会话2中查询该表 我们发现是没有执行结果的，也就是说 处于阻塞状态。因为写锁是排它锁，其他用户线程不可以读取当前锁住的表，只有解锁之后 其他用户线程才可以执行select 4) 在会话1中对lock_one进行写锁后，会话1会否可以查询其他表呢？ select * from users; 我们发现是不可以的。道理和读锁的时候一样，当前会话已经对lock_one表加上了锁，即当前线程锁住了lock_one表，只可以操作lock_one表，就不可以查询其他的表。 5) 那么在会话2中是否可以查询其他表呢？ 答案肯定是可以的。因为之和锁的表有关系，和其他表没有任何关系。 6) 在会话1中是否可以进行写（增删改）操作呢？ 答案一定是可以的。因为会话1对lock_one表进行了写锁操作，也就是只可以写。 7) 在会话2中是否可以进行写（增删改）操作呢？ 我们发现是不可以的。因为写锁是排它锁，也就是只可以当前线程操作锁住的表，其他用户线程需要等到解锁之后才可以操作该表。 3.总结1) 甲对表A加了读锁 甲对表A可以执行读（查询）操作，但不可以执行写（增删改）操作 甲对其他表不可以执行读写（增删改查）操作 乙对表A可以执行读（查询）操作，但不可以执行写（增删改）操作 乙对其他表可以执行读写（增删改查）操作 2) 甲对表A加了写锁 甲对表A可以执行读写（增删改查）操作 甲对其他表不可以执行读写（增删改查）操作 乙对表A不可以执行读写（增删改查）操作 乙对其他表可以执行读写（增删改查）操作 四、MyISAM存储引擎中锁特点1.执行select语句的时候，会自动给涉及的表加上表锁，在执行更新操作时，会自动给表加上写锁 2.MyISAM存储引擎比较适合作为以查询为主的表存储引擎，不适合写为主的表存储引擎，因为加写锁后，是锁住整个表，其他用户线程不能做任何操作， 这样会导致大量用户线程阻塞的情况。 五、表锁的状态查询查询指令：show status like ‘table_lock%’; 说明： Table_locks_immediate：表示可以立即获取锁的查询次数，每获取一次锁就增加1 Table_locks_waited：锁等待的次数（重要，如果这个值的大，则说明锁表的次数多，需要优化，通过 show open tables，查看哪些表锁了，然后分析为什么会锁）。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>表锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发 -- Fork/Join框架]]></title>
    <url>%2F2018%2F07%2F23%2FJava%E5%B9%B6%E5%8F%91-Fork-Join%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[文章基于jdk1.7，通过学习《Java并发编程的艺术》，对Fork/Join框架的理解。 什么是Fork/Join框架Fork/Join框架是Java7提供了的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。 它的主要思想是：分而治之。 工作窃取算法工作窃取（work-stealing）算法是指某个线程从其他队列里窃取任务来执行。 什么需要使用工作窃取算法呢？假如我们需要做一个比较大的任务，我们可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，于是把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应，比如A线程负责处理A队列里的任务。但是有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。而在这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。 工作窃取算法的优点是充分利用线程进行并行计算，并减少了线程间的竞争，其缺点是在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且消耗了更多的系统资源，比如创建多个线程和多个双端队列。 介绍Fork/Join框架的设计分为两步： 第一步分割任务。首先我们需要有一个fork类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。 第二步执行任务并合并结果。分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。 Fork/Join使用两个类来完成以上两件事情： ForkJoinTask：我们要使用ForkJoin框架，必须首先创建一个ForkJoin任务。它提供在任务中执行fork()和join()操作的机制，通常情况下我们不需要直接继承ForkJoinTask类，而只需要继承它的子类，Fork/Join框架提供了以下两个子类： RecursiveAction：用于没有返回结果的任务。RecursiveTask ：用于有返回结果的任务。 ForkJoinPool ：ForkJoinTask需要通过ForkJoinPool来执行，任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进入队列的头部。当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务。 使用使用Fork/Join框架计算：1+2+3+……+100000000. 使用Fork／Join框架首先要考虑到的是如何分割任务，如果我们希望每个子任务最多执行10000个数的相加，那么我们设置分割的阈值是10000，由于是100000000个数字相加，所以会不停的分割，第一次先分割成两部分，即1~50000000 和 50000001~100000000，第二次继续将 1~50000000 分割成 1~25000000 和 25000001~50000000 ，将50000001~100000000 分割成 50000001~75000000 和 75000001~100000000 ……，一直分割，直到 开始和 结束的的差小于等于10000。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.util.concurrent.*;public class CountTask extends RecursiveTask&lt;Long&gt; &#123; /** * 阀值 */ private static final long THRESHOLD = 10000; // 开始数 private long start; // 结束数 private long end; public CountTask(long start, long end) &#123; this.start = start; this.end = end; &#125; @Override protected Long compute() &#123; long sum = 0; // 如果足够小就计算 boolean canComplute = (end - start) &lt;= THRESHOLD; if(canComplute) &#123; for(long i = start; i &lt;= end; i++) &#123; sum += i; &#125; &#125; else &#123; // 否则，对大任务进行拆分 // 对半分 long middle = (start + end) /2; // 进行递归 CountTask left = new CountTask(start, middle); CountTask right = new CountTask(middle + 1, end); // 执行子任务 invokeAll(left, right); // 获取结果 long lResult = left.join(); long rRight = right.join(); sum = lResult + rRight; &#125; return sum; &#125; public static void main(String[] args) &#123; long s = System.currentTimeMillis(); ForkJoinPool pool = ForkJoinPool.commonPool(); CountTask countTask = new CountTask(1,100000000); // 参数为起始值与结束值 Future&lt;Long&gt; result = pool.submit(countTask); // 如果任务完成 if(!((ForkJoinTask&lt;Long&gt;) result).isCompletedAbnormally()) &#123; try &#123; // 获取任务结果 System.out.println("fork/join计算为：" + result.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("fork/join计算花费时间：" + (System.currentTimeMillis() - s) + "ms"); s = System.currentTimeMillis(); long sum = 0; for(int i = 1; i &lt;= 100000000 ; i++) &#123; sum += i; &#125; System.out.println("计算结果：" + sum); System.out.println("普通计算花费时间：" + (System.currentTimeMillis() - s) + "ms"); &#125;&#125; fork/join计算为：5000000050000000fork/join计算花费时间：55ms计算结果：5000000050000000普通计算花费时间：53ms 三种提交任务到ForkJoinPool的方法： execute():异步执行，没有任何返回 。 invoke():同步执行，调用之后需要等待任务完成，才能执行后面的代码 。 submit():异步执行，当调用get方法的时候会阻塞，完成时返回一个future对象用于检查状态以及运行结果。 1ForkJoinPool commonPool = ForkJoinPool.commonPool(); 为公共池提供一个引用，使用预定义的公共池减少了资源消耗，因为这阻碍了每个任务创建一个单独的线程池。 检查任务运行的状态 无论以什么方式结束任务，isDone() 方法返回true； 如果完成任务过程中没有被取消或者发生异常，isCompletedNormally() 方法返回true； 如果任务被取消， isCancelled() 方法返回true； 如果任务被取消或者遇到异常，isCompletedAbnormally() 方法返回true 异常处理ForkJoinTask在执行的时候可能会抛出异常，但是我们没办法在主线程里直接捕获异常，所以ForkJoinTask提供了isCompletedAbnormally()方法来检查任务是否已经抛出异常或已经被取消了，并且可以通过ForkJoinTask的getException方法获取异常。使用如下代码： 123if(task.isCompletedAbnormally()) &#123; System.out.println(task.getException());&#125; getException方法返回Throwable对象，如果任务被取消了则返回CancellationException。如果任务没有完成或者没有抛出异常则返回null。 与ExecutorService 的区别Fork/Join采用“工作窃取模式”，当执行新的任务时他可以将其拆分成更小的任务执行，并将小任务加到线程队列中，然后再从一个随即线程中偷一个并把它加入自己的队列中。 就比如两个CPU上有不同的任务，这时候A已经执行完，B还有任务等待执行，这时候A就会将B队尾的任务偷过来，加入自己的队列中，对于传统的线程，ForkJoin更有效的利用的CPU资源！ 实现原理ForkJoinPool由ForkJoinTask数组和ForkJoinWorkerThread数组组成，ForkJoinTask数组负责存放程序提交给ForkJoinPool的任务，而ForkJoinWorkerThread数组负责执行这些任务。 ForkJoinTask的fork方法实现原理。当我们调用ForkJoinTask的fork方法时，程序会调用ForkJoinWorkerThread的pushTask方法异步的执行这个任务，然后立即返回结果。代码如下： 12345public final ForkJoinTask fork() &#123; ((ForkJoinWorkerThread) Thread.currentThread()) .pushTask(this); return this;&#125; pushTask方法把当前任务存放在ForkJoinTask 数组queue里。然后再调用ForkJoinPool的signalWork()方法唤醒或创建一个工作线程来执行任务。代码如下： 123456789101112final void pushTask(ForkJoinTask t) &#123; ForkJoinTask[] q; int s, m; if ((q = queue) != null) &#123; // ignore if queue removed long u = (((s = queueTop) &amp; (m = q.length - 1)) &lt;&lt; ASHIFT) + ABASE; UNSAFE.putOrderedObject(q, u, t); queueTop = s + 1; // or use putOrderedInt if ((s -= queueBase) &lt;= 2) pool.signalWork(); else if (s == m) growQueue(); &#125; &#125; 首先，它调用了doJoin()方法，通过doJoin()方法得到当前任务的状态来判断返回什么结果，任务状态有四种：已完成（NORMAL），被取消（CANCELLED），信号（SIGNAL）和出现异常（EXCEPTIONAL）。 如果任务状态是已完成，则直接返回任务结果。 如果任务状态是被取消，则直接抛出CancellationException。 如果任务状态是抛出异常，则直接抛出对应的异常。 让我们再来分析下doJoin()方法的实现代码： 12345678910111213141516171819202122private int doJoin() &#123; Thread t; ForkJoinWorkerThread w; int s; boolean completed; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) &#123; if ((s = status) &lt; 0) return s; if ((w = (ForkJoinWorkerThread)t).unpushTask(this)) &#123; try &#123; completed = exec(); &#125; catch (Throwable rex) &#123; return setExceptionalCompletion(rex); &#125; if (completed) return setCompletion(NORMAL); &#125; return w.joinTask(this); &#125; else return externalAwaitDone(); &#125; 在doJoin()方法里，首先通过查看任务的状态，看任务是否已经执行完了，如果执行完了，则直接返回任务状态，如果没有执行完，则从任务数组里取出任务并执行。如果任务顺利执行完成了，则设置任务状态为NORMAL，如果出现异常，则纪录异常，并将任务状态设置为EXCEPTIONAL。]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
        <tag>Fork/Join</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux上监控Tomcat down掉后自动重启Tomcat]]></title>
    <url>%2F2018%2F07%2F20%2FLinux%E4%B8%8A%E7%9B%91%E6%8E%A7Tomcat-down%E6%8E%89%E5%90%8E%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AFTomcat%2F</url>
    <content type="text"><![CDATA[tomcat运行一段时间后，凌晨无缘无故挂掉，看了tomcat日志、项目日志、系统日志，没有发现错误。于是想到写一个shell脚本，每隔2分钟监控一次tomcat 的状态，若挂掉，则重新启动。解决方案参考网络，同时修改成符合自己的脚本。 使用环境 操作系统：CentOS 7 JDK版本：1.8.0_161-b12 64位 Tomcat版本：8.5.29 编写脚本在win下新建：monitor.sh，内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/bin/sh# function:自动监控tomcat进程，挂了就执行重启操作# DEFINE# 环境变量export JAVA_HOME=/usr/local/jdk1.8.0_161export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar# 获取tomcat PID，要加上grep java，否则会打印多个进程IDTomcatID=$(ps -ef | grep java |grep tomcat |grep -w 'apache-tomcat-8.5.29_2'|grep -v 'grep'|awk '&#123;print $2&#125;')# tomcat_startupStartTomcat=`nohup /usr/local/apache-tomcat-8.5.29_2/bin/startup.sh &amp;`TomcatCache=usr/local/apache-tomcat-8.5.29_2/work# 定义要监控的页面地址WebUrl=http://127.0.0.1:8080/xxxxx/monitor# 日志输出GetPageInfo=/dev/nullTomcatMonitorLog=/tmp/TomcatMonitor.logMonitor()&#123; echo "[info]开始监控tomcat...[$(date +'%F %H:%M:%S')],进程ID:$TomcatID." if [ "$TomcatID" != "" ];then echo "[info]tomcat进程ID为:$TomcatID." # 获取返回状态码 TomcatServiceCode=$(curl -s -o $GetPageInfo -m 10 --connect-timeout 10 $WebUrl -w %&#123;http_code&#125;) if [ $TomcatServiceCode -eq 200 ];then echo "[info]返回码为$TomcatServiceCode,tomcat处于running状态." else echo "[error]访问出错，状态码为$TomcatServiceCode,错误日志已输出到$GetPageInfo" echo "[error]开始重启tomca,kill前进程id为:$TomcatID" kill -9 $TomcatID # 杀掉原tomcat进程 sleep 3 rm -rf $TomcatCache # 清理tomcat缓存 $StartTomcat echo "[info] tomcat启动成功." fi else echo "[error]进程不存在!tomcat自动重启..." echo "[info]$StartTomcat,请稍候......" rm -rf $TomcatCache $StartTomcat fi echo "------------------------------"&#125;Monitor&gt;&gt;$TomcatMonitorLog 在bin目录下执行：./monitor.sh（点不出的话先授权：sudo chmod a+x monitor.sh），发现报错，错误如下： 1-bash: ./monitor.sh: /bin/sh^M: bad interpreter: No such file or directory 原因是因为在windows下编辑的，然后上传到linux系统里执行。.sh文件的格式为dos格式，而linux只能执行格式为unix格式的脚本 ，解决方法： 1234# 没有 需要先安装yum install dos2unix# 修改格式dos2unix monitor.sh 添加任务12345678910111213141516# 没有安装 需要先安装crontabyum install crontab # 启动/sbin/service crond start# 停止/sbin/service crond stop# 重启服务/sbin/service crond restart# 重新加载/sbin/service crond reload# crontab其他命令要把cron设为在开机的时候自动启动，在 /etc/rc.d/rc.local 脚本中加入 /sbin/service crond start 即可查看当前用户的crontab，输入 crontab -l编辑crontab，输入 crontab -e删除crontab，输入 crontab -r crontab -e ，在文档末尾处添加（每隔2分钟执行一次）： 1*/2 * * * * /usr/local/hrfiles/apache-tomcat-8.5.29_2/bin/monitor.sh 执行日志可查看： 1tail -f /tmp/TomcatMonitor.log]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[com.netflix.zuul.exception.ZuulException: Forwarding error]]></title>
    <url>%2F2018%2F07%2F19%2Fcom-netflix-zuul-exception-ZuulException-Forwarding-error%2F</url>
    <content type="text"><![CDATA[场景项目基于SpringCloud，生产者做了负载，消费者（此处用的ribbon + restTemplcate）做了降级(hystrix)，当一个消费者关掉后，访问网关（zuul），出现如下错误： 123456789101112com.netflix.zuul.exception.ZuulException: Forwarding error at org.springframework.cloud.netflix.zuul.filters.route.RibbonRoutingFilter.handleException(RibbonRoutingFilter.java:183) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at org.springframework.cloud.netflix.zuul.filters.route.RibbonRoutingFilter.forward(RibbonRoutingFilter.java:158) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at org.springframework.cloud.netflix.zuul.filters.route.RibbonRoutingFilter.run(RibbonRoutingFilter.java:106) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at com.netflix.zuul.ZuulFilter.runFilter(ZuulFilter.java:112) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.FilterProcessor.processZuulFilter(FilterProcessor.java:193) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.FilterProcessor.runFilters(FilterProcessor.java:157) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.FilterProcessor.route(FilterProcessor.java:118) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.ZuulRunner.route(ZuulRunner.java:96) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.http.ZuulServlet.route(ZuulServlet.java:116) ~[zuul-core-1.3.0.jar:1.3.0] at com.netflix.zuul.http.ZuulServlet.service(ZuulServlet.java:81) ~[zuul-core-1.3.0.jar:1.3.0]................... 省略一大片.............. 解决方法在网关服务中的配置文件中配置如下信息： 123456789101112131415161718192021222324252627zuul: okhttp: enabled: true # 使用okhttp方式请求，正常来说okhttp比较速度快一点 semaphore: max-semaphores: 500 # 并发处理数，值越大越好，但到到达一个临界点之后，就不会提高响应速度了 host: socket-timeout-millis: 30000 # socket超时时间，如果使用service-id方式是不用配置的 connect-timeout-millis: 30000 # 连接时间semaphores max-total-connections: 5000 # 最大连接数，值越大越好，但到到达一个临界点之后，就不会提高响应速度了 max-per-route-connections: 5 # 每个router最大连接数，降低请求时间，越小越好，但达到一定层级就没用了hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 30000 # Hystrix超时时间 strategy: THREADribbon: ReadTimeout: 20000 # 处理时间 ConnectTimeout: 20000 # 连接时间 MaxAutoRetries: 0 #最大自动重试次数 MaxAutoRetriesNextServer: 1 # 换实例重试次数 MaxTotalHttpConnections: 2000 # 最大http连接数，越大越好，但到到达一个临界点之后，就不会提高响应速度了 MaxConnectionsPerHost: 1000 # 每个host连接数 注：解决方案来源于网上，具体链接没有找到。]]></content>
      <categories>
        <category>Exception</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
        <tag>zuul</tag>
        <tag>Exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git基本使用]]></title>
    <url>%2F2018%2F07%2F19%2Fgit%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前提配置邮箱和用户名 12$ git config --global user.name "你的名字"$ git config --global user.email "你的邮箱" 上传项目到github123456$ git init $ git add . (添加到暂存区里面去)$ git commit -m 'first commit' (把文件提交到仓库)$ git remote add origin https://github.com/xxx/xxx.git (关联到远程库)$ git pull --rebase origin master (获取远程库与本地同步合并（如果远程库不为空必须做这一步，否则后面的提交会失败）)$ git push -u origin master (把本地库的内容推送到远程) 1.初始化1$ git init 2.提交123456789101112131415$ git pull origin master$ git touch init.txt //如果已经存在更改的文件,则这一步不是必须的$ git add .$ git commit -m "first commit"$ git push -u origin master # 第一次提交需要加 -u$ git push origin master # 后面提交就不需要-u了$ git push -u origin master -f # 强制覆盖解决(慎用)# 同步冲突如果您舍弃线上的文件，则在推送时选择强制推送，强制推送需要执行下面的命令$ git push origin master -f如果您选择保留线上的文件,则需要先执行git pull origin master然后才可以推送,如果发生冲突，则需要先解决冲突 更新项目1234$ git pull --rebase origin master(会覆盖本地文件，慎用)或者：$ git fetch --all$ git reset --hard origin/master 异常1.当出现no changes added to commit时如何正确使用git提交命令对于这个问题，最好的解决方法就是按如下步骤： 到解决方案根目录下：git add . (“.”是必须要的) git commit -m “some word” git push -u origin master]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发-volitile的应用]]></title>
    <url>%2F2018%2F07%2F16%2FJava%E5%B9%B6%E5%8F%91-volitile%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[volatile的定义 volatile是Java语言中的类型修饰符，它是被设计用来修饰被不同线程访问和修改的变量。是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”，可见性是指当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。volatile比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。 实现原理 使用volatile修饰的变量在汇编阶段，会多出一条lock前缀指令，它在多核处理器下会引发两件事： 将当前处理器缓存行的数据写回到系统内存 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 通常处理器和内存之间都有几级缓存来提高处理速度，处理器先将内存中的数据读取到内部缓存后再进行操作，但是对于缓存写回内存的时机则无法得知，因此在一个处理器里修改的变量值，不一定能及时写回缓存，这种变量修改对其他处理器变得“不可见”了。但是，使用volatile修饰的变量，在写操作的时候，会强制将这个变量所在缓存行的数据写回到内存中，但即使写回到内存，其他处理器也有可能使用内部的缓存数据，从而导致变量不一致，所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期，如果过期，就会将该缓存行设置成无效状态，下次要使用就会重新从内存中读取。]]></content>
      <categories>
        <category>Concurrent</category>
      </categories>
      <tags>
        <tag>Concurrent</tag>
        <tag>volitile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The last packet successfully received from the server was 1,480 milliseconds ago.]]></title>
    <url>%2F2018%2F07%2F16%2FThe-last-packet-successfully-received-from-the-server-was-1-480-milliseconds-ago%2F</url>
    <content type="text"><![CDATA[场景：一个上传接口，需要上传几十M的文件，文件中包含10几W的数据，然后对10+W的数据进行同步批量插入，每次批量插入1W。最后返回结果。 项目上线一段时间后，上传接口出现问题，数据库用的MySQL5.7.21，报了如下错误： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172732018-07-16 01:30:03.497 ERROR com.alibaba.druid.pool.DruidDataSource Line:1594 - discard connectioncom.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failureThe last packet successfully received from the server was 1,480 milliseconds ago. The last packet sent successfully to the server was 1,480 milliseconds ago. at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1121) at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3603) at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3492) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4043) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2503) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2664) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2815) at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2155) at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379) at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:498) at org.apache.ibatis.executor.statement.PreparedStatementHandler.query(PreparedStatementHandler.java:63) at org.apache.ibatis.executor.statement.RoutingStatementHandler.query(RoutingStatementHandler.java:79) at org.apache.ibatis.executor.SimpleExecutor.doQuery(SimpleExecutor.java:63) at org.apache.ibatis.executor.BaseExecutor.queryFromDatabase(BaseExecutor.java:324) at org.apache.ibatis.executor.BaseExecutor.query(BaseExecutor.java:156) at org.apache.ibatis.executor.CachingExecutor.query(CachingExecutor.java:109) at com.github.pagehelper.PageInterceptor.intercept(PageInterceptor.java:137) at org.apache.ibatis.plugin.Plugin.invoke(Plugin.java:61) at com.sun.proxy.$Proxy63.query(Unknown Source) at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:148) at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:141) at sun.reflect.GeneratedMethodAccessor102.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.mybatis.spring.SqlSessionTemplate$SqlSessionInterceptor.invoke(SqlSessionTemplate.java:434) at com.sun.proxy.$Proxy25.selectList(Unknown Source) at org.mybatis.spring.SqlSessionTemplate.selectList(SqlSessionTemplate.java:231) at org.apache.ibatis.binding.MapperMethod.executeForMany(MapperMethod.java:137) at org.apache.ibatis.binding.MapperMethod.execute(MapperMethod.java:75) at org.apache.ibatis.binding.MapperProxy.invoke(MapperProxy.java:53) at com.sun.proxy.$Proxy51.selectReportListByReportDate(Unknown Source) at com.rxwx.service.report.impl.PrcmtReportServiceImpl.generateProcurementReport(PrcmtReportServiceImpl.java:97) at com.rxwx.service.report.impl.PrcmtReportServiceImpl$$FastClassBySpringCGLIB$$f325780.invoke(&lt;generated&gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:99) at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:280) at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:655) at com.rxwx.service.report.impl.PrcmtReportServiceImpl$$EnhancerBySpringCGLIB$$8c585594.generateProcurementReport(&lt;generated&gt;) at com.rxwx.service.report.impl.PrcmtReportServiceImpl$$FastClassBySpringCGLIB$$f325780.invoke(&lt;generated&gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:99) at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:280) at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:655) at com.rxwx.service.report.impl.PrcmtReportServiceImpl$$EnhancerBySpringCGLIB$$9a830f32.generateProcurementReport(&lt;generated&gt;) at com.rxwx.task.sup.GenarateReportJob.selfGenReportTask1(GenarateReportJob.java:37) at com.rxwx.task.sup.GenarateReportJob$$FastClassBySpringCGLIB$$64b31449.invoke(&lt;generated&gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:115) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost. at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3052) at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3503) ... 58 common frames omitted 首先百度了一发，按照网上的解决方案，修改了my.cnf中的超时时间： 12wait_timeout=31536000interactive_timeout=31536000 将原来默认的8小时改为1年。这里单位是秒。 修改了德鲁伊的：testOnBorrow = true。 跑了一天，MMP，凌晨又出现这个错误了。 一个电话把睡梦中的我吵醒，扰我周末扰我梦，没得办法，重启下服务，重启下MySQL，先上传吧，睡觉。 周一到公司谷歌了一发，修改my.cnf的net_read_timeout和net_write_timeout 参数，将这两个参数调大，改为6000。由于项目中的数据存在无效数据，所以删除了10天之前的无效数据，减少了表中的数量。明天看效果。 show global variables like “%timeout%”; connect_timeout 连接超时 mysql连接共有6次握手，3次TCP协议这个跟connect_timeout参数没有关系，另外3次跟connect_timeout参数有关系，该参数主要是为了防止网络不佳时应用重连导致连接数涨太快，一般默认即可。 delayed_insert_timeout 这是为MyISAM INSERT DELAY设计的超时参数，在INSERT DELAY中止前等待INSERT语句的时间 interactive_timeout 服务器关闭交互式连接前等待活动的秒数。交互式客户端定义为在mysql_real_connect()中使用CLIENT_INTERACTIVE选项的客户端。参数默认值：28800秒（8小时） lock_wait_timeout 锁等待超时时间 net_read_timeout / net_write_timeout 这个参数只对TCP/IP链接有效，分别是数据库等待接收客户端发送网络包和发送网络包给客户端的超时时间，这是在Activity状态下的线程才有效的参数 slave_net_timeout 这是Slave判断主机是否挂掉的超时设置，在设定时间内依然没有获取到Master的回应就人为Master挂掉了 wait_timeout 服务器关闭非交互连接之前等待活动的秒数。 效果很明显。今天没有出现问题。]]></content>
      <categories>
        <category>Exception</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis问题汇总]]></title>
    <url>%2F2018%2F06%2F10%2FRedis%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1. 什么是RedisRedis是由意大利人Salvatore Sanfilippo（网名：antirez）开发的一款内存高速缓存数据库。Redis全称为：Remote Dictionary Server（远程数据服务），该软件使用C语言编写，Redis是一个key-value存储系统，它支持丰富的数据类型，如：string、list、set、zset(sorted set)、hash。 2. Redis特点 以内存作为数据存储介质，读写数据的效率极高，远远超过数据库。以设置和获取一个256字节字符串为例，它的读取速度可高达110000次/s，写速度高达81000次/s。 支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 支持多种数据类型String、List、Hash、Set、zset 支持数据的备份，即master-slave模式的数据备份 3.Redis有哪些数据类型，以及每种数据类型使用的场景Redis支持多种数据类型，有String、List、Hash、Set、zset String(字符串)String类型是二进制安全的，意思是Redis的String可以包含任何数据，比如图片或者序列化的对象等。一个Redis中字符串的value最多可以是512M。一般做一些复杂的计数功能的缓存。 List(列表)List是按照插入顺序排序的字符串链表。 从元素插入和删除的效率视角来看，如果我们是在链表的两头插入或删除元素，这将会是非常高效的操作，即使链表中已经存储了百万条记录，该操作也可以在常量时间内完成。然而需要说明的是，如果元素插入或删除操作是作用于链表中间，那将会是非常低效的 。 Redis链表经常会被用于消息队列的服务，以完成多程序之间的消息交换。假设一个应用程序正在执行LPUSH操作向链表中添加新的元素，我们通常将这样的程序称之为”生产者(Producer)”，而另外一个应用程序正在执行RPOP操作从链表中取出元素，我们称这样的程序为”消费者(Consumer)”。如果此时，消费者程序在取出消息元素后立刻崩溃，由于该消息已经被取出且没有被正常处理，那么我们就可以认为该消息已经丢失，由此可能会导致业务数据丢失，或业务状态的不一致等现象的发生。然而通过使用RPOPLPUSH命令，消费者程序在从主消息队列中取出消息之后再将其插入到备份队列中，直到消费者程序完成正常的处理逻辑后再将该消息从备份队列中删除。同时我们还可以提供一个守护进程，当发现备份队列中的消息过期时，可以重新将其再放回到主消息队列中，以便其它的消费者程序继续处理。 可以利用 lrange 命令，做基于 Redis 的分页功能，性能极佳，用户体验好 Hash(字典)Hash是一个健值对集合，是一个String类型的key与value的映射表，特别适合用于存储对象。 可用于存储、读取、修改用户属性， Hash 结构可以使你像在数据库中 Update 一个属性一样只修改某一项属性值。 Set(集合)Set 是一个集合，集合的概念就是一堆不重复值的组合。利用 Redis 提供的 Set 数据结构，可以存储一些集合性的数据。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1) 。Redis 非常人性化的为集合提供了求交集、并集、差集等操作，那么就可以非常方便的实现如共同关注、共同喜好、二度好友等功能 。也可以做全局去重的功能。 zset(Sorted Set,有序集合)和Sets相比，Sorted Sets是将 Set 中的元素增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列。可以做排行榜应用，取 TOP N 操作。Sorted Set 可以用来做延时任务。最后一个应用就是可以做范围查找。 4.为什么要用Redis性能和并发。 性能：将一些耗时比较久，且结果不经常变动的SQL，放到Redis中，这样，请求直接从缓存中读取，使得能够迅速响应。 并发：大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用Redis做一个缓冲操作，让请求先访问到redis，而不是直接访问数据库 。 5.Redis有什么缺点 缓存和数据库双写一致性问题 缓存击穿问题 缓存雪崩问题 缓存的并发竞争问题 6.Redis为什么这么快 纯内存操作 单线程模型，避免了频繁的上下文切换 采用非阻塞I/O多路复用机制 什么是非阻塞I/O呢？ 阻塞与非阻塞可以简单理解为调用一个IO操作能不能立即得到返回应答，如果不能立即获得返回，需要等待，那就阻塞了；否则就可以理解为非阻塞。 什么是I/O多路复用机制呢？ 单个线程，通过记录跟踪每个I/O流(sock)的状态，来同时管理多个I/O流 。 I/O多路复用的优势并不是对于单个连接能处理的更快，而是在于可以在单个线程/进程中处理更多的连接。与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。 7.Redis的过期策略Redis采用的过期策略是：定期删除+惰性删除策略。 定期删除，Redis 默认每隔100ms 检查，是否有过期的 Key，有过期 Key 则删除。 需要说明的是，Redis 不是每隔100ms 将所有的 Key 检查一次，而是随机抽取进行检查(如果每隔 100ms，全部 Key 进行检查，Redis 岂不是卡死)。 123定期删除可以通过：第一、配置redis.conf 的hz选项，默认为10 （即1秒执行10次，100ms一次，值越大说明刷新频率越快，最Redis性能损耗也越大，建议不要超过100） 第二、配置redis.conf的maxmemory最大值，当已用内存超过maxmemory限定时，就会触发主动清理策略 因此，如果只采用定期删除策略，会导致很多 Key 到时间没有删除。于是，惰性删除派上用场。 也就是说在你获取某个 Key 的时候，Redis 会检查一下，这个 Key 如果设置了过期时间，如果过期了，此时就会删除。 过期策略可以参考：Redis数据过期策略详解 8.Redis内存淘汰机制在 redis.conf 中有一行配置 1# maxmemory-policy volatile-lru Redis内存淘汰策略有（触发该策略的机制是 当内存不足以容纳新写入数据时）： noeviction：谁也不删，直接在写操作时返回错误 。应该没人用吧。 allkeys-lru：在键空间中，移除最近最少使用的 Key。推荐使用，目前项目在用这种。 allkeys-random：在键空间中，随机移除某个 Key。应该也没人用吧，你不删最少使用 Key，去随机删。 volatile-lru：在设置了过期时间的键空间中，移除最近最少使用的 Key。这种情况一般是把 Redis 既当缓存，又做持久化存储的时候才用。不推荐。 volatile-random：在设置了过期时间的键空间中，随机移除某个 Key。依然不推荐。 volatile-ttl：在设置了过期时间的键空间中，有更早过期时间的 Key 优先移除。不推荐。 9.Redis和数据库双写一致性问题首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。 10.如何应对缓存穿透、缓存雪崩、缓存击穿问题缓存穿透：黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。 解决方法： 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层数据库的查询压力（推荐） 如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟（推荐，简单暴力） 利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试 采用异步更新策略，无论 Key 是否取到值，都直接返回。Value 值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。 缓存雪崩：当缓存服务器重启或者大量缓存集中在某一个时间段失效，来了一批请求，请求全部到DB，DB瞬时压力过重雪崩。 解决方法： 给缓存的失效时间加上一个随机值，避免集体失效 使用互斥锁，但是该方案吞吐量明显下降了 双缓存，我们有两个缓存，缓存 A 和缓存 B。缓存 A 的失效时间为 20 分钟，缓存 B 不设失效时间。自己做缓存预热操作。然后细分以下几个小点：从缓存 A 读数据库，有则直接返回；A 没有数据，直接从 B 读数据，直接返回，并且异步启动一个更新线程，更新线程同时更新缓存 A 和缓存 B。（推荐） 缓存击穿：对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一key缓存，前者则是很多key。 缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 解决方法： 使用互斥锁 “提前”使用互斥锁 “永不过期” 参考： ​ 缓存穿透，缓存击穿，缓存雪崩解决方案分析 ​ Redis架构之防雪崩设计：网站不宕机背后的兵法 11.若Redis中有1亿个key，其中有10w是以某个固定的已知前缀开头，怎么将它们全部找出来使用keys指令可以扫出指定模式的key列表。如果这个redis正在给线上的业务提供服务，keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 12.怎么使用Redis做异步队列一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 如果不用sleep，list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 使用pub/sub主题订阅者模式，可以实现1:N的消息队列。 也就是生产一次消费多次。但是使用pub/sub是有缺点的，在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。 redis如何实现延时队列：使用sortedset，拿时间戳作为score，消息内容作为key，调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 13.怎么用Redis实现分布式锁主要是使用了redis 的setnx命令，缓存了锁，reids缓存的key是锁的key,所有的共享, value是锁的到期时间(注意:这里把过期时间放在value了,没有时间上设置其超时时间)。 1.通过setnx尝试设置某个key的值,成功(当前没有这个锁)则返回,成功获得锁 2.锁已经存在则获取锁的到期时间,和当前时间比较,超时的话,则设置新的值 实现方法可以参考：Redis分布式锁实现 14.参考为什么分布式一定要有Redis? 天下无难试之Redis面试刁难大全]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java注解(三) - 注解的使用]]></title>
    <url>%2F2018%2F03%2F10%2FJava%E6%B3%A8%E8%A7%A3-%E4%B8%89-%E6%B3%A8%E8%A7%A3%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[学会了如何定义自定义注解，那还要会用起来才行。 其实自定义注解使用也非常简单，像我们上篇文章定义的一个 Sweet 注解。 123public @interface Sweet &#123; String sweetLevel();&#125; 要使用它只需要像下面这样就可以了。 123456public class SweetDemo &#123; @Sweet (sweetLevel="Level.05") public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 但是有时候注解会有些特殊用法，我们需要了解一下。 第一种情况：如果没有任何注解属性，那么可以省略注解的中括号。 在上面的例子中，如果 @Sweet 注解没有任何属性。 12public @interface Sweet &#123;&#125; 那么我们使用的时候就可以直接写上直接名称，不需要中括号。 123456public class SweetDemo &#123; @Sweet public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 第二种情况：注解属性有默认值，可以不进行赋值操作。 在上面的 SweetDemo 中会发现我们在使用 @Sweet 注解的时候，手动给 sweetLevel 属性赋值。如果没有赋值，那么会报错。 但是如果在 @Sweet 注解声明的时候，给 sweetLevel 属性定义一个默认值，那么在使用的时候就不需要赋值操作了。 例如我们重新定义 Sweet，让你有一个「Level.03」的默认值。 123public @interface Sweet &#123; String sweetLevel();&#125; 那么在使用的时候就可以直接这样使用： 123456public class SweetDemo &#123; @Sweet public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 这个时候，sweetLevel 属性就是默认值：Level.03。 第三种情况：注解内有且仅有一个名字为 value 的属性时，应用这个注解时可以直接接属性值填写到括号内。 例如上面的 @Sweet 注解改写成这样： 123public @interface Sweet &#123; String value();&#125; 那么在使用的时候，我们本来应该这样用： 123456public class SweetDemo &#123; @Sweet(value = "Level.03") public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 但是我们可以忽略 value 属性名的声明，直接这么用： 123456public class SweetDemo &#123; @Sweet("Level.03") public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; &#125; 总结下面就来总结一下，其实自定义注解使用不复杂，但有下面三种情况比较特殊： 注解没有任何注解属性，那么可以省略注解的中括号。 注解的注解属性有默认值，可以不进行赋值操作。 注解内有且仅有一个名字为 value 的属性时，应用这个注解时可以直接接属性值填写到括号内。 原文地址注解的那些事儿（三）| 注解的使用]]></content>
      <categories>
        <category>Annotation</category>
      </categories>
      <tags>
        <tag>Java注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java注解(二) - 如何自定义注解]]></title>
    <url>%2F2018%2F03%2F10%2FJava%E6%B3%A8%E8%A7%A3-%E4%BA%8C-%E5%A6%82%E4%BD%95%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[要自定义注解，首先需要了解一个注解的构成部分。 一个注解大致可以分为三个部分：注解体、元注解、注解属性。 在这三个主要组成部分中，注解体指定了注解的名字，而元注解则标记了该注解的使用场景、留存时间等信息，而注解属性则指明该注解拥有的属性。 注解体注解体是最简单的一个组成部分，只需要实例中一样有样学样即可。与接口的声明唯一的不同是在 interface 关键字前多了一个 @ 符号。 1234//声明了一个名为sweet的注解体@Retention(RetentionPolicy.RUNTIME) public @interface sweet&#123;&#125; 元注解元注解（meta-annotation）本身也是一个注解，用来标记普通注解的存留时间、使用场景、继承属性、文档生成信息。 元注解是一个特殊的注解，它是 Java 源码中就自带的注解。在Java 中只有四个元注解，它们分别是：@Target、@Retention、@Documented、@Inherited。 @Target注解Target 注解限定了该注解的使用场景 它有下面这些取值： ElementType.ANNOTATION_TYPE 可以给一个注解进行注解 ElementType.CONSTRUCTOR 可以给构造方法进行注解 ElementType.FIELD 可以给属性进行注解 ElementType.LOCAL_VARIABLE 可以给局部变量进行注解 ElementType.METHOD 可以给方法进行注解 ElementType.PACKAGE 可以给一个包进行注解 ElementType.PARAMETER 可以给一个方法内的参数进行注解 ElementType.TYPE 可以给一个类型进行注解，比如类、接口、枚举 1234@Target(&#123;ElementType.CONSTRUCTOR, ElementType.METHOD, ElementType.PARAMETER, ElementType.FIELD, ElementType.ANNOTATION_TYPE&#125;) public @interface Autowired &#123; boolean required() default true;&#125; 在上面 Autowire的 注解中，其 Target 注解的值为 CONSTRUCTOR、METHOD、PARAMETER、FIELD、ANNOTATION_TYPE 这 5 个值。这表示 Autowired 注解只能在构造方法、方法、方法形参、属性、类型这 5 种场景下使用。 @Retention注解Retention 注解用来标记这个注解的留存时间。 它其有四个可选值： RetentionPolicy.SOURCE。注解只在源码阶段保留，在编译器进行编译时它将被丢弃忽视。 RetentionPolicy.CLASS。注解只被保留到编译进行的时候，它并不会被加载到 JVM 中。 RetentionPolicy.RUNTIME。注解可以保留到程序运行的时候，它会被加载进入到 JVM 中，所以在程序运行时可以获取到它们。 1234@Retention(RetentionPolicy.RUNTIME) public @interface Autowired &#123; boolean required() default true;&#125; 在上面 Autowire的 注解中，其 Retention 注解的值为 RetentionPolicy.RUNTIME，说明该注解会保留到程序运行的时候。 @Documented@ Documented 注解表示将注解信息写入到 javadoc 文档中。 在默认情况下，我们的注解信息是不会写入到 Javadoc 文档中的。但如果该注解有 @Documented 标识，那么该注解信息则会写入到 javadoc 文档中。 例如在下面这个例子中，我们声明了一个 @Spicy 的注解，没有 @Documented 元注解。 123public @interface Spicy &#123; String spicyLevel();&#125; 声明一个 @Sweet 注解，有 @Documented 元注解。 1234@Documentedpublic @interface Sweet &#123; String sweetLevel();&#125; 接下来写一个 SweetDemo 类，类中的 sweetWithDoc 方法使用 @Sweet 注解，spicyWithoutDoc 方法使用 @Spicy 注解。 1234567891011121314public class SweetDemo &#123; public static void main(String arg[]) &#123; new SweetDemo().sweetWithDoc(); new SweetDemo().spicyWithoutDoc(); &#125; @Sweet (sweetLevel="Level.05") public void sweetWithDoc() &#123; System.out.printf("sweet With Doc."); &#125; @Spicy (spicyLevel="Level.04") public void spicyWithoutDoc() &#123; System.out.printf("spicy Without Doc."); &#125;&#125; 最后我们使用 Javadoc 命令去生成对应的 JavaDoc 文档，打开文档你会看到：sweetWithDoc方法上面有一个注解信息，而 spicyWithoutDoc 方法上却没有注解信息。 这个就是 @Documented 这个元注解的作用。 @Inherited@ Inherited注解标识子类将继承父类的注解属性。 在下面的例子中，我们声明了一个 Sweet 注解，接着在 Peach 类使用了 @Sweet 注解，但是并没有在 RedPeach 类使用该注解。 123456789//声明一个Sweet注解，标识甜味。@Inherited@Retention(RetentionPolicy.RUNTIME)@interface Sweet &#123;&#125;//桃子有甜味@Sweetpublic class Peach &#123;&#125;//红色的水蜜桃public class RedPeach extends Peach &#123;&#125; 虽然我们没在 RedPeach 类上使用了 @Sweet 注解，但是我们在 Sweet 注解声明中使用了 @Inherited 注解，所以 RedPeach 继承了 Peach 的 @Sweet 注解。 注解属性注解属性类似于类方法的声明，注解属性里有三部分信息，分别是：属性名、数据类型、默认值。 在 @Autowired 注解中就声明了一个名为 required 的 boolean 类型数据，其默认值是 true。 123public @interface Autowired &#123; boolean required() default true;&#125; 需要注意的是，注解中定义的属性，它的数据类型必须是 8 种基本数据类型（byte、short、int、long、float、double、boolean、char）或者是类、接口、注解及它们的数组。 总结个注解大致可以分为三个部分：注解体、元注解、注解属性。在这三个主要组成部分中：注解体指定了注解的名字、元注解则标记了该注解的使用信息，注解属性指明注解的属性。 学习注解只要知道这三个部分就够了，至于那些繁杂的属性，就用下面这张图来解决吧。用到的时候翻一翻，查一查，足矣！ 原文地址注解的那些事儿（二）| 如何自定义注解]]></content>
      <categories>
        <category>Annotation</category>
      </categories>
      <tags>
        <tag>Java注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java注解(一) - 为什么要使用注解]]></title>
    <url>%2F2018%2F03%2F09%2FJava%E6%B3%A8%E8%A7%A3-%E4%B8%80-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是注解注解 是 JDK 1.5 引入的功能。可以看作是对 一个 类／方法 的一个扩展的模版，每个 类／方法 按照注解类中的规则，来为 类／方法 注解不同的参数，在用到的地方可以得到不同的 类／方法 中注解的各种参数与值。 为什么要用注解在 JDK 1.5 之前，Java 还没引入注解，这个时候如果我们要在 Spring 中声明一个 Bean，我们只能通过 XML 配置的方式。 1public class DemoService&#123; &#125; 1&lt;bean id="demoService" class="com.chenshuyi.DemoService"/&gt; 但当有了注解，我们就可以不必写一个 XML 配置文件，可以直接在 DemoService 类上完成 Bean 的声明工作。 12@Service public class DemoService&#123; &#125; 在表面上看来，我们通过注解的方式减少了一个XML配置文件，减少了开发代码量。但这真的是我们用注解而不用 XML 配置文件的原因吗？ 在回答这个问题之前，我们再来回顾一下上面两种配置方式的特点： 对于注解的方式。我们会发现它和代码结合得很紧密，所以注解比较适合做一些与代码相关度高的操作，例如将Bean对应的服务暴露出去。 对于XML配置方式。我们会发现它将配置和代码隔离开来了所以XML配置更适合做一些全局的、与具体代码无关的操作，例如全局的配置等。 我相信很多人此前对于注解的认识就是方便开发。但事实上使用注解还是XML的判断标准应该是：该配置与代码的相关度。如果代码与配置相关度高，那么使用注解配置，否则使用XML配置。 注解和配置文件的比较优点： 配置文件 遵循OCP开发原则，修改配置文件即可进行功能扩展（OCP 开闭原则 Open Closed Principle） 集中管理对象和对象之间的组合关系，易于阅读 注解 开发速度快 编译期间容易发现错误的出处 缺点： 配置文件 开发速度相对较慢 编译时很难检查出错误，运行中的错误很难定位，调试难度较大 注解 管理分散，基本每个类上都有 扩展功能时，没有遵循OCP开发原则 原文地址​ 注解的那些事儿（一）| 为什么要使用注解？]]></content>
      <categories>
        <category>Annotation</category>
      </categories>
      <tags>
        <tag>Java注解</tag>
      </tags>
  </entry>
</search>
